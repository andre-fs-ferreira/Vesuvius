{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a3670b",
   "metadata": {},
   "source": [
    "# The notebook cell at kaggle is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(len(filenames))\n",
    "    for filename in filenames:\n",
    "        print(f\"Doing case:\")\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58aeab8",
   "metadata": {},
   "source": [
    "# Let's prepare everything to make it ready to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_content = {\n",
    "  \"architecture\": \"STU-Net\",\n",
    "  \"patch_size\": [128, 128, 128],\n",
    "  \"device\": \"cuda\",\n",
    "  \"batch_size\": 1,\n",
    "  \"num_workers\": 4,\n",
    "\n",
    "  \"infer_sw_batch_size\": 2,\n",
    "  \"blend_mode\": \"gaussian\",\n",
    "  \"deep_supervision\": False,\n",
    "  \"activation\": True,\n",
    "\n",
    "  \"infer_overlap\": 0.5,\n",
    "  \"TTA\": True,\n",
    "  \"TH\": 0.5,\n",
    "\n",
    "  \"checkpoint_path\": \"/kaggle/input/focal-tversky-dasafe-epoch-150/pytorch/default/1/model_epoch_150.pth\", \n",
    "  \"dataset_path_imgs\": \"/kaggle/input\",\n",
    "  \"pred_save_dir\": \"/kaggle/working/\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c091343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any\n",
    "\n",
    "class BaseInfer(ABC):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.model = self._build_model()\n",
    "        self.criterion = self._set_evaluation_criterion()\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_model(self) -> Any:\n",
    "        \"\"\"Initialize the neural network architecture. Load pretrained weights if necessary.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _set_evaluation_criterion(self) -> Any:\n",
    "        \"\"\"Define the evaluation function.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def infer(self, input, **kwargs) -> Any:\n",
    "        \"\"\"Logic for inference. Returns predictions.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, predictions, gt, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Logic for evaluation. Takes predictions and ground truth as input.\n",
    "        Returns a dictionary of metrics. \n",
    "        Expected to be used after infer.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import sigmoid\n",
    "\n",
    "class BasicResBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(stride, int):\n",
    "            self.stride = (stride, stride, stride)\n",
    "        else:\n",
    "            self.stride = stride\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_c, out_c, kernel_size=3, stride=self.stride, padding=1)\n",
    "        self.norm1 = nn.InstanceNorm3d(out_c, affine=True, track_running_stats=False)\n",
    "        self.act1 = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(out_c, out_c, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm3d(out_c, affine=True, track_running_stats=False)\n",
    "        self.act2 = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        \n",
    "        self.conv3 = None\n",
    "        if self.stride != (1,1,1) or in_c != out_c:\n",
    "            self.conv3 = nn.Conv3d(in_c, out_c, kernel_size=1, stride=self.stride, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.act1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        \n",
    "        if self.conv3 is not None:\n",
    "            residual = self.conv3(x)\n",
    "        \n",
    "        out += residual\n",
    "        \n",
    "        # --- FIX: Activation MUST happen after addition ---\n",
    "        out = self.act2(out) \n",
    "        \n",
    "        return out\n",
    "\n",
    "class Upsample_Layer_nearest(nn.Module):\n",
    "    def __init__(self, in_c, out_c, scale_factor=(2,2,2)):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.conv = nn.Conv3d(in_c, out_c, kernel_size=1, stride=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, scale_factor=self.scale_factor, mode='nearest')\n",
    "        return self.conv(x)\n",
    "\n",
    "class STUNetReconstruction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- ENCODER ---\n",
    "        self.conv_blocks_context = nn.ModuleList([\n",
    "            nn.Sequential(BasicResBlock(1, 64, stride=1), BasicResBlock(64, 64)),\n",
    "            nn.Sequential(BasicResBlock(64, 128, stride=2), BasicResBlock(128, 128)),\n",
    "            nn.Sequential(BasicResBlock(128, 256, stride=2), BasicResBlock(256, 256)),\n",
    "            nn.Sequential(BasicResBlock(256, 512, stride=2), BasicResBlock(512, 512)),\n",
    "            nn.Sequential(BasicResBlock(512, 1024, stride=2), BasicResBlock(1024, 1024)),\n",
    "            nn.Sequential(BasicResBlock(1024, 1024, stride=(1,1,2)), BasicResBlock(1024, 1024)),\n",
    "        ])\n",
    "\n",
    "        # --- DECODER ---\n",
    "        self.upsample_layers = nn.ModuleList([\n",
    "            Upsample_Layer_nearest(1024, 1024, scale_factor=(1,1,2)), \n",
    "            Upsample_Layer_nearest(1024, 512, scale_factor=(2,2,2)),\n",
    "            Upsample_Layer_nearest(512, 256, scale_factor=(2,2,2)),\n",
    "            Upsample_Layer_nearest(256, 128, scale_factor=(2,2,2)),\n",
    "            Upsample_Layer_nearest(128, 64, scale_factor=(2,2,2)),\n",
    "        ])\n",
    "        \n",
    "        self.conv_blocks_localization = nn.ModuleList([\n",
    "            nn.Sequential(BasicResBlock(2048, 1024), BasicResBlock(1024, 1024)),\n",
    "            nn.Sequential(BasicResBlock(1024, 512), BasicResBlock(512, 512)),\n",
    "            nn.Sequential(BasicResBlock(512, 256), BasicResBlock(256, 256)),\n",
    "            nn.Sequential(BasicResBlock(256, 128), BasicResBlock(128, 128)),\n",
    "            nn.Sequential(BasicResBlock(128, 64), BasicResBlock(64, 64)),\n",
    "        ])\n",
    "\n",
    "        # --- OUTPUT ---\n",
    "        self.seg_outputs = nn.ModuleList([\n",
    "            nn.Identity(), nn.Identity(), nn.Identity(), nn.Identity(),\n",
    "            nn.Conv3d(64, 1, kernel_size=1, stride=1) \n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for i, block in enumerate(self.conv_blocks_context):\n",
    "            x = block(x)\n",
    "            if i < len(self.conv_blocks_context) - 1:\n",
    "                skips.append(x)\n",
    "                \n",
    "        for i in range(len(self.upsample_layers)):\n",
    "            x = self.upsample_layers[i](x)\n",
    "            skip = skips[-(i+1)]\n",
    "            if x.shape[2:] != skip.shape[2:]:\n",
    "                x = torch.nn.functional.interpolate(x, size=skip.shape[2:], mode='nearest')\n",
    "            x = torch.cat((x, skip), dim=1)\n",
    "            x = self.conv_blocks_localization[i](x)\n",
    "\n",
    "        return self.seg_outputs[4](x)\n",
    "\n",
    "\n",
    "class STUNetSegmentation(nn.Module):\n",
    "    def __init__(self, deep_supervision=True, activation=False):\n",
    "        super().__init__()\n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.activation = activation\n",
    "        # --- ENCODER ---\n",
    "        self.conv_blocks_context = nn.ModuleList([\n",
    "            nn.Sequential(BasicResBlock(1, 64, stride=1), BasicResBlock(64, 64)),\n",
    "            nn.Sequential(BasicResBlock(64, 128, stride=2), BasicResBlock(128, 128)),\n",
    "            nn.Sequential(BasicResBlock(128, 256, stride=2), BasicResBlock(256, 256)),\n",
    "            nn.Sequential(BasicResBlock(256, 512, stride=2), BasicResBlock(512, 512)),\n",
    "            nn.Sequential(BasicResBlock(512, 1024, stride=2), BasicResBlock(1024, 1024)),\n",
    "            nn.Sequential(BasicResBlock(1024, 1024, stride=(1,1,2)), BasicResBlock(1024, 1024)),\n",
    "        ])\n",
    "\n",
    "        # --- DECODER ---\n",
    "        self.upsample_layers = nn.ModuleList([\n",
    "            Upsample_Layer_nearest(1024, 1024, scale_factor=(1,1,2)), \n",
    "            Upsample_Layer_nearest(1024, 512, scale_factor=(2,2,2)),\n",
    "            Upsample_Layer_nearest(512, 256, scale_factor=(2,2,2)),\n",
    "            Upsample_Layer_nearest(256, 128, scale_factor=(2,2,2)),\n",
    "            Upsample_Layer_nearest(128, 64, scale_factor=(2,2,2)),\n",
    "        ])\n",
    "        \n",
    "        self.conv_blocks_localization = nn.ModuleList([\n",
    "            nn.Sequential(BasicResBlock(2048, 1024), BasicResBlock(1024, 1024)),\n",
    "            nn.Sequential(BasicResBlock(1024, 512), BasicResBlock(512, 512)),\n",
    "            nn.Sequential(BasicResBlock(512, 256), BasicResBlock(256, 256)),\n",
    "            nn.Sequential(BasicResBlock(256, 128), BasicResBlock(128, 128)),\n",
    "            nn.Sequential(BasicResBlock(128, 64), BasicResBlock(64, 64)),\n",
    "        ])\n",
    "\n",
    "        # --- OUTPUT ---\n",
    "        if deep_supervision:\n",
    "            print(f\"DOING DEEP SUPERVISION!\")\n",
    "            self.seg_outputs = nn.ModuleList([\n",
    "                #nn.Conv3d(1024, 1, kernel_size=1, stride=1), \n",
    "                #nn.Conv3d(512, 1, kernel_size=1, stride=1), \n",
    "                None, # ignoring the deepest two outputs for memory efficiency\n",
    "                None, # not using these outputs for deep supervision\n",
    "                nn.Conv3d(256, 1, kernel_size=1, stride=1), \n",
    "                nn.Conv3d(128, 1, kernel_size=1, stride=1),\n",
    "                nn.Conv3d(64, 1, kernel_size=1, stride=1) \n",
    "            ])\n",
    "        else:\n",
    "            self.seg_outputs = nn.ModuleList([\n",
    "                #nn.Conv3d(1024, 1, kernel_size=1, stride=1), \n",
    "                #nn.Conv3d(512, 1, kernel_size=1, stride=1), \n",
    "                None, # ignoring the deepest two outputs for memory efficiency\n",
    "                None, # not using these outputs for deep supervision\n",
    "                None, \n",
    "                None,\n",
    "                nn.Conv3d(64, 1, kernel_size=1, stride=1) \n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for i, block in enumerate(self.conv_blocks_context):\n",
    "            x = block(x)\n",
    "            if i < len(self.conv_blocks_context) - 1:\n",
    "                skips.append(x)\n",
    "        \n",
    "        deep_supervision_preds = []\n",
    "        for i in range(len(self.upsample_layers)):\n",
    "            x = self.upsample_layers[i](x)\n",
    "            skip = skips[-(i+1)]\n",
    "            if x.shape[2:] != skip.shape[2:]:\n",
    "                x = torch.nn.functional.interpolate(x, size=skip.shape[2:], mode='nearest')\n",
    "            x = torch.cat((x, skip), dim=1)\n",
    "            x = self.conv_blocks_localization[i](x)\n",
    "            if i>=2 and self.deep_supervision:  # Start collecting predictions from the 3rd decoder block\n",
    "                deep_supervision_preds.append(self.seg_outputs[i](x))\n",
    "        if not self.deep_supervision:\n",
    "            deep_supervision_preds.append(self.seg_outputs[-1](x))\n",
    "        if self.training:\n",
    "            # Return all scales for Deep Supervision Loss\n",
    "            # index 0 = low res, index -1 = high res\n",
    "            return deep_supervision_preds\n",
    "        else:\n",
    "            # Return only the final high-res prediction for validation/inference\n",
    "            if self.activation:\n",
    "                return sigmoid(deep_supervision_preds[-1])\n",
    "            else:\n",
    "                return deep_supervision_preds[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cdbc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.amp import autocast\n",
    "\n",
    "import monai\n",
    "from monai.transforms.transform import MapTransform, Transform\n",
    "from monai.data import CacheDataset\n",
    "from monai.metrics import DiceHelper\n",
    "from monai.losses import DiceLoss, TverskyLoss, FocalLoss\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    CopyItemsd,\n",
    "    LoadImaged, \n",
    "    ScaleIntensityRanged, \n",
    "    ResizeWithPadOrCropd, \n",
    "    RandCoarseDropoutd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    Resized,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCropByPosNegLabeld\n",
    ")\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "\n",
    "import tifffile  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # ðŸš€ Import tqdm\n",
    "import itertools\n",
    "from monai.transforms.transform import MapTransform, Transform\n",
    "\n",
    "class GetROIMaskdd(MapTransform):\n",
    "    \"\"\"\n",
    "    Create a ROI mask from the ground truth by setting to 0 the regions with ignore_mask_value.\n",
    "    The ROI mask will have 1 where the ground truth is not equal to ignore_mask_value, and 0 elsewhere.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys, ignore_mask_value=2, new_key_names=None):\n",
    "        self.keys = keys\n",
    "        self.ignore_mask_value = ignore_mask_value\n",
    "        self.new_key_names = new_key_names\n",
    "\n",
    "    def __call__(self, data):\n",
    "        for key, new_key in zip(self.keys, self.new_key_names):\n",
    "            gt = data[key]\n",
    "            roi_mask = (gt != self.ignore_mask_value).float()\n",
    "            data[new_key] = roi_mask\n",
    "        return data\n",
    "\n",
    "class GetBinaryLabeld(MapTransform):\n",
    "    def __init__(self, keys, ignore_mask_value=2):\n",
    "        super().__init__(keys)\n",
    "        self.ignore_mask_value = ignore_mask_value\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            val = d[key]\n",
    "            # 1. Use a small epsilon for the ignore value check (safety)\n",
    "            # This handles values like 1.999 or 2.0000153\n",
    "            mask = (val > (self.ignore_mask_value - 0.1)) & (val < (self.ignore_mask_value + 0.1))\n",
    "            val[mask] = 0\n",
    "            \n",
    "            # 2. FORCE the remaining values into strictly 0 or 1\n",
    "            # Anything that isn't background (0) should be 1\n",
    "            # This fixes the 1.0000153 issue\n",
    "            val[val > 0.5] = 1.0\n",
    "            val[val <= 0.5] = 0.0\n",
    "            \n",
    "            d[key] = val\n",
    "        return d\n",
    "\n",
    "\n",
    "class VesuviusInferer(BaseInfer):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = self._build_model()\n",
    "        self.criterion = self._set_evaluation_criterion()\n",
    "        self.val_transforms = self._set_val_transforms()\n",
    "        self.test_transforms = self._set_test_transforms()\n",
    "        self.sliding_window = self._set_sliding_window_inferer()\n",
    "\n",
    "        self.axes_combinations = self._define_flip_tta(spatial_dims=(2, 3, 4))\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Initialize the neural network architecture. Load pretrained weights if necessary.\"\"\"\n",
    "        print(f\"Loading weights from: {self.config['checkpoint_path']}\")\n",
    "        \n",
    "        # Initialize the new Segmentation Model (1 output channels)\n",
    "        model = STUNetSegmentation(self.config['deep_supervision'], activation=self.config['activation'])\n",
    "        \n",
    "        # Load trained Weights (1 output channel)\n",
    "        checkpoint_state_dict = torch.load(self.config['checkpoint_path'], map_location='cpu')\n",
    "        checkpoint_state_dict = checkpoint_state_dict['model_weights']\n",
    "        load_result = model.load_state_dict(checkpoint_state_dict, strict=True)\n",
    "        \n",
    "        # Move to GPU\n",
    "        model = model.to(self.config['device'])\n",
    "        return model\n",
    "\n",
    "    def _set_evaluation_criterion(self):\n",
    "        \"\"\"Define the evaluation function.\"\"\"\n",
    "        val_metric = DiceHelper\n",
    "        \n",
    "        def compute_masked_val(pred, target, roi_mask):\n",
    "            pred_masked = pred * roi_mask\n",
    "            pred_masked = (pred_masked > 0.5).float()\n",
    "            # check if empty\n",
    "            if torch.sum(pred_masked) == 0:\n",
    "                # If both pred and target are empty in the ROI, return Dice of 1.0\n",
    "                if torch.sum(target * roi_mask) == 0:\n",
    "                    return torch.tensor(1.0, device=target.device, dtype=torch.float32)\n",
    "                else:\n",
    "                    return torch.tensor(0.0, device=target.device, dtype=torch.float32)\n",
    "            target_masked = target * roi_mask\n",
    "            results = val_metric(include_background=True, sigmoid=False, softmax=False)(pred_masked, target_masked)\n",
    "            return results\n",
    "        return compute_masked_val\n",
    "        \n",
    "    def save_nifti(self, torch_tensor, save_path, real_file, **kwargs):\n",
    "        \"\"\"Logic for saving predictions to disk.\"\"\"\n",
    "        # Ensure tensor is 3D (H, W, D) or (H, W)\n",
    "        data = torch_tensor.detach().cpu().numpy().astype('float32')\n",
    "        while data.ndim > 3:\n",
    "            data = np.squeeze(data, axis=0)\n",
    "\n",
    "        if real_file is not None:\n",
    "            # Load the real file to get metadata\n",
    "            real_nii = nib.load(real_file)\n",
    "            real_affine = real_nii.affine\n",
    "            real_header = real_nii.header\n",
    "            pred_nii = nib.Nifti1Image(data, affine=real_affine, header=real_header)\n",
    "        else:\n",
    "            # If no real file is provided, use identity affine\n",
    "            affine = np.eye(4)\n",
    "            pred_nii = nib.Nifti1Image(data, affine=affine)\n",
    "\n",
    "        # Save the Nifti1Image to disk\n",
    "        nib.save(pred_nii, save_path)\n",
    "    \n",
    "    def save_tiff(self, torch_tensor, save_path, **kwargs):\n",
    "        \"\"\"Logic for saving predictions to disk.\"\"\"\n",
    "        # Ensure tensor is 3D (H, W, D) or (H, W)\n",
    "        data = torch_tensor.detach().cpu().numpy().astype('uint8')\n",
    "        while data.ndim > 3:\n",
    "            data = np.squeeze(data, axis=0)\n",
    "        data = np.transpose(data, (2, 1, 0))\n",
    "        \n",
    "        # If the data is a probability map (0-1), \n",
    "        # many Vesuvius tools expect uint16 or uint8\n",
    "        # Convert if necessary: (data * 255).astype('uint8')\n",
    "        tifffile.imwrite(save_path, data)\n",
    "\n",
    "    def _set_dataloader_transforms(self, gt_available, **kwargs):\n",
    "        \"\"\"Define the data loading and augmentation transforms.\"\"\"\n",
    "\n",
    "        if gt_available:\n",
    "            general_keys = [\"image\", 'gt']\n",
    "            all_keys = [\"image\", 'gt', 'roi_mask']\n",
    "        else:\n",
    "            general_keys = [\"image\"]\n",
    "            all_keys = [\"image\"]\n",
    "\n",
    "        transform_list = [\n",
    "                LoadImaged(keys=general_keys),\n",
    "                EnsureChannelFirstd(keys=general_keys),\n",
    "                # Normalize uint8 input\n",
    "                ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=255, b_min=0, b_max=1, clip=True),\n",
    "            ]\n",
    "        \n",
    "        if gt_available:\n",
    "            transform_list.append(GetROIMaskdd(keys=[\"gt\"], ignore_mask_value=2, new_key_names=[\"roi_mask\"]))\n",
    "            transform_list.append(GetBinaryLabeld(keys=[\"gt\"], ignore_mask_value=2))\n",
    "\n",
    "        # Get random patches\n",
    "        transform_list.append(EnsureTyped(keys=all_keys, track_meta=False))\n",
    "\n",
    "        return transform_list\n",
    "    \n",
    "    def _set_val_transforms(self, **kwargs):\n",
    "        \"\"\"Define the validation data loading transforms.\"\"\"\n",
    "        transform_list = self._set_dataloader_transforms(gt_available=True, **kwargs)\n",
    "        return Compose(transform_list)\n",
    "\n",
    "    def _set_test_transforms(self, **kwargs):   \n",
    "        \"\"\"Define the test data loading transforms.\"\"\"\n",
    "        transform_list = self._set_dataloader_transforms(gt_available=False, **kwargs)\n",
    "        return Compose(transform_list)\n",
    "\n",
    "    def load_input_data(self, file_dict, transforms, **kwargs):\n",
    "        '''\n",
    "        Monai data loading logic. It must handle tiff files.\n",
    "        '''\n",
    "        transformed_data = transforms(file_dict)\n",
    "        return transformed_data\n",
    "    \n",
    "    def _set_sliding_window_inferer(self, **kwargs):\n",
    "        \"\"\"Define the sliding window inferer.\"\"\"\n",
    "        \n",
    "        inferer = monai.inferers.SlidingWindowInferer(\n",
    "            roi_size=self.config['patch_size'],\n",
    "            sw_batch_size=self.config['infer_sw_batch_size'],\n",
    "            overlap=self.config['infer_overlap'],\n",
    "            mode=self.config['blend_mode'],\n",
    "            padding_mode='constant',\n",
    "            cval=0.0,\n",
    "            device=self.config['device'],\n",
    "            progress=True\n",
    "        )\n",
    "\n",
    "        return inferer\n",
    "\n",
    "    def _define_flip_tta(self, spatial_dims=(2, 3, 4)):\n",
    "        \"\"\"\n",
    "        # All combinations possble\n",
    "        axes_combinations = []\n",
    "        for i in range(len(spatial_dims) + 1):\n",
    "            axes_combinations.extend(itertools.combinations(spatial_dims, i))\n",
    "        return axes_combinations\n",
    "        \"\"\"\n",
    "        # ignore the dim=2 (z axis)\n",
    "        return [(), (3,), (4,), (3, 4)]\n",
    "    def infer(self, input, test=False, threshold=0.5, **kwargs):\n",
    "        \"\"\"Logic for inference. Returns predictions.\"\"\"\n",
    "        if test:\n",
    "            data = self.load_input_data(file_dict=input, transforms=self.test_transforms)\n",
    "        else:\n",
    "            data = self.load_input_data(file_dict=input, transforms=self.val_transforms)\n",
    "\n",
    "        input_image = data['image'].unsqueeze(0).to(self.config['device'])\n",
    "\n",
    "        self.model.eval()\n",
    "        \n",
    "        # TTA\n",
    "        if self.config[\"TTA\"]:\n",
    "            print(f\"Doing inference to these axes combinations:\")\n",
    "            print(self.axes_combinations)\n",
    "            all_predictions = []\n",
    "            all_logits_pred = []\n",
    "            for axes in self.axes_combinations:\n",
    "                print(f\"Doing axes: {axes}\")\n",
    "                if axes:\n",
    "                    # torch.flip requires a tuple of dims\n",
    "                    aug_input = torch.flip(input_image, dims=axes)\n",
    "                else:\n",
    "                    aug_input = input_image  # Original image\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    with autocast(device_type=self.config['device']):\n",
    "                        if self.config['activation']:\n",
    "                            prediction = self.sliding_window(inputs=aug_input, network=self.model)\n",
    "                        else: # in case sigmoid is not applied in the end of the network (recommended to be True!)\n",
    "                            logits_pred = self.sliding_window(inputs=aug_input, network=self.model)\n",
    "                            prediction = sigmoid(logits_pred)\n",
    "                    \n",
    "                # revert flip\n",
    "                if axes:\n",
    "                    pred_prob_aligned = torch.flip(prediction, dims=axes)\n",
    "                    if not self.config['activation']:\n",
    "                        logits_pred = torch.flip(logits_pred, dims=axes)\n",
    "                else:\n",
    "                    pred_prob_aligned = prediction\n",
    "\n",
    "                all_predictions.append(pred_prob_aligned)\n",
    "                if not self.config['activation']:\n",
    "                    all_logits_pred.append(logits_pred)\n",
    "\n",
    "            final_pred = torch.stack(all_predictions).mean(dim=0)\n",
    "            if not self.config['activation']:\n",
    "                all_logits_pred = torch.stack(all_logits_pred).mean(dim=0)\n",
    "                \n",
    "        else:\n",
    "            if self.config['activation']:\n",
    "                with torch.no_grad():\n",
    "                    with autocast(device_type=self.config['device']):\n",
    "                        final_pred = self.sliding_window(inputs=input_image, network=self.model)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    with autocast(device_type=self.config['device']):\n",
    "                        logits_pred = self.sliding_window(inputs=input_image, network=self.model)\n",
    "                        final_pred = sigmoid(logits_pred)\n",
    "                        all_logits_pred = logits_pred\n",
    "        # Binary seg\n",
    "        final_pred[final_pred>threshold] = 1.0\n",
    "        final_pred[final_pred<=threshold] = 0.0\n",
    "\n",
    "        if test:\n",
    "            if self.config['activation']:\n",
    "                return None, final_pred\n",
    "            else:\n",
    "                return all_logits_pred, final_pred\n",
    "        else:\n",
    "            if self.config['activation']:\n",
    "                return final_pred, data['gt'].unsqueeze(0).to(self.config['device']), data['roi_mask'].unsqueeze(0).to(self.config['device']) \n",
    "            else:\n",
    "                return all_logits_pred, final_pred, data['gt'].unsqueeze(0).to(self.config['device']), data['roi_mask'].unsqueeze(0).to(self.config['device']) \n",
    "\n",
    "    def create_dfs(self, path_dir):\n",
    "        # Generate DataFrame\n",
    "        result_df = glob.glob(os.path.join(path_dir, '**/*.tif'), recursive=True)\n",
    "        result_df = pd.DataFrame({'tif_paths': result_df})\n",
    "        result_df['id'] = result_df['tif_paths'].apply(lambda x: os.path.basename(x).split('.')[0])\n",
    "\n",
    "        # save dataframe to csv (take name from the path_dir last folder)\n",
    "        csv_name = os.path.basename(os.path.normpath(path_dir)) + '_df.csv'\n",
    "        result_df.to_csv(os.path.join(path_dir, csv_name), index=False)\n",
    "        return result_df\n",
    "\n",
    "    def evaluate(self, predictions, gt, roi_mask, **kwargs):\n",
    "        \"\"\"\n",
    "        Logic for evaluation. Takes predictions and ground truth as input.\n",
    "        Returns a dictionary of metrics. \n",
    "        Expected to be used after infer.\n",
    "        \"\"\"\n",
    "        results = self.criterion(predictions, gt, roi_mask)\n",
    "        return results\n",
    "    \n",
    "    def dataset_inference_save_logits(self, dataset_path, pred_save_dir):\n",
    "        \"\"\" inference on all cases in a dataset directory and save predictions \"\"\"\n",
    "        os.makedirs(pred_save_dir, exist_ok=True)\n",
    "        tif_files = glob.glob(os.path.join(dataset_path, '**/*.tif'), recursive=True)\n",
    "        nii_files = glob.glob(os.path.join(dataset_path, '**/*.nii.gz'), recursive=True)\n",
    "\n",
    "        all_cases = tif_files + nii_files\n",
    "\n",
    "        print(f\"Found cases: {all_cases}\")\n",
    "        \n",
    "        # ðŸ Wrap the list in tqdm for a visual progress bar\n",
    "        # 'desc' adds a label to the bar, 'unit' labels each iteration\n",
    "        for case in tqdm(all_cases, desc=\"ðŸŒ‹ Running Vesuvius Inference\", unit=\"vol\"):\n",
    "            # Optional: you can still print the case, but it might \"flicker\" the bar. \n",
    "            # Using tqdm.write() keeps the bar at the bottom.\n",
    "            # tqdm.write(f\"ðŸ” Processing: {os.path.basename(case)}\")\n",
    "            \n",
    "            input_data = {\n",
    "                'image': str(case),\n",
    "                'gt': None\n",
    "            }\n",
    "            \n",
    "            logits_pred, pred = self.infer(input_data, test=True)\n",
    "            \n",
    "            # ðŸ’¾ Robust filename extraction\n",
    "            file_name = os.path.basename(case).replace('.tif', '')\n",
    "            file_name = os.path.basename(case).replace('.nii.gz', '')\n",
    "            save_path = os.path.join(pred_save_dir, f\"{file_name}.nii.gz\")\n",
    "            \n",
    "            self.save_nifti(\n",
    "                torch_tensor=logits_pred, \n",
    "                save_path=save_path, \n",
    "                real_file=str(case)\n",
    "            )\n",
    "\n",
    "        print(f\"âœ… Inference completed. Predictions saved to: {pred_save_dir}\")\n",
    "\n",
    "    def dataset_inference(self, dataset_path, pred_save_dir):\n",
    "        \"\"\" inference on all cases in a dataset directory and save predictions \"\"\"\n",
    "        os.makedirs(pred_save_dir, exist_ok=True)\n",
    "        all_cases = glob.glob(os.path.join(dataset_path, '**/*.tif'), recursive=True)\n",
    "        \n",
    "        # ðŸ Wrap the list in tqdm for a visual progress bar\n",
    "        # 'desc' adds a label to the bar, 'unit' labels each iteration\n",
    "        for case in tqdm(all_cases, desc=\"ðŸŒ‹ Running Vesuvius Inference\", unit=\"vol\"):\n",
    "            # Optional: you can still print the case, but it might \"flicker\" the bar. \n",
    "            # Using tqdm.write() keeps the bar at the bottom.\n",
    "            # tqdm.write(f\"ðŸ” Processing: {os.path.basename(case)}\")\n",
    "            \n",
    "            input_data = {\n",
    "                'image': str(case),\n",
    "                'gt': None\n",
    "            }\n",
    "\n",
    "            # ðŸ’¾ Robust filename extraction\n",
    "            file_name = os.path.basename(case).replace('.tif', '')\n",
    "            save_path = os.path.join(pred_save_dir, f\"{file_name}.tif\")\n",
    "            if os.path.isfile(save_path):\n",
    "                print(f\"Case already predicted: {save_path}\")\n",
    "            else:\n",
    "                # Inference\n",
    "                logits_pred, pred = self.infer(input_data, test=True, threshold=self.config[\"TH\"])\n",
    "            \n",
    "            self.save_tiff(\n",
    "                torch_tensor=pred, \n",
    "                save_path=save_path\n",
    "            )\n",
    "\n",
    "        # ðŸ“Š Creating the dataframe for testing\n",
    "        print(\"\\nðŸ“ Generating submission dataframes...\")\n",
    "        self.create_dfs(pred_save_dir)\n",
    "        print(f\"âœ… Inference completed. Predictions saved to: {pred_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "# Competition\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "###################################################\n",
    "################ Inference process ################\n",
    "print(f\"ðŸš€ Starting inference on dataset: {config_content['dataset_path_imgs']}\")\n",
    "print(\"ðŸ“‹ Using the following configuration:\")\n",
    "for key, value in config_content.items():\n",
    "    print(f\"  ðŸ”¹ {key}: {value}\")\n",
    "\n",
    "# ðŸ§ª Initialize Inference Object\n",
    "infer_object = VesuviusInferer(config_content)\n",
    "pred_save_dir = config_content[\"pred_save_dir\"]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for dirname, _, filenames in os.walk(config_content[\"dataset_path_imgs\"]):\n",
    "    for filename in filenames:\n",
    "        case = os.path.join(dirname, filename)\n",
    "        print(f\"Doing case: {case}\")\n",
    "        \n",
    "        input_data = {\n",
    "                'image': str(case),\n",
    "                'gt': None\n",
    "            }\n",
    "        \n",
    "        logits_pred, pred = infer_object.infer(input_data, test=True, threshold=config_content[\"TH\"])\n",
    "\n",
    "        # ðŸ’¾ Robust filename extraction\n",
    "        file_name = os.path.basename(case).replace('.tif', '')\n",
    "        save_path = os.path.join(pred_save_dir, f\"{file_name}.tif\")\n",
    "        infer_object.save_tiff(\n",
    "                        torch_tensor=pred, \n",
    "                        save_path=save_path\n",
    "                    )\n",
    "        print(f\"âœ… Inference completed. Predictions saved to: {pred_save_dir}\")\n",
    "\n",
    "print(\"ðŸŽ‰ Results saved! ðŸ†\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
