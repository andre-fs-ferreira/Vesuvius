{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307f0d24",
   "metadata": {},
   "source": [
    "# Develop a light GAN for post processing\n",
    "* The input must be the biggest size in the dataset\n",
    "* Both Generator and Discriminator must fit in the GPU (48GB VRAM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ecb6d",
   "metadata": {},
   "source": [
    "### Check biggest (384,384,384 although there is only 1 case with this shape...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the bigest shape\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "all_shapes = []\n",
    "root_dataset = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/Challenge_dataset_updated/train_labels_nii_crop\"\n",
    "for file_name in os.listdir(root_dataset):\n",
    "    if file_name.endswith('.nii.gz'):\n",
    "        file_path = os.path.join(root_dataset, file_name)\n",
    "        img_array = nib.load(file_path).get_fdata()\n",
    "        all_shapes.append(img_array.shape)\n",
    "        #if img_array.shape[0]>320:\n",
    "        print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_shapes = [shape_element[0] for shape_element in all_shapes]\n",
    "y_all_shapes = [shape_element[1] for shape_element in all_shapes]\n",
    "z_all_shapes = [shape_element[2] for shape_element in all_shapes]\n",
    "\n",
    "print(f\"Biggest x: {max(x_all_shapes)}\")\n",
    "print(f\"Biggest y: {max(y_all_shapes)}\")\n",
    "print(f\"Biggest z: {max(z_all_shapes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59e1f1",
   "metadata": {},
   "source": [
    "### Create MedNeXt network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409d74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint # Crucial for 48GB VRAM optimization\n",
    "\n",
    "# --- 1. Modern Components ---\n",
    "\n",
    "class LayerNorm3d(nn.Module):\n",
    "    \"\"\" Modern LayerNorm that supports Channels First (N, C, D, H, W) \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        return self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
    "\n",
    "class GRN(nn.Module):\n",
    "    \"\"\" \n",
    "    Global Response Normalization (The V2 Secret Weapon)\n",
    "    Prevents \"dead channels\" in sparse data by normalizing feature competition.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [N, D, H, W, C] (Channels Last for efficiency)\n",
    "        Gx = torch.norm(x, p=2, dim=(1,2,3), keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return self.gamma * (x * Nx) + self.beta + x\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\" Stochastic Depth: Randomly drops paths to prevent overfitting in deep models \"\"\"\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_() \n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "# --- 2. The Block ---\n",
    "\n",
    "class ConvNeXtV2Block(nn.Module):\n",
    "    def __init__(self, dim, dilation=1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A. Depthwise Conv (Spatial Context)\n",
    "        # padding=dilation ensures we maintain 1:1 resolution (No shrinking!)\n",
    "        self.dwconv = nn.Conv3d(dim, dim, kernel_size=3, padding=dilation, \n",
    "                                groups=dim, dilation=dilation) \n",
    "        self.norm = LayerNorm3d(dim, eps=1e-6)\n",
    "        \n",
    "        # B. Pointwise MLP (Channel Reasoning)\n",
    "        # Inverted Bottleneck: Expand 1 -> 4 -> 1\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) \n",
    "        self.act = nn.GELU()\n",
    "        self.grn = GRN(4 * dim) # The V2 upgrade\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        \n",
    "        # 1. Spatial Processing\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # 2. Permute to Channels Last (N, D, H, W, C) for Linear Layers\n",
    "        x = x.permute(0, 2, 3, 4, 1) \n",
    "        \n",
    "        # 3. MLP with GRN\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        \n",
    "        # 4. Restore Layout\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        # 5. Residual Connection\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "# --- 3. The Network ---\n",
    "\n",
    "class RefinerNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=2, base_dim=96, depth=18, use_checkpoint=True):\n",
    "        super().__init__()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        \n",
    "        # 1. Stem: Project [Scan + Mask] into Feature Space\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, base_dim, kernel_size=3, padding=1),\n",
    "            LayerNorm3d(base_dim)\n",
    "        )\n",
    "        \n",
    "        # 2. Deep Body with \"Sawtooth\" Dilation\n",
    "        # This pattern expands the receptive field to >60 pixels to see across gaps\n",
    "        # Pattern: 1, 2, 4, 8, 4, 2 ...\n",
    "        base_dilations = [1, 2, 4, 8, 4, 2]\n",
    "        self.blocks = nn.ModuleList()\n",
    "        \n",
    "        # Stochastic depth decay (linear)\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, 0.2, depth)] \n",
    "        \n",
    "        for i in range(depth):\n",
    "            d = base_dilations[i % len(base_dilations)]\n",
    "            self.blocks.append(ConvNeXtV2Block(base_dim, dilation=d, drop_path=dp_rates[i]))\n",
    "            \n",
    "        # 3. Head: Project back to 1 channel mask\n",
    "        self.head = nn.Conv3d(base_dim, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, 2, D, H, W]\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            if self.use_checkpoint and self.training:\n",
    "                # Trades compute for VRAM, allowing Depth=18 on 48GB GPU\n",
    "                x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block(x)\n",
    "                \n",
    "        return self.head(x)\n",
    "\n",
    "# --- 4. Instantiation ---\n",
    "def get_refiner():\n",
    "    # in_channels=2: Channel 0 is Original Scan, Channel 1 is Broken Prediction\n",
    "    # proposed refinerNetwork = RefinerNetwork(in_channels=2, base_dim=96, depth=18, use_checkpoint=True)\n",
    "    refinerNetwork = RefinerNetwork(in_channels=2, base_dim=16, depth=18, use_checkpoint=True)\n",
    "    return refinerNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5449c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Running Benchmark on: NVIDIA GeForce RTX 4060 Ti\n",
      "Config: (1, 2, 128, 128, 128) | Training Mode: True\n",
      "============================================================\n",
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "============================================================================================================================================\n",
      "RefinerNetwork                           [1, 2, 128, 128, 128]     [1, 1, 128, 128, 128]     --                        --\n",
      "├─Sequential: 1-1                        [1, 2, 128, 128, 128]     [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─Conv3d: 2-1                       [1, 2, 128, 128, 128]     [1, 16, 128, 128, 128]    880                       [3, 3, 3]\n",
      "│    └─LayerNorm3d: 2-2                  [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "├─ModuleList: 1-2                        --                        --                        --                        --\n",
      "│    └─ConvNeXtV2Block: 2-3              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-1                  [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-2             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-3                  [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-4                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-5                     [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-6                  [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─Identity: 3-7                [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-4              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-8                  [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-9             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-10                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-11                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-12                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-13                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-14               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-5              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-15                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-16            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-17                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-18                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-19                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-20                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-21               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-6              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-22                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-23            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-24                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-25                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-26                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-27                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-28               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-7              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-29                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-30            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-31                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-32                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-33                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-34                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-35               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-8              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-36                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-37            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-38                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-39                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-40                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-41                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-42               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-9              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-43                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-44            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-45                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-46                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-47                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-48                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-49               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-10             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-50                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-51            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-52                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-53                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-54                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-55                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-56               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-11             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-57                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-58            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-59                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-60                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-61                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-62                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-63               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-12             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-64                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-65            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-66                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-67                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-68                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-69                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-70               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-13             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-71                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-72            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-73                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-74                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-75                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-76                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-77               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-14             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-78                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-79            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-80                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-81                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-82                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-83                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-84               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-15             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-85                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-86            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-87                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-88                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-89                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-90                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-91               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-16             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-92                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-93            [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-94                 [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-95                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-96                    [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-97                 [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-98               [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-17             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-99                 [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-100           [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-101                [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-102                  [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-103                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-104                [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-105              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-18             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-106                [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-107           [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-108                [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-109                  [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-110                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-111                [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-112              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-19             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-113                [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-114           [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-115                [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-116                  [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-117                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-118                [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-119              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    └─ConvNeXtV2Block: 2-20             [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "│    │    └─Conv3d: 3-120                [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    448                       [3, 3, 3]\n",
      "│    │    └─LayerNorm3d: 3-121           [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    32                        --\n",
      "│    │    └─Linear: 3-122                [1, 128, 128, 128, 16]    [1, 128, 128, 128, 64]    1,088                     --\n",
      "│    │    └─GELU: 3-123                  [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    --                        --\n",
      "│    │    └─GRN: 3-124                   [1, 128, 128, 128, 64]    [1, 128, 128, 128, 64]    2                         --\n",
      "│    │    └─Linear: 3-125                [1, 128, 128, 128, 64]    [1, 128, 128, 128, 16]    1,040                     --\n",
      "│    │    └─DropPath: 3-126              [1, 16, 128, 128, 128]    [1, 16, 128, 128, 128]    --                        --\n",
      "├─Conv3d: 1-3                            [1, 16, 128, 128, 128]    [1, 1, 128, 128, 128]     17                        [1, 1, 1]\n",
      "============================================================================================================================================\n",
      "Total params: 47,909\n",
      "Trainable params: 47,909\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 18.79\n",
      "============================================================================================================================================\n",
      "Input size (MB): 16.78\n",
      "Forward/backward pass size (MB): 53703.87\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 53720.84\n",
      "============================================================================================================================================\n",
      "\n",
      "Warming up GPU...\n",
      "Measuring VRAM usage...\n",
      "Peak VRAM Usage: 6.33 GB\n",
      "Measuring Speed (50 steps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30042/2380928521.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() # Optional: simulate AMP overhead if needed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time per Step: 4071.90 ms\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def profile_model(model, input_shape=(1, 2, 128, 128, 128), device='cuda', train_mode=True):\n",
    "    \"\"\"\n",
    "    Prints model summary, measures VRAM usage and Speed.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        input_shape: Tuple of input shape (N, C, D, H, W)\n",
    "        device: 'cuda' or 'cpu'\n",
    "        train_mode: If True, measures backward pass memory/time too.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Running Benchmark on: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Config: {input_shape} | Training Mode: {train_mode}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Print Model Structure (using torchinfo if available)\n",
    "    try:\n",
    "        from torchinfo import summary\n",
    "        print(summary(model, input_size=input_shape, depth=3, \n",
    "                      col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"]))\n",
    "    except ImportError:\n",
    "        print(\"[Info] Install 'torchinfo' for a prettier model summary.\")\n",
    "        print(f\"Total Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # 2. Setup\n",
    "    model.to(device)\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(input_shape).to(device)\n",
    "    dummy_target = torch.randn((input_shape[0], 1, input_shape[2], input_shape[3], input_shape[4])).to(device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # 3. Warmup (Stabilize GPU clock speeds)\n",
    "    print(\"\\nWarming up GPU...\")\n",
    "    for _ in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(dummy_input)\n",
    "        if train_mode:\n",
    "            loss = criterion(out, dummy_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 4. Measure VRAM (Peak Allocation)\n",
    "    print(\"Measuring VRAM usage...\")\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Run one step\n",
    "    start_vram = torch.cuda.memory_allocated()\n",
    "    out = model(dummy_input)\n",
    "    if train_mode:\n",
    "        loss = criterion(out, dummy_target)\n",
    "        loss.backward()\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated()\n",
    "    memory_gb = peak_memory / (1024 ** 3)\n",
    "    \n",
    "    print(f\"Peak VRAM Usage: {memory_gb:.2f} GB\")\n",
    "\n",
    "    # 5. Measure Speed (Average over 50 steps)\n",
    "    print(\"Measuring Speed (50 steps)...\")\n",
    "    timings = []\n",
    "    scaler = torch.cuda.amp.GradScaler() # Optional: simulate AMP overhead if needed\n",
    "    \n",
    "    # Using CUDA Events for precise timing\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    for _ in range(50):\n",
    "        optimizer.zero_grad()\n",
    "        start_event.record()\n",
    "        \n",
    "        out = model(dummy_input)\n",
    "        if train_mode:\n",
    "            loss = criterion(out, dummy_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        timings.append(start_event.elapsed_time(end_event)) # Returns milliseconds\n",
    "\n",
    "    avg_time_ms = sum(timings) / len(timings)\n",
    "    print(f\"Average Time per Step: {avg_time_ms:.2f} ms\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# --- 3. EXECUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Error: CUDA is not available. This benchmark requires a GPU.\")\n",
    "    else:\n",
    "        # Instantiate your specific configuration\n",
    "        # NOTE: use_checkpoint=True saves memory but adds ~30% time overhead\n",
    "        model = RefinerNetwork(in_channels=2, base_dim=16, depth=18, use_checkpoint=True)\n",
    "        \n",
    "        # Test with a standard Vesuvius crop size\n",
    "        # Change batch_size (1st dim) to see how much your 48GB card can handle!\n",
    "        profile_model(model, input_shape=(1, 2, 128, 128, 128), train_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69b6e0",
   "metadata": {},
   "source": [
    "### Building the training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc2198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "from os.path import join\n",
    "sys.path.append(\"../utils\")\n",
    "from main_train_class import main_train_STU_Net\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import sigmoid, binary_cross_entropy_with_logits\n",
    "# Standard Library Imports\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import sigmoid\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.nn import MSELoss\n",
    "# MONAI Specific Imports\n",
    "import monai\n",
    "from monai.data import CacheDataset\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    CopyItemsd,\n",
    "    LoadImaged, \n",
    "    ScaleIntensityRanged, \n",
    "    ResizeWithPadOrCropd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd\n",
    ")\n",
    "from mask_utils import GetROIMaskdd, GetBinaryLabeld\n",
    "# Local Project Imports\n",
    "\n",
    "class postprocessConvNeXt(main_train_STU_Net):\n",
    "    def __init__(self, config):\n",
    "        # TODO predict and save logits from the segmentation network\n",
    "        self.config = config\n",
    "        self.labda_seg = self.config['labda_seg']\n",
    "        self.G_model = self._build_models()\n",
    "        self.train_loader = self._set_train_dataloader() \n",
    "        self.val_loader = self._set_val_dataloader() \n",
    "        self.opt_G = self._set_optimizers()\n",
    "        self.wandb_run = self._set_wandb_checkpoint() # Heritage\n",
    "        # Expects lists of predictions # TODO put predictions inside of a list\n",
    "        self.G_voxel_criterion = self._set_train_criterion() # Heritage\n",
    "        \n",
    "        self.val_metric = self._set_val_metric() # Heritage\n",
    "        self.G_cosAnnealLR = self._set_scheduler()\n",
    "\n",
    "        # set scaler for mix precision \n",
    "        self.G_scaler = GradScaler()\n",
    "        \n",
    "        # check resume\n",
    "        self._resume()\n",
    "\n",
    "    def _build_models(self):\n",
    "        # TODO change network to base_dim=96 depth=18\n",
    "        model = RefinerNetwork(in_channels=2, base_dim=4, depth=9, use_checkpoint=True).to(self.config['device'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _resume(self):\n",
    "        if self.config.get('resume'):\n",
    "            G_checkpoint = torch.load(self.config['resume'], map_location=\"cpu\", weights_only=False) \n",
    "            G_model_weights = G_checkpoint['model_weights']  \n",
    "            self.G_model.load_state_dict(G_model_weights, strict=True)\n",
    "            self.G_model = self.G_model.to(self.config['device'])\n",
    "            # optimizer load\n",
    "            self.opt_G.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
    "            # parameters\n",
    "            self.start_epoch = G_checkpoint['epoch'] + 1 # To continue to the next epoch instead of repeating  \n",
    "            self.val_value = G_checkpoint['val_value']\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            self.val_value = 0\n",
    "\n",
    "    def _set_train_dataloader(self):\n",
    "        \"\"\" Getting the list of cases for training and loading using MONAI (all into memory)\"\"\"\n",
    "        data_list = []\n",
    "        # TODO: change back to this data list\n",
    "        #with open(self.config['data_split_json'], \"r\") as f:\n",
    "        #    split = json.load(f)\n",
    "        #train_cases = split[\"train\"]\n",
    "        #\n",
    "        train_cases = [\"2290837.nii.gz\"]\n",
    "        for train_case in train_cases:\n",
    "            complete_data_dict = {}\n",
    "            complete_data_dict[\"image\"] = join(self.config['vol_data_path'], train_case)\n",
    "            complete_data_dict[\"gt\"] = join(self.config['label_data_path'], train_case)\n",
    "            complete_data_dict[\"bridge_weight_map\"] = join(self.config['bridge_weight_map_path'], train_case)\n",
    "            # TODO in the json the pred_seg_logits needs to be changed!\n",
    "            complete_data_dict[\"pred_seg_logits\"] = join(self.config['pred_seg_logits'], train_case)\n",
    "            data_list.append(complete_data_dict)\n",
    "            \n",
    "            if self.config['debug']:\n",
    "                for i in range(30):\n",
    "                    data_list.append(complete_data_dict)\n",
    "                print(f\"training using case: {data_list[0]}\")\n",
    "                break  # repeat 30 cases for debug mode\n",
    "\n",
    "        print(f\"Train cases: {len(train_cases)}\")\n",
    "        print(f\"Some examples:\")\n",
    "        print(train_cases[:5])\n",
    "\n",
    "        transforms_list = [   \n",
    "                # Load image \n",
    "                LoadImaged(keys=[\"image\", 'gt', 'bridge_weight_map', 'pred_seg_logits']),\n",
    "                EnsureChannelFirstd(keys=[\"image\", 'gt', 'bridge_weight_map', 'pred_seg_logits']),\n",
    "\n",
    "                # Normalize uint8 input\n",
    "                ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=255, b_min=0, b_max=1, clip=True),\n",
    "\n",
    "                # Create a ROI mask for cropping \n",
    "                GetROIMaskdd(keys=[\"gt\"], ignore_mask_value=2, new_key_names=[\"roi_mask\"]),\n",
    "\n",
    "                # Cropping or padding if bigger or smaller (expected to be all equall of smaller)\n",
    "                ResizeWithPadOrCropd(keys=[\"image\", 'gt', 'roi_mask', 'bridge_weight_map', 'pred_seg_logits'], spatial_size=self.config['patch_size'], mode=\"minimum\"),\n",
    "                GetBinaryLabeld(keys=[\"gt\"], ignore_mask_value=2),\n",
    "                EnsureTyped(keys=[\"image\", \"gt\", \"roi_mask\", \"bridge_weight_map\", 'pred_seg_logits'], track_meta=False)\n",
    "        ]\n",
    "\n",
    "        transforms = Compose(transforms_list)\n",
    "        \n",
    "        print(\"Initializing Dataset...\")\n",
    "        train_ds = CacheDataset(\n",
    "            data=data_list, \n",
    "            transform=transforms, \n",
    "            cache_rate=self.config['train_cache_rate'],  \n",
    "            num_workers=self.config['num_workers'], \n",
    "            progress=True\n",
    "        )\n",
    "\n",
    "        print(\"Initializing Train DataLoader...\")\n",
    "        train_loader = monai.data.DataLoader(\n",
    "            train_ds, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            num_workers=self.config['num_workers'],\n",
    "            shuffle=True,      \n",
    "            pin_memory=True\n",
    "        )\n",
    "        return train_loader\n",
    "    \n",
    "    def _set_val_dataloader(self):\n",
    "        data_list = []\n",
    "        with open(self.config['data_split_json'], \"r\") as f:\n",
    "            split = json.load(f)\n",
    "\n",
    "        val_cases = split[\"val\"]\n",
    "\n",
    "        if self.config['debug']: \n",
    "            print(f\"Debug mode: Using training cases for validation dataloader\")\n",
    "            # train_cases = split[\"train\"] # TODO uncomment\n",
    "            train_cases = [\"2290837.nii.gz\"] # TODO remove\n",
    "            for train_case in train_cases:\n",
    "                complete_data_dict = {}\n",
    "                complete_data_dict[\"image\"] = join(self.config['vol_data_path'], train_case)\n",
    "                complete_data_dict[\"gt\"] = join(self.config['label_data_path'], train_case)\n",
    "                complete_data_dict[\"bridge_weight_map\"] = join(self.config['bridge_weight_map_path'], train_case)\n",
    "                complete_data_dict[\"pred_seg_logits\"] = join(self.config['pred_seg_logits'], train_case)\n",
    "                data_list.append(complete_data_dict)\n",
    "                print(f\"Validation using case: {train_case}\")\n",
    "                break  # The same training sample for validation in debug mode\n",
    "        else:\n",
    "            for val_case in val_cases:\n",
    "                complete_data_dict = {}\n",
    "                complete_data_dict[\"image\"] = join(self.config['vol_data_path'], val_case)\n",
    "                complete_data_dict[\"gt\"] = join(self.config['label_data_path'], val_case)\n",
    "                complete_data_dict[\"bridge_weight_map\"] = join(self.config['bridge_weight_map_path'], val_case)\n",
    "                complete_data_dict[\"pred_seg_logits\"] = join(self.config['pred_seg_logits'], val_case)\n",
    "                data_list.append(complete_data_dict)\n",
    "\n",
    "        print(f\"Val cases: {len(val_cases)}\")\n",
    "        print(f\"Some examples:\")\n",
    "        print(val_cases[:5])\n",
    "\n",
    "        transforms = Compose(\n",
    "            [   \n",
    "                # Load image \n",
    "                LoadImaged(keys=[\"image\", 'gt', 'bridge_weight_map', 'pred_seg_logits']),\n",
    "                EnsureChannelFirstd(keys=[\"image\", 'gt', 'bridge_weight_map', 'pred_seg_logits']),\n",
    "                # Normalize uint8 input\n",
    "                ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=255, b_min=0, b_max=1, clip=True),\n",
    "                # Create a ROI mask for cropping \n",
    "                GetROIMaskdd(keys=[\"gt\"], ignore_mask_value=2, new_key_names=[\"roi_mask\"]),\n",
    "                # Get random patches\n",
    "                ResizeWithPadOrCropd(keys=[\"image\", \"gt\", \"roi_mask\", 'bridge_weight_map', 'pred_seg_logits'], spatial_size=self.config['patch_size'], mode=\"minimum\"),\n",
    "                GetBinaryLabeld(keys=[\"gt\"], ignore_mask_value=2),\n",
    "                EnsureTyped(keys=[\"image\", \"gt\", \"roi_mask\", 'bridge_weight_map', 'pred_seg_logits'], track_meta=False)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"Initializing Dataset...\")\n",
    "        val_ds = CacheDataset(\n",
    "            data=data_list, \n",
    "            transform=transforms, \n",
    "            cache_rate=self.config['val_cache_rate'],  \n",
    "            num_workers=self.config['num_workers'], \n",
    "            progress=True\n",
    "        )\n",
    "        \n",
    "        print(\"Initializing Val DataLoader...\")\n",
    "        val_loader = monai.data.DataLoader(val_ds, batch_size=1, num_workers=self.config['num_workers'])\n",
    "        return val_loader\n",
    "\n",
    "    def _set_optimizers(self):\n",
    "        \"\"\"Define the optimizer (e.g., Adam, SGD).\"\"\"\n",
    "        opt_g = optim.AdamW(self.G_model.parameters(), lr=self.config['learning_rate'])\n",
    "        return opt_g\n",
    "    \n",
    "    def _set_scheduler(self):\n",
    "        \"\"\"Define learning rate scheduler.\"\"\"\n",
    "        # If resuming, last_epoch should be start_epoch - 1\n",
    "        last_epoch = self.config.get('resume_epoch', 0) - 1 if self.config.get('resume') else -1\n",
    "        G_cosAnnealLR = CosineAnnealingLR(self.opt_G, self.config['num_epochs'], eta_min=self.config['learning_rate']/10, last_epoch=last_epoch)\n",
    "        return G_cosAnnealLR\n",
    "    \n",
    "    def saving_logic(self, best_val_value, val_avg_value, epoch):\n",
    "        \"\"\" Logic to save the best model and periodic checkpoints \"\"\"\n",
    "\n",
    "        if best_val_value < val_avg_value: \n",
    "            best_val_value = val_avg_value\n",
    "            G_save_path = join(self.model_save_path, f\"model_best.pth\")\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_weights': self.G_model.state_dict(),  \n",
    "                    'optimizer_state_dict': self.opt_G.state_dict(),\n",
    "                    'val_value': val_avg_value,\n",
    "                }, G_save_path)\n",
    "            print(f\"Saved checkpoint: {G_save_path}\")\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        if epoch % 10 == 0: \n",
    "            G_save_path = join(self.model_save_path, f\"model_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_weights': self.G_model.state_dict(),  \n",
    "                    'optimizer_state_dict': self.opt_G.state_dict(),\n",
    "                    'val_value': val_avg_value,\n",
    "                }, G_save_path)\n",
    "            print(f\"Saved checkpoint: {G_save_path}\")\n",
    "        return best_val_value\n",
    "\n",
    "    def train_epoch(self, **kwargs):\n",
    "        \"\"\"Logic for a single training epoch. Returns average loss.\"\"\"\n",
    "        epoch = kwargs.get('epoch')\n",
    "        \n",
    "        G_epoch_loss = 0\n",
    "\n",
    "        G_per_criterio_loss = {} # voxel wise metrics dict\n",
    "\n",
    "        self.G_model.train()\n",
    "\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch}/{self.config['num_epochs']}\")\n",
    "\n",
    "        for idx, batch_dict in enumerate(pbar):\n",
    "            # batch_dict contains:\n",
    "                # image (volume normalized)\n",
    "                # gt (the real ground truth binary)\n",
    "                # the roi_mask (with the region to ignore 0 and to consider 1)\n",
    "                # weighted map for the bridge weight loss\n",
    "                # segmentation logits (pre-computed to save time!)\n",
    "            input_image = batch_dict['image'].to(self.config['device'])\n",
    "            ground_truth = batch_dict['gt'].to(self.config['device'])\n",
    "            roi_mask = batch_dict['roi_mask'].to(self.config['device'])\n",
    "            bridge_weight_map = batch_dict['bridge_weight_map'].to(self.config['device'])\n",
    "            pred_seg_logits = batch_dict['pred_seg_logits'].to(self.config['device']) # TODO change to pred_seg_logits\n",
    "\n",
    "            self.opt_G.zero_grad()\n",
    "            \n",
    "            with autocast(device_type=self.config['device']):\n",
    "                input_G = torch.cat([input_image, pred_seg_logits], dim=1)\n",
    "                refined_logits = self.G_model(input_G)\n",
    "                refined_probs_for_g = torch.sigmoid(refined_logits) \n",
    "                \n",
    "                # Segmentation Loss (Ground Truth Accuracy)\n",
    "                # Compare refined_logits directly to real_gt_mask\n",
    "                voxel_wise_loss, G_losses_dict = self.G_voxel_criterion([refined_logits], [ground_truth], roi_mask=[roi_mask], bridge_weight_map=bridge_weight_map) \n",
    "                \n",
    "                # loss_seg dominates (weight 100), loss_adv refines\n",
    "                loss_g_total = voxel_wise_loss\n",
    "\n",
    "\n",
    "            # Backward G\n",
    "            self.G_scaler.scale(loss_g_total).backward()\n",
    "            self.G_scaler.step(self.opt_G)\n",
    "            \n",
    "            # Update Scaler once per batch\n",
    "            self.G_scaler.update()\n",
    "            \n",
    "            \n",
    "            ##### Handle loss graphs #####\n",
    "            # Overall loss\n",
    "            G_epoch_loss += loss_g_total.item()\n",
    "            \n",
    "            # adding all individual metrics to the G dict\n",
    "            for G_criterio_name in G_losses_dict.keys():\n",
    "                if G_criterio_name in G_per_criterio_loss:\n",
    "                    G_per_criterio_loss[G_criterio_name] += G_losses_dict[G_criterio_name]\n",
    "                else:\n",
    "                    G_per_criterio_loss[G_criterio_name] = G_losses_dict[G_criterio_name]\n",
    "            \n",
    "            # Update status bar\n",
    "            pbar.set_postfix({\n",
    "                \"G_Loss\": loss_g_total.item()\n",
    "            })\n",
    "        \n",
    "        if epoch%10 == 0:\n",
    "            # Save a prediction\n",
    "            self.save_vol(refined_probs_for_g, join(self.preds_path, f\"epoch_{epoch}_pred_train.nii.gz\"))\n",
    "            self.save_vol(input_image, join(self.preds_path, f\"epoch_{epoch}_input_train.nii.gz\"))\n",
    "            self.save_vol(pred_seg_logits, join(self.preds_path, f\"epoch_{epoch}_input_pred_seg_logits_train.nii.gz\"))\n",
    "            self.save_vol(ground_truth, join(self.preds_path, f\"epoch_{epoch}_gt_train.nii.gz\"))\n",
    "\n",
    "        \n",
    "        G_train_avg_loss = G_epoch_loss / len(self.train_loader)\n",
    "        print(f\"Epoch {epoch} Finished. Avg Loss: {G_train_avg_loss:.6f}\")\n",
    "        \n",
    "        # This will replace each element in the dict with the mean\n",
    "        for criterio_name in G_losses_dict.keys():\n",
    "            G_per_criterio_loss[criterio_name] = G_per_criterio_loss[criterio_name] / len(self.train_loader)\n",
    "\n",
    "        return G_train_avg_loss, G_per_criterio_loss\n",
    "    \n",
    "    def val(self, **kwargs):\n",
    "        \"\"\"Logic for evaluation. Returns a dictionary of metrics.\"\"\"\n",
    "        epoch = kwargs.get('epoch')\n",
    "        self.G_model.eval()\n",
    "        \n",
    "        # General DSC validation value for quality controll\n",
    "        val_value_sum = 0\n",
    "        epoch_val_pixel_loss = 0\n",
    "        adv_fake = 0\n",
    "        adv_real = 0\n",
    "        # Add the per criterio val loss for checking overfitting\n",
    "        per_criterio_val_loss = {}\n",
    "        for val_criterio_name in self.config['criterion']:\n",
    "            per_criterio_val_loss[f\"val_{val_criterio_name}\"] = 0\n",
    "\n",
    "        pbar = tqdm(self.val_loader, desc=f\"Val epoch {epoch}/{self.config['num_epochs']}\")\n",
    "        for idx, batch_dict in enumerate(pbar):\n",
    "            input_image = batch_dict['image'].to(self.config['device'])\n",
    "            ground_truth = batch_dict['gt'].to(self.config['device']) \n",
    "            # Create the mask of the region to compute the loss\n",
    "            roi_mask = batch_dict['roi_mask'].to(self.config['device'])\n",
    "            bridge_weight_map = batch_dict['bridge_weight_map'].to(self.config['device'])\n",
    "            pred_seg_logits = batch_dict['pred_seg_logits'].to(self.config['device'])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                input_G = torch.cat([input_image, pred_seg_logits], dim=1)\n",
    "                refined_logits = self.G_model(input_G)\n",
    "                # Calculate DSC (Compare Prediction vs. GT)\n",
    "                val_value = self.val_metric(pred=refined_logits, target=ground_truth, roi_mask=roi_mask)\n",
    "                # Also compute val losses for logging (no deep supervision here)\n",
    "                val_loss, val_losses_dict = self.G_voxel_criterion([refined_logits], [ground_truth], roi_mask=[roi_mask], bridge_weight_map=bridge_weight_map, deep_supervision_weights=[1.0]) \n",
    "                # commented to avoid overwhelming \n",
    "                #self.wandb_run.log({\"val_value\": val_value.item()})\n",
    "\n",
    "            val_value_sum += val_value # val metric (DSC)\n",
    "            epoch_val_pixel_loss += val_loss.item() # all losses function used for training (except Adv)\n",
    "            for val_criterio_name in val_losses_dict.keys():\n",
    "                per_criterio_val_loss[f\"val_{val_criterio_name}\"] += val_losses_dict[f\"{val_criterio_name}\"]\n",
    "            pbar.set_postfix({\"DSC\": val_value})\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            pred_save = sigmoid(refined_logits)\n",
    "            pred_save[pred_save>0.5] = 1.0\n",
    "            pred_save[pred_save<=0.5] = 0.0\n",
    "            self.save_vol(refined_logits, join(self.preds_path, f\"epoch_{epoch}_logits_val.nii.gz\"))\n",
    "            self.save_vol(pred_save, join(self.preds_path, f\"epoch_{epoch}_pred_val.nii.gz\"))\n",
    "            self.save_vol(input_image, join(self.preds_path, f\"epoch_{epoch}_input_val.nii.gz\"))\n",
    "            self.save_vol(pred_seg_logits, join(self.preds_path, f\"epoch_{epoch}_input_pred_seg_logits_val.nii.gz\"))\n",
    "            self.save_vol(ground_truth, join(self.preds_path, f\"epoch_{epoch}_gt_val.nii.gz\"))\n",
    "\n",
    "        # computing mean of metrics\n",
    "        val_avg_value = val_value_sum / len(self.val_loader)\n",
    "        val_avg_pixel_loss = epoch_val_pixel_loss / len(self.val_loader)\n",
    "\n",
    "        for val_criterio_name in val_losses_dict.keys():\n",
    "            per_criterio_val_loss[f\"val_{val_criterio_name}\"] = per_criterio_val_loss[f\"val_{val_criterio_name}\"] / len(self.val_loader)\n",
    "        print(f\"Epoch {epoch} with validation avg DSC: {val_avg_value:.6f} | avg Loss: {val_avg_pixel_loss:.6f}\")\n",
    "        return val_avg_value, val_avg_pixel_loss, per_criterio_val_loss \n",
    "    \n",
    "    def train_loop(self, **kwargs):\n",
    "        \"\"\"Standardized training loop.\"\"\"\n",
    "        best_val_value = self.val_value\n",
    "        \n",
    "        # Make sure all weights are trainable\n",
    "        for param in self.G_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for self.epoch in range(self.start_epoch, self.config['num_epochs'] + 1):\n",
    "            # Train one epoch\n",
    "            G_train_avg_loss, G_per_criterio_loss = self.train_epoch(\n",
    "                epoch=self.epoch\n",
    "            )\n",
    "            # Perform evaluation \n",
    "            val_avg_value, val_avg_pixel_loss, per_criterio_val_loss  = self.val(\n",
    "                epoch=self.epoch\n",
    "            )\n",
    "\n",
    "            # Save in wandb\n",
    "            log_train_data = {\n",
    "                    \"epoch_train\": self.epoch,\n",
    "                    \"train_avg_loss\": G_train_avg_loss,\n",
    "                    \"val_Dice\": val_avg_value,\n",
    "                    \"val_avg_pixel_loss\": val_avg_pixel_loss,\n",
    "                    \"G_lr\": self.opt_G.param_groups[0]['lr'],\n",
    "                    \"D_lr\": self.opt_D.param_groups[0]['lr']  \n",
    "                }\n",
    "\n",
    "            for criterio_name in G_per_criterio_loss.keys():\n",
    "                log_train_data[criterio_name] = G_per_criterio_loss[criterio_name]\n",
    "                if criterio_name.endswith(\"_fullres\"):\n",
    "                    print(f\"_fullres is still in the loss function! It should not!\")\n",
    "\n",
    "\n",
    "            for val_criterio_name in per_criterio_val_loss.keys():\n",
    "                log_train_data[val_criterio_name] = per_criterio_val_loss[val_criterio_name]\n",
    "\n",
    "            self.wandb_run.log(\n",
    "                log_train_data\n",
    "            )\n",
    "\n",
    "            # Checking if saving \n",
    "            best_val_value = self.saving_logic(\n",
    "                best_val_value=best_val_value, \n",
    "                val_avg_value=val_avg_value, \n",
    "                epoch=self.epoch\n",
    "            )\n",
    "\n",
    "            # Applying learning rate Cosine Annealing\n",
    "            self.G_cosAnnealLR.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284d273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training using case: {'image': '/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/small_train/images/2290837.nii.gz', 'gt': '/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/small_train/labels/2290837.nii.gz', 'bridge_weight_map': '/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/small_train/train_bridge_weight_map/2290837.nii.gz', 'pred_seg_logits': '/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/small_train/pred_seg_logits/2290837.nii.gz'}\n",
      "Train cases: 1\n",
      "Some examples:\n",
      "['2290837.nii.gz']\n",
      "Initializing Dataset...\n",
      "Initializing Train DataLoader...\n",
      "Debug mode: Using training cases for validation dataloader\n",
      "Validation using case: 2290837.nii.gz\n",
      "Val cases: 79\n",
      "Some examples:\n",
      "['693501383.nii.gz', '118041886.nii.gz', '571334887.nii.gz', '2536049117.nii.gz', '1127903126.nii.gz']\n",
      "Initializing Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Val DataLoader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshadowtwin\u001b[0m (\u001b[33mfaking_it\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../checkpoints/post_processing_step/wandb/run-20260123_184137-nsbb1q47</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/faking_it/Vesuvius/runs/nsbb1q47' target=\"_blank\">main_post-processing_Loss_1.0DSC_1.0Focal__23-1-2026</a></strong> to <a href='https://wandb.ai/faking_it/Vesuvius' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/faking_it/Vesuvius' target=\"_blank\">https://wandb.ai/faking_it/Vesuvius</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/faking_it/Vesuvius/runs/nsbb1q47' target=\"_blank\">https://wandb.ai/faking_it/Vesuvius/runs/nsbb1q47</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/30:   0%|          | 0/31 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.57 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 13.63 GiB memory in use. Of the allocated memory 13.37 GiB is allocated by PyTorch, and 51.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     config_content = json.load(f)\n\u001b[32m      4\u001b[39m GANs_train_object = postprocessConvNeXt(config_content)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mGANs_train_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 401\u001b[39m, in \u001b[36mpostprocessConvNeXt.train_loop\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m     param.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.start_epoch, \u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m] + \u001b[32m1\u001b[39m):\n\u001b[32m    400\u001b[39m     \u001b[38;5;66;03m# Train one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     G_train_avg_loss, G_per_criterio_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m     \u001b[38;5;66;03m# Perform evaluation \u001b[39;00m\n\u001b[32m    405\u001b[39m     val_avg_value, val_avg_pixel_loss, per_criterio_val_loss  = \u001b[38;5;28mself\u001b[39m.val(\n\u001b[32m    406\u001b[39m         epoch=\u001b[38;5;28mself\u001b[39m.epoch\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 292\u001b[39m, in \u001b[36mpostprocessConvNeXt.train_epoch\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    288\u001b[39m     loss_g_total = voxel_wise_loss\n\u001b[32m    291\u001b[39m \u001b[38;5;66;03m# Backward G\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mG_scaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_g_total\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.G_scaler.step(\u001b[38;5;28mself\u001b[39m.opt_G)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Update Scaler once per batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/utils/checkpoint.py:1154\u001b[39m, in \u001b[36m_checkpoint_hook.__init__.<locals>.unpack_hook\u001b[39m\u001b[34m(holder)\u001b[39m\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1150\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _recomputation_hook(\n\u001b[32m   1151\u001b[39m         weakref.ref(frame), gid\n\u001b[32m   1152\u001b[39m     ), torch.autograd.enable_grad():\n\u001b[32m   1153\u001b[39m         \u001b[38;5;66;03m# See Note: [compiled autograd and checkpoint unpack hook]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1154\u001b[39m         \u001b[43m_run_fn_with_dynamo_disabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecompute_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _StopRecomputationError:\n\u001b[32m   1156\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1046\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/utils/checkpoint.py:1124\u001b[39m, in \u001b[36m_run_fn_with_dynamo_disabled\u001b[39m\u001b[34m(fn, *args, **kwargs)\u001b[39m\n\u001b[32m   1122\u001b[39m \u001b[38;5;129m@torch\u001b[39m._disable_dynamo\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_fn_with_dynamo_disabled\u001b[39m(fn, *args, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/utils/checkpoint.py:1555\u001b[39m, in \u001b[36m_checkpoint_without_reentrant_generator.<locals>.recompute_fn\u001b[39m\u001b[34m(*inputs)\u001b[39m\n\u001b[32m   1551\u001b[39m device_autocast_ctx = torch.amp.autocast(\n\u001b[32m   1552\u001b[39m     device_type=device_type, **device_autocast_kwargs\n\u001b[32m   1553\u001b[39m ) \u001b[38;5;28;01mif\u001b[39;00m torch.amp.is_autocast_available(device_type) \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext()\n\u001b[32m   1554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m device_autocast_ctx, torch.amp.autocast(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, **cpu_autocast_kwargs), recompute_context:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m     \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mConvNeXtV2Block.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     87\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pwconv1(x)\n\u001b[32m     88\u001b[39m x = \u001b[38;5;28mself\u001b[39m.act(x)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pwconv2(x)\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# 4. Restore Layout\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mGRN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m Gx = torch.norm(x, p=\u001b[32m2\u001b[39m, dim=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m), keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     36\u001b[39m Nx = Gx / (Gx.mean(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[38;5;28mself\u001b[39m.eps)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mNx\u001b[49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.beta + x\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.57 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 13.63 GiB memory in use. Of the allocated memory 13.37 GiB is allocated by PyTorch, and 51.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = '../configs/post_process_ConvNeXt.json'\n",
    "with open(CONFIG_FILE, \"r\") as f:\n",
    "    config_content = json.load(f)\n",
    "GANs_train_object = postprocessConvNeXt(config_content)\n",
    "GANs_train_object.train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check if the D_loss is within the value expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuvius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
