{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0316bc89",
   "metadata": {},
   "source": [
    "# Test code for the test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8ce246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/topological-metrics-kaggle/src\"))\n",
    "\n",
    "\n",
    "import topometrics.leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5530a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vesuvius competition metric.\n",
    "\n",
    "Expects standard Kaggle paths and Linux in order to manage dependencies.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import importlib\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class HostVisibleError(Exception):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504e3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_volume(path):\n",
    "    im = Image.open(path)\n",
    "    slices = []\n",
    "    for i, page in enumerate(ImageSequence.Iterator(im)):\n",
    "        slice_array = np.array(page)\n",
    "        slices.append(slice_array)\n",
    "    volume = np.stack(slices, axis=0)\n",
    "    return volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c24a4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    \"\"\"On Kaggle, the topometrics library must be installed during the run. This function handles the entire process.\"\"\"\n",
    "    try:\n",
    "        import topometrics.leaderboard\n",
    "\n",
    "        return None\n",
    "    # The broad exception is necessary as the initial import can fail for multiple reasons.\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    resources_dir = '/kaggle/input/vesuvius-metric-resources'\n",
    "    install_dir = '/kaggle/working/topological-metrics-kaggle'\n",
    "\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            f'cd {resources_dir} && uv pip install --no-index --find-links=wheels -r topological-metrics-kaggle/requirements.txt',\n",
    "            shell=True,\n",
    "            check=True,\n",
    "        )\n",
    "        subprocess.run(f'cd /kaggle/working && cp -r {resources_dir}/topological-metrics-kaggle .', shell=True, check=True)\n",
    "        subprocess.run(\n",
    "            f'cd {install_dir} && chmod +x scripts/setup_submodules.sh scripts/build_betti.sh && make build-betti',\n",
    "            shell=True,\n",
    "            check=True,\n",
    "        )\n",
    "        subprocess.run(\n",
    "            f'cd {install_dir} && uv pip install -e . --no-deps --no-index --no-build-isolation -v',\n",
    "            shell=True,\n",
    "            check=True,\n",
    "        )\n",
    "        # Add the new library to Python's path and invalidate caches to ensure it's found.\n",
    "        sys.path.append('/kaggle/working/topological-metrics-kaggle/src')\n",
    "        importlib.invalidate_caches()\n",
    "\n",
    "    except Exception as err:\n",
    "        raise HostVisibleError(f'Failed to install topometrics library: {err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2b2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def generate_standard_submission(submission_dir: str) -> None:\n",
    "    # 1. Look for TIFs in the provided submission directory\n",
    "    submission_tifs = glob.glob(os.path.join(submission_dir, '**/*.tif'), recursive=True)\n",
    "    \n",
    "    # 2. Local fallback: if not found, look in a local 'tmp' folder instead of /kaggle/tmp\n",
    "    if len(submission_tifs) == 0:\n",
    "        submission_tifs = glob.glob('tmp/**/*.tif', recursive=True)\n",
    "        \n",
    "    if len(submission_tifs) == 0:\n",
    "        # Using a standard ValueError for local debugging\n",
    "        raise ValueError(f'No submission files found in {submission_dir} or local tmp folder.')\n",
    "\n",
    "    df = pd.DataFrame({'tif_paths': submission_tifs})\n",
    "    \n",
    "    # Use os.path.basename for cross-platform compatibility (Windows/Linux)\n",
    "    df['id'] = df['tif_paths'].apply(lambda x: os.path.basename(x).split('.')[0])\n",
    "\n",
    "    # 3. Save to the current working directory or a specific output path\n",
    "    # Removed: os.chdir('/kaggle/working') \n",
    "    \n",
    "    output_path = 'submission.csv'\n",
    "    df[['id', 'tif_paths']].to_csv(output_path, index=False)\n",
    "    print(f\"Submission saved to {os.path.abspath(output_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7be6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_single_tif(\n",
    "    gt_path,\n",
    "    pred_path,\n",
    "    surface_tolerance,\n",
    "    voi_connectivity=26,\n",
    "    voi_transform='one_over_one_plus',\n",
    "    voi_alpha=0.3,\n",
    "    topo_weight=0.3,\n",
    "    surface_dice_weight=0.35,\n",
    "    voi_weight=0.35,\n",
    "):\n",
    "    gt: np.ndarray = load_volume(gt_path)\n",
    "    pr: np.ndarray = load_volume(pred_path)\n",
    "\n",
    "    #install_dependencies()\n",
    "    # The import is here to ensure dependencies are loaded first.\n",
    "    try:\n",
    "        # Use a standard import now that the path is reliably set.\n",
    "        import topometrics.leaderboard\n",
    "    except Exception as err:\n",
    "        raise HostVisibleError(f'Failed to import topometrics after installation: {err}')\n",
    "\n",
    "    score_report = topometrics.leaderboard.compute_leaderboard_score(\n",
    "        predictions=pr,\n",
    "        labels=gt,\n",
    "        dims=(0, 1, 2),\n",
    "        spacing=(1.0, 1.0, 1.0),  # (z, y, x)\n",
    "        surface_tolerance=surface_tolerance,  # in spacing units\n",
    "        voi_connectivity=voi_connectivity,\n",
    "        voi_transform=voi_transform,\n",
    "        voi_alpha=voi_alpha,\n",
    "        combine_weights=(topo_weight, surface_dice_weight, voi_weight),  # (Topo, SurfaceDice, VOI)\n",
    "        fg_threshold=None,  # None => legacy \"!= 0\"; else uses \"x > threshold\"\n",
    "        ignore_label=2,  # voxels with this GT label are ignored\n",
    "        ignore_mask=None,  # or pass an explicit boolean mask\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'image_score': np.clip(score_report.score, 0.0, 1.0),\n",
    "        'topo_score': score_report.topo.toposcore,\n",
    "        'surface_dice': score_report.surface_dice,\n",
    "        'voi_score': score_report.voi.voi_score,\n",
    "        'voi_split': score_report.voi.voi_split,\n",
    "        'voi_merge': score_report.voi.voi_merge\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    surface_tolerance: float = 2.0,\n",
    "    voi_connectivity: int = 26,\n",
    "    voi_transform: str = 'one_over_one_plus',\n",
    "    voi_alpha: float = 0.3,\n",
    "    topo_weight: float = 0.3,\n",
    "    surface_dice_weight: float = 0.35,\n",
    "    voi_weight: float = 0.35,\n",
    "    ) -> float:\n",
    "    \"\"\"Returns the mean per-volume scores and attaches sub-metrics to the solution df.\"\"\"\n",
    "    if not solution['tif_paths'].apply(os.path.exists).all():\n",
    "        raise HostVisibleError('Invalid solution file paths')\n",
    "\n",
    "    solution['pred_paths'] = submission['tif_paths']\n",
    "    \n",
    "    # Apply the scoring function and expand the returned dict into new columns\n",
    "    metrics_df = solution.apply(\n",
    "        lambda row: score_single_tif(\n",
    "            row['tif_paths'],\n",
    "            row['pred_paths'],\n",
    "            surface_tolerance,\n",
    "            voi_connectivity=voi_connectivity,\n",
    "            voi_transform=voi_transform,\n",
    "            voi_alpha=voi_alpha,\n",
    "            topo_weight=topo_weight,\n",
    "            surface_dice_weight=surface_dice_weight,\n",
    "            voi_weight=voi_weight,\n",
    "        ),\n",
    "        axis=1,\n",
    "    ).apply(pd.Series)\n",
    "\n",
    "    # Merge the new metric columns back into the original solution dataframe\n",
    "    solution = pd.concat([solution, metrics_df], axis=1)\n",
    "\n",
    "    # Return the mean of the primary leaderboard score\n",
    "    return float(np.mean(solution['image_score'])), solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f65252",
   "metadata": {},
   "source": [
    "#### small tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59853302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_report: {'image_score': np.float64(0.3493847817704987), 'topo_score': 0.007314644154704989, 'surface_dice': 0.5816345181502148, 'voi_score': 0.41033802049003426, 'voi_split': 2.676790371813977, 'voi_merge': 2.1132603511304806}\n"
     ]
    }
   ],
   "source": [
    "score_report = score_single_tif(\n",
    "    gt_path=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_labels/3925240194.tif\",\n",
    "    pred_path=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_pred/3925240194.tif\",\n",
    "    surface_tolerance=2.0,\n",
    "    voi_connectivity=26,\n",
    "    voi_transform='one_over_one_plus',\n",
    "    voi_alpha=0.3,\n",
    "    topo_weight=0.3,\n",
    "    surface_dice_weight=0.35,\n",
    "    voi_weight=0.35,\n",
    ")\n",
    "print(f\"score_report: {score_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_report: 0.2753458417894591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82068cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Leaderboard score: 0.3494\n",
      "Topo score:        0.0073\n",
      "Surface Dice:      0.5816\n",
      "VOI score:         0.4103\n",
      "VOI split/merge:   2.6768, 2.1133\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print summary of results\n",
    "print(\"-\" * 30)\n",
    "print(f\"Leaderboard score: {score_report['image_score']:.4f}\")\n",
    "print(f\"Topo score:        {score_report['topo_score']:.4f}\")\n",
    "print(f\"Surface Dice:      {score_report['surface_dice']:.4f}\")\n",
    "print(f\"VOI score:         {score_report['voi_score']:.4f}\")\n",
    "print(f\"VOI split/merge:   {score_report['voi_split']:.4f}, {score_report['voi_merge']:.4f}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c96bcc",
   "metadata": {},
   "source": [
    "## Folder test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "811b5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Mean Score: 0.3123653117799789\n",
      "Saved detailed row-by-row metrics to: /home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/detailed_scores.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Constants & Paths ---\n",
    "solution_path = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_labels/test_labels_df.csv\"\n",
    "submission_path = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_pred/test_pred_df.csv\"\n",
    "output_file = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_pred/detailed_scores.csv\"\n",
    "\n",
    "# 1. Load the solution and submission dataframes\n",
    "solution = pd.read_csv(solution_path)\n",
    "submission = pd.read_csv(submission_path)\n",
    "\n",
    "# 2. Call the score function\n",
    "# Note: 'row_id_column_name' should match the ID column in your CSV (e.g., 'id' or 'segment_id')\n",
    "final_score, solution = score(\n",
    "    solution=solution,\n",
    "    submission=submission,\n",
    "    row_id_column_name='id'\n",
    ")\n",
    "\n",
    "# 3. Save to disk so you don't lose the hard work!\n",
    "\n",
    "solution.to_csv(output_file, index=False)\n",
    "print(f\"\\nFinal Mean Score: {final_score}\")\n",
    "print(f\"Saved detailed row-by-row metrics to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12ffd8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Mean Score: 0.3123653117799789\n",
      "Saved detailed row-by-row metrics to: /home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_pred/detailed_scores_obj.csv\n"
     ]
    }
   ],
   "source": [
    "# test using object-oriented approach\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/utils\"))\n",
    "from test_obj import VesuviusMetric\n",
    "\n",
    "test_metric_obj = VesuviusMetric(\n",
    "    solution_path=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_labels/test_labels_df.csv\",\n",
    "    submission_path=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_pred/test_pred_df.csv\",\n",
    "    output_file=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_pred/detailed_scores_obj.csv\"\n",
    ")\n",
    "test_metric_obj._run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac374ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topo-metrics-3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
