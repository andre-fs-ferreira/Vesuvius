{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307f0d24",
   "metadata": {},
   "source": [
    "# Develop a light GAN for post processing\n",
    "* The input must be the biggest size in the dataset\n",
    "* Both Generator and Discriminator must fit in the GPU (48GB VRAM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ecb6d",
   "metadata": {},
   "source": [
    "### Check biggest (384,384,384 although there is only 1 case with this shape...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd4a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pre_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x769b37980890>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 769b34361690, raw_cell=\"# Find the bigest shape\n",
      "import os\n",
      "import tifffile ..\" transformed_cell=\"# Find the bigest shape\n",
      "import os\n",
      "import tifffile ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/notebooks/11_GANs_postprocess.ipynb#W2sZmlsZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenPipeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:593\u001b[39m, in \u001b[36m_WandbInit._pre_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend.interface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mpausing backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:803\u001b[39m, in \u001b[36mInterfaceBase.publish_pause\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    802\u001b[39m     pause = pb.PauseRequest()\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:330\u001b[39m, in \u001b[36mInterfaceShared._publish_pause\u001b[39m\u001b[34m(self, pause)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb.PauseRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    329\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(pause=pause)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:380\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._reader.exception()\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/IPython/core/events.py:82\u001b[39m, in \u001b[36mEventManager.trigger\u001b[39m\u001b[34m(self, event, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks[event][:]:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_on_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:604\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:811\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    810\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:380\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._reader.exception()\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/IPython/core/events.py:82\u001b[39m, in \u001b[36mEventManager.trigger\u001b[39m\u001b[34m(self, event, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks[event][:]:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_on_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:593\u001b[39m, in \u001b[36m_WandbInit._pre_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend.interface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mpausing backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:803\u001b[39m, in \u001b[36mInterfaceBase.publish_pause\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    802\u001b[39m     pause = pb.PauseRequest()\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:330\u001b[39m, in \u001b[36mInterfaceShared._publish_pause\u001b[39m\u001b[34m(self, pause)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb.PauseRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    329\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(pause=pause)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:380\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._reader.exception()\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/selector_events.py:1061\u001b[39m, in \u001b[36m_SelectorSocketTransport.write\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer:\n\u001b[32m   1059\u001b[39m     \u001b[38;5;66;03m# Optimization: try to send now.\u001b[39;00m\n\u001b[32m   1060\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1061\u001b[39m         n = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mBlockingIOError\u001b[39;00m, \u001b[38;5;167;01mInterruptedError\u001b[39;00m):\n\u001b[32m   1063\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mBrokenPipeError\u001b[39m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 297, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 216, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 170, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(250, 139, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 134, 256)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(378, 177, 384)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(250, 250, 256)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "(314, 314, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 314, 320)\n",
      "(314, 314, 320)\n",
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x769b37980890>> (for post_run_cell), with arguments args (<ExecutionResult object at 769b34360150, execution_count=3 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 769b34361690, raw_cell=\"# Find the bigest shape\n",
      "import os\n",
      "import tifffile ..\" transformed_cell=\"# Find the bigest shape\n",
      "import os\n",
      "import tifffile ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/notebooks/11_GANs_postprocess.ipynb#W2sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenPipeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:604\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:811\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    810\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:380\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._reader.exception()\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/IPython/core/events.py:82\u001b[39m, in \u001b[36mEventManager.trigger\u001b[39m\u001b[34m(self, event, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks[event][:]:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_on_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:593\u001b[39m, in \u001b[36m_WandbInit._pre_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend.interface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mpausing backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:803\u001b[39m, in \u001b[36mInterfaceBase.publish_pause\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    802\u001b[39m     pause = pb.PauseRequest()\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:330\u001b[39m, in \u001b[36mInterfaceShared._publish_pause\u001b[39m\u001b[34m(self, pause)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb.PauseRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    329\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(pause=pause)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:380\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._reader.exception()\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/IPython/core/events.py:82\u001b[39m, in \u001b[36mEventManager.trigger\u001b[39m\u001b[34m(self, event, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks[event][:]:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_on_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:604\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:811\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    810\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:380\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._reader.exception()\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/IPython/core/events.py:82\u001b[39m, in \u001b[36mEventManager.trigger\u001b[39m\u001b[34m(self, event, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks[event][:]:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_on_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:593\u001b[39m, in \u001b[36m_WandbInit._pre_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend.interface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mpausing backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:803\u001b[39m, in \u001b[36mInterfaceBase.publish_pause\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    802\u001b[39m     pause = pb.PauseRequest()\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:330\u001b[39m, in \u001b[36mInterfaceShared._publish_pause\u001b[39m\u001b[34m(self, pause)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb.PauseRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    329\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(pause=pause)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:380\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._reader.exception()\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/selector_events.py:1061\u001b[39m, in \u001b[36m_SelectorSocketTransport.write\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer:\n\u001b[32m   1059\u001b[39m     \u001b[38;5;66;03m# Optimization: try to send now.\u001b[39;00m\n\u001b[32m   1060\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1061\u001b[39m         n = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mBlockingIOError\u001b[39;00m, \u001b[38;5;167;01mInterruptedError\u001b[39;00m):\n\u001b[32m   1063\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mBrokenPipeError\u001b[39m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "# Find the bigest shape\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "all_shapes = []\n",
    "root_dataset = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/Challenge_dataset_updated/train_labels_nii_crop\"\n",
    "for file_name in os.listdir(root_dataset):\n",
    "    if file_name.endswith('.nii.gz'):\n",
    "        file_path = os.path.join(root_dataset, file_name)\n",
    "        img_array = nib.load(file_path).get_fdata()\n",
    "        all_shapes.append(img_array.shape)\n",
    "        #if img_array.shape[0]>320:\n",
    "        print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfe2abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest x: 384\n",
      "Biggest y: 384\n",
      "Biggest z: 384\n"
     ]
    }
   ],
   "source": [
    "x_all_shapes = [shape_element[0] for shape_element in all_shapes]\n",
    "y_all_shapes = [shape_element[1] for shape_element in all_shapes]\n",
    "z_all_shapes = [shape_element[2] for shape_element in all_shapes]\n",
    "\n",
    "print(f\"Biggest x: {max(x_all_shapes)}\")\n",
    "print(f\"Biggest y: {max(y_all_shapes)}\")\n",
    "print(f\"Biggest z: {max(z_all_shapes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59e1f1",
   "metadata": {},
   "source": [
    "### Create GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694124c",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7ecc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, in_channels=2, first_channels=16, out_channels=1, use_checkpointing=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.first_channels = first_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "        \n",
    "        # ----------------------\n",
    "        # ENCODER\n",
    "        # ----------------------\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv3d(self.in_channels, self.first_channels, kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            nn.InstanceNorm3d(self.first_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        ) \n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv3d(self.first_channels, self.first_channels*2, kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            nn.InstanceNorm3d(self.first_channels*2),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        ) \n",
    "\n",
    "        # ----------------------\n",
    "        # BOTTLENECK (Hybrid Dilated)\n",
    "        # ----------------------\n",
    "        # FIX: Added Dilation=4 to close larger holes.\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            # 1. Standard\n",
    "            nn.Conv3d(self.first_channels*2, self.first_channels*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(self.first_channels*2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            \n",
    "            # 2. Wide View (Dilation 2)\n",
    "            nn.Conv3d(self.first_channels*2, self.first_channels*2, kernel_size=3, padding=2, dilation=2, bias=False),\n",
    "            nn.InstanceNorm3d(self.first_channels*2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            # 3. Wider View (Dilation 4) <--- ADDED\n",
    "            # Receptive field increases significantly without memory cost\n",
    "            nn.Conv3d(self.first_channels*2, self.first_channels*2, kernel_size=3, padding=4, dilation=4, bias=False),\n",
    "            nn.InstanceNorm3d(self.first_channels*2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            \n",
    "            # 4. Consolidate\n",
    "            nn.Conv3d(self.first_channels*2, self.first_channels*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(self.first_channels*2)\n",
    "        )\n",
    "\n",
    "        # ----------------------\n",
    "        # DECODER\n",
    "        # ----------------------\n",
    "        \n",
    "        # Dec 2: It's okay to use Nearest here because Dec 1 will clean it up.\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv3d(self.first_channels*4, self.first_channels*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(self.first_channels*2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest') \n",
    "        )\n",
    "\n",
    "        # Dec 1: FIX -> Switched to TRILINEAR\n",
    "        # Since we feed directly into a 1x1 Conv, 'nearest' would leave blocky artifacts.\n",
    "        # Trilinear ensures smooth gradients at the final boundary.\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv3d(self.first_channels + self.first_channels*2, self.first_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(self.first_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False) # <--- CHANGED\n",
    "        )\n",
    "\n",
    "        # ----------------------\n",
    "        # OUTPUT\n",
    "        # ----------------------\n",
    "        # FIX: bias=True. \n",
    "        # Allows the network to learn a global \"confidence shift\" easily.\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv3d(self.first_channels, self.out_channels, kernel_size=1, stride=1, padding=0, bias=True) \n",
    "        )\n",
    "        \n",
    "        # Zero Init for Residual Learning\n",
    "        nn.init.constant_(self.out[0].weight, 0)\n",
    "        nn.init.constant_(self.out[0].bias, 0) # Init the bias to 0 too\n",
    "\n",
    "    def forward(self, x):\n",
    "        def run_layer(layer, inp):\n",
    "            if self.use_checkpointing and inp.requires_grad:\n",
    "                return checkpoint(layer, inp, use_reentrant=False)\n",
    "            return layer(inp)\n",
    "\n",
    "        # Encoder\n",
    "        x_enc1 = self.enc1(x)\n",
    "        x_enc2 = run_layer(self.enc2, x_enc1)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x_bottleneck = run_layer(self.bottleneck, x_enc2)\n",
    "        \n",
    "        # Decoder 2\n",
    "        cat_d2 = torch.cat([x_enc2, x_bottleneck], dim=1)\n",
    "        x_dec2 = run_layer(self.dec2, cat_d2)\n",
    "        \n",
    "        # Decoder 1\n",
    "        cat_d1 = torch.cat([x_enc1, x_dec2], dim=1)\n",
    "        x_dec1 = run_layer(self.dec1, cat_d1)\n",
    "        \n",
    "        # Delta (Correction)\n",
    "        delta_logits = self.out(x_dec1)\n",
    "        \n",
    "        # Input Logits (Assumption: Input Ch 1 is ALREADY logits)\n",
    "        input_logits = x[:, 1:2, :, :, :]\n",
    "        if self.training and input_logits.max() <= 1.0 and input_logits.min() >= 0.0:\n",
    "            import warnings\n",
    "            warnings.warn(\"Input Channel 1 looks like Probabilities (0-1), but the network expects Logits (-inf to inf). Residual learning will fail!\")\n",
    "        \n",
    "        return input_logits + delta_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d72d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda...\n",
      "Starting training step with AMP...\n",
      "Step successful. Loss: 1.9999\n",
      "Peak Training VRAM: 12.56 GiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nRunning on cuda...\\nStarting training step with AMP...\\n\\n--- Forward Pass Start ---\\nInput x: torch.Size([1, 2, 320, 320, 320])\\nx_enc1 (after stride 2): torch.Size([1, 16, 160, 160, 160])\\nx_enc2 (after stride 2): torch.Size([1, 32, 80, 80, 80])\\nx_bottleneck: torch.Size([1, 32, 80, 80, 80])\\ncat_d2 (enc2 + bottleneck): torch.Size([1, 64, 80, 80, 80])\\nx_dec2 (after upsample): torch.Size([1, 32, 160, 160, 160])\\ncat_d1 (enc1 + dec2): torch.Size([1, 48, 160, 160, 160])\\nx_dec1 (after upsample): torch.Size([1, 16, 320, 320, 320])\\ndelta_logits: torch.Size([1, 1, 320, 320, 320])\\ninput_logits (from channel 1): torch.Size([1, 1, 320, 320, 320])\\nFinal output: torch.Size([1, 1, 320, 320, 320])\\n--- Forward Pass End ---\\n\\nStep successful. Loss: 2.0002\\nPeak Training VRAM: 12.56 GiB\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import psutil\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def test_train_step():\n",
    "    # 1. Setup Device\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Running on {device}...\")\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # 2. Initialize Model, Optimizer, and Scaler (for AMP)\n",
    "    generator = Generator(in_channels=2, first_channels=16, out_channels=1).to(device)\n",
    "    optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss() # Simple loss for testing\n",
    "    scaler = GradScaler()     # Necessary for stable AMP training\n",
    "\n",
    "    # 3. Create Inputs and Target\n",
    "    # Input: [B, 2, 320, 320, 320]\n",
    "    input_tensor = torch.randn(1, 2, 320, 320, 320, device=device)\n",
    "    # Target: [B, 1, 320, 320, 320] (the ideal segmentation/output)\n",
    "    target = torch.randn(1, 1, 320, 320, 320, device=device)\n",
    "\n",
    "    print(f\"Starting training step with AMP...\")\n",
    "\n",
    "\n",
    "    # --- TRAINING STEP START ---\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass with AMP\n",
    "    with autocast(device_type=device.type):\n",
    "        output = generator(input_tensor)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    # Backward pass with Scaler\n",
    "    # This is where memory peaks!\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Optimizer Step\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # --- TRAINING STEP END ---\n",
    "\n",
    "    print(f\"Step successful. Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 4. Measure Memory\n",
    "    if device.type == 'cuda':\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "        print(f\"Peak Training VRAM: {peak_mem:.2f} GiB\")\n",
    "    else:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(f\"System RAM used: {process.memory_info().rss / (1024 ** 3):.2f} GiB\")\n",
    "\n",
    "test_train_step()\n",
    "\n",
    "\"\"\"\n",
    "Running on cuda...\n",
    "Starting training step with AMP...\n",
    "\n",
    "--- Forward Pass Start ---\n",
    "Input x: torch.Size([1, 2, 320, 320, 320])\n",
    "x_enc1 (after stride 2): torch.Size([1, 16, 160, 160, 160])\n",
    "x_enc2 (after stride 2): torch.Size([1, 32, 80, 80, 80])\n",
    "x_bottleneck: torch.Size([1, 32, 80, 80, 80])\n",
    "cat_d2 (enc2 + bottleneck): torch.Size([1, 64, 80, 80, 80])\n",
    "x_dec2 (after upsample): torch.Size([1, 32, 160, 160, 160])\n",
    "cat_d1 (enc1 + dec2): torch.Size([1, 48, 160, 160, 160])\n",
    "x_dec1 (after upsample): torch.Size([1, 16, 320, 320, 320])\n",
    "delta_logits: torch.Size([1, 1, 320, 320, 320])\n",
    "input_logits (from channel 1): torch.Size([1, 1, 320, 320, 320])\n",
    "Final output: torch.Size([1, 1, 320, 320, 320])\n",
    "--- Forward Pass End ---\n",
    "\n",
    "Step successful. Loss: 2.0002\n",
    "Peak Training VRAM: 12.56 GiB\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae328e0",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd6ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.amp import autocast, GradScaler\n",
    "import psutil\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=2, initial_filters=16):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        \n",
    "        # Input: (B, 2, 320, 320, 320)\n",
    "        # Channel 0: CT Scan\n",
    "        # Channel 1: Predicted Logits (or Probability)\n",
    "        \n",
    "        # We use Spectral Norm on all layers.\n",
    "        # It is the modern standard to prevent the Discriminator from becoming too strong too fast.\n",
    "        \n",
    "        # Layer 1: 320 -> 160\n",
    "        self.conv1 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv3d(in_channels, initial_filters, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Layer 2: 160 -> 80\n",
    "        self.conv2 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv3d(initial_filters, initial_filters*2, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "            nn.InstanceNorm3d(initial_filters*2), # Norm helps stability\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Layer 3: 80 -> 40\n",
    "        self.conv3 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv3d(initial_filters*2, initial_filters*4, kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "            nn.InstanceNorm3d(initial_filters*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Layer 4: 40 -> 38 (Stride 1, just reducing FOV slightly)\n",
    "        # We stop downsampling here to keep the receptive field focused.\n",
    "        self.conv4 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv3d(initial_filters*4, initial_filters*8, kernel_size=4, stride=1, padding=1, bias=False)),\n",
    "            nn.InstanceNorm3d(initial_filters*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Output Layer: 38 -> 37\n",
    "        # Maps to a 1-channel grid of \"Real/Fake\" scores\n",
    "        self.final = nn.Conv3d(initial_filters*8, 1, kernel_size=4, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should be concatenation of [CT, Logits/Mask]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        out = self.final(x)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb87cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda...\n",
      "Starting training step with AMP...\n",
      "D Input: torch.Size([1, 2, 320, 320, 320])\n",
      "D Layer 1: torch.Size([1, 16, 160, 160, 160])\n",
      "D Layer 2: torch.Size([1, 32, 80, 80, 80])\n",
      "D Layer 3: torch.Size([1, 64, 40, 40, 40])\n",
      "D Layer 4: torch.Size([1, 128, 39, 39, 39])\n",
      "D Output (Patch Grid): torch.Size([1, 1, 38, 38, 38])\n",
      "Discriminator Output Shape: torch.Size([1, 1, 38, 38, 38])\n",
      "Computing Gradients...\n",
      "Step successful. Loss: 1.3487\n",
      "Peak Training VRAM: 1.64 GiB\n"
     ]
    }
   ],
   "source": [
    "def test_train_step():\n",
    "    # 1. Setup Device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    print(f\"Running on {device}...\")\n",
    "\n",
    "    # 2. Initialize Model, Optimizer, and Scaler (for AMP)\n",
    "    discriminator = PatchDiscriminator(in_channels=2, initial_filters=16).to(device)\n",
    "    optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss() # Least Squares GAN (LSGAN) loss\n",
    "    scaler = GradScaler()     # Necessary for stable AMP training\n",
    "\n",
    "    # 3. Create Inputs\n",
    "    # Input: [B, 2, 320, 320, 320] -> (CT + Predicted_Mask)\n",
    "    input_tensor = torch.randn(1, 2, 320, 320, 320, device=device)\n",
    "    \n",
    "    print(f\"Starting training step with AMP...\")\n",
    "\n",
    "    # --- TRAINING STEP START ---\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass with AMP\n",
    "    with autocast(device_type=device.type):\n",
    "        output = discriminator(input_tensor)\n",
    "        print(f\"Discriminator Output Shape: {output.shape}\")\n",
    "        \n",
    "        # CRITICAL FIX: Creates target based on ACTUAL output shape (e.g., 38x38x38)\n",
    "        # We cannot use a 320x320x320 target for a PatchGAN output.\n",
    "        target = torch.randn_like(output, device=device) \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    # Backward pass with Scaler\n",
    "    print(\"Computing Gradients...\")\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Optimizer Step\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # --- TRAINING STEP END ---\n",
    "\n",
    "    print(f\"Step successful. Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 4. Measure Memory\n",
    "    if device.type == 'cuda':\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "        print(f\"Peak Training VRAM: {peak_mem:.2f} GiB\")\n",
    "    else:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(f\"System RAM used: {process.memory_info().rss / (1024 ** 3):.2f} GiB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834e608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Helper Loss: Tversky (Crucial for Holes/Bridges)\n",
    "# -------------------------------------------------------\n",
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.3, beta=0.7, smooth=1):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.alpha = alpha # False Positives (Bridges)\n",
    "        self.beta = beta   # False Negatives (Holes) - Higher = Punishment for holes\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Apply Sigmoid because Generator outputs Logits\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Flatten\n",
    "        probs = probs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Calculate TF/FP/FN\n",
    "        TP = (probs * targets).sum()    \n",
    "        FP = ((1 - targets) * probs).sum()\n",
    "        FN = (targets * (1 - probs)).sum()\n",
    "       \n",
    "        tversky = (TP + self.smooth) / (TP + self.alpha*FP + self.beta*FN + self.smooth)  \n",
    "        return 1 - tversky\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Setup (Run this once)\n",
    "# -------------------------------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Models\n",
    "generator = Generator(in_channels=2, first_channels=16, out_channels=1).to(device)\n",
    "discriminator = PatchDiscriminator(in_channels=2, initial_filters=16).to(device)\n",
    "\n",
    "# Optimizers\n",
    "# Discriminator usually needs slightly lower LR or same as Generator in LSGAN\n",
    "opt_g = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "opt_d = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.MSELoss()            # LSGAN Loss (Real=1, Fake=0)\n",
    "criterion_BCE = nn.BCEWithLogitsLoss()  # Pixel-wise Accuracy\n",
    "criterion_Tversky = TverskyLoss(alpha=0.3, beta=0.7) # Topology\n",
    "\n",
    "# Scaler for AMP\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Hyperparameters\n",
    "LAMBDA_SEG = 100.0  # Pixel/Dice loss is 100x more important than GAN loss\n",
    "LAMBDA_ADV = 1.0    # GAN loss is just the \"spice\" to fix texture\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. The Loop\n",
    "# -------------------------------------------------------\n",
    "def train_one_epoch(dataloader, epoch_index):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for batch_idx, (real_ct, coarse_logits, real_gt_mask) in enumerate(dataloader):\n",
    "        \n",
    "        # Move to GPU\n",
    "        real_ct = real_ct.to(device)\n",
    "        coarse_logits = coarse_logits.to(device)\n",
    "        real_gt_mask = real_gt_mask.to(device)\n",
    "        \n",
    "        # Input to Generator: (CT, Coarse_Logits)\n",
    "        g_input = torch.cat([real_ct, coarse_logits], dim=1)\n",
    "        \n",
    "        #################################################\n",
    "        #  UPDATE DISCRIMINATOR\n",
    "        #  Goal: Maximize log(D(x, y)) + log(1 - D(x, G(x, z)))\n",
    "        #################################################\n",
    "        opt_d.zero_grad()\n",
    "        \n",
    "        with autocast(device_type=device.type):\n",
    "            # --- Train with REAL (CT + Ground Truth) ---\n",
    "            # Note: Discriminator sees (CT, Mask) concatenation\n",
    "            real_pair = torch.cat([real_ct, real_gt_mask], dim=1)\n",
    "            \n",
    "            d_real_pred = discriminator(real_pair)\n",
    "            \n",
    "            # Target for REAL is 1.0\n",
    "            target_real = torch.ones_like(d_real_pred)\n",
    "            loss_d_real = criterion_GAN(d_real_pred, target_real)\n",
    "            \n",
    "            # --- Train with FAKE (CT + Refined Prediction) ---\n",
    "            # 1. Generate Fake\n",
    "            refined_logits = generator(g_input)\n",
    "            refined_probs = torch.sigmoid(refined_logits) # Convert logits to prob for D\n",
    "            \n",
    "            # 2. Detach! Crucial! \n",
    "            # We don't want to backprop into Generator yet.\n",
    "            fake_pair = torch.cat([real_ct, refined_probs.detach()], dim=1)\n",
    "            \n",
    "            d_fake_pred = discriminator(fake_pair)\n",
    "            \n",
    "            # Target for FAKE is 0.0\n",
    "            target_fake = torch.zeros_like(d_fake_pred)\n",
    "            loss_d_fake = criterion_GAN(d_fake_pred, target_fake)\n",
    "            \n",
    "            # Combine\n",
    "            loss_d = (loss_d_real + loss_d_fake) * 0.5\n",
    "\n",
    "        # Backward D\n",
    "        scaler.scale(loss_d).backward()\n",
    "        scaler.step(opt_d)\n",
    "        \n",
    "        #################################################\n",
    "        #  UPDATE GENERATOR\n",
    "        #  Goal: Fool the Discriminator (D outputs 1 for fake)\n",
    "        #        AND minimize segmentation error\n",
    "        #################################################\n",
    "        opt_g.zero_grad()\n",
    "        \n",
    "        with autocast(device_type=device.type):\n",
    "            # 1. Adversarial Loss (Fool D)\n",
    "            # We construct the pair again (this time WITH gradients)\n",
    "            # Use probabilities for D input\n",
    "            refined_probs_for_g = torch.sigmoid(refined_logits) \n",
    "            fake_pair_for_g = torch.cat([real_ct, refined_probs_for_g], dim=1)\n",
    "            \n",
    "            d_pred_for_g = discriminator(fake_pair_for_g)\n",
    "            \n",
    "            # Generator wants D to say \"1.0\" (Real)\n",
    "            target_g = torch.ones_like(d_pred_for_g)\n",
    "            loss_g_adv = criterion_GAN(d_pred_for_g, target_g)\n",
    "            \n",
    "            # 2. Segmentation Loss (Ground Truth Accuracy)\n",
    "            # Compare refined_logits directly to real_gt_mask\n",
    "            loss_g_bce = criterion_BCE(refined_logits, real_gt_mask)\n",
    "            loss_g_tversky = criterion_Tversky(refined_logits, real_gt_mask)\n",
    "            \n",
    "            # Combine\n",
    "            # loss_seg dominates (weight 100), loss_adv refines (weight 1)\n",
    "            loss_g_total = (LAMBDA_SEG * (loss_g_bce + loss_g_tversky)) + (LAMBDA_ADV * loss_g_adv)\n",
    "\n",
    "        # Backward G\n",
    "        scaler.scale(loss_g_total).backward()\n",
    "        scaler.step(opt_g)\n",
    "        \n",
    "        # Update Scaler once per batch\n",
    "        scaler.update()\n",
    "\n",
    "        # Logging\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch_index}] Batch [{batch_idx}] \"\n",
    "                  f\"Loss D: {loss_d.item():.4f} | \"\n",
    "                  f\"Loss G Total: {loss_g_total.item():.4f} \"\n",
    "                  f\"(Adv: {loss_g_adv.item():.4f}, Seg: {loss_g_bce.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c672da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test on: cuda\n",
      "Setup complete. Starting training loop...\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "Step 1: D Loss=0.4743, G Loss=137.0488\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "Step 2: D Loss=9.8179, G Loss=137.0488\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "D Input: torch.Size([2, 2, 64, 64, 64])\n",
      "D Layer 1: torch.Size([2, 16, 32, 32, 32])\n",
      "D Layer 2: torch.Size([2, 32, 16, 16, 16])\n",
      "D Layer 3: torch.Size([2, 64, 8, 8, 8])\n",
      "D Layer 4: torch.Size([2, 128, 7, 7, 7])\n",
      "D Output (Patch Grid): torch.Size([2, 1, 6, 6, 6])\n",
      "Step 3: D Loss=9.8176, G Loss=137.0493\n",
      "\n",
      "SUCCESS: Loss is changing. Gradients are flowing.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.nn.utils import spectral_norm\n",
    "def run_test():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running test on: {device}\")\n",
    "\n",
    "    # A. Create Dummy Data\n",
    "    # Batch=2, Size=64^3 (Small enough for CPU testing if needed)\n",
    "    B, D, H, W = 2, 64, 64, 64\n",
    "    \n",
    "    # CT Scan (Normalized)\n",
    "    ct_data = torch.randn(B, 1, D, H, W)\n",
    "    # Coarse Logits (Simulated output from previous net)\n",
    "    coarse_logits = torch.randn(B, 1, D, H, W)\n",
    "    # Ground Truth (Binary Mask 0 or 1)\n",
    "    gt_mask = torch.randint(0, 2, (B, 1, D, H, W)).float()\n",
    "\n",
    "    dataset = TensorDataset(ct_data, coarse_logits, gt_mask)\n",
    "    dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "    # B. Initialize Models & Optimizers\n",
    "    gen = Generator().to(device)\n",
    "    disc = PatchDiscriminator().to(device)\n",
    "    \n",
    "    opt_g = optim.Adam(gen.parameters(), lr=1e-3)\n",
    "    opt_d = optim.Adam(disc.parameters(), lr=1e-3)\n",
    "    \n",
    "    criterion_GAN = nn.MSELoss()\n",
    "    criterion_BCE = nn.BCEWithLogitsLoss()\n",
    "    criterion_Tversky = TverskyLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    LAMBDA_SEG = 100.0\n",
    "    LAMBDA_ADV = 1.0\n",
    "\n",
    "    print(\"Setup complete. Starting training loop...\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. The Training Loop Logic\n",
    "    # ==========================================\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "    \n",
    "    # Run 3 steps to verify gradients actually change\n",
    "    initial_loss = None\n",
    "    \n",
    "    for step in range(3):\n",
    "        for real_ct, coarse, real_gt in dataloader:\n",
    "            real_ct, coarse, real_gt = real_ct.to(device), coarse.to(device), real_gt.to(device)\n",
    "            \n",
    "            # --- 1. Discriminator Step ---\n",
    "            opt_d.zero_grad()\n",
    "            with autocast(device_type=device.type):\n",
    "                # Real\n",
    "                d_input_real = torch.cat([real_ct, real_gt], dim=1)\n",
    "                d_real = disc(d_input_real)\n",
    "                loss_d_real = criterion_GAN(d_real, torch.ones_like(d_real))\n",
    "                \n",
    "                # Fake\n",
    "                g_input = torch.cat([real_ct, coarse], dim=1)\n",
    "                refined_logits = gen(g_input)\n",
    "                refined_probs = torch.sigmoid(refined_logits)\n",
    "                \n",
    "                d_input_fake = torch.cat([real_ct, refined_probs.detach()], dim=1)\n",
    "                d_fake = disc(d_input_fake)\n",
    "                loss_d_fake = criterion_GAN(d_fake, torch.zeros_like(d_fake))\n",
    "                \n",
    "                loss_d = (loss_d_real + loss_d_fake) * 0.5\n",
    "\n",
    "            scaler.scale(loss_d).backward()\n",
    "            scaler.step(opt_d)\n",
    "\n",
    "            # --- 2. Generator Step ---\n",
    "            opt_g.zero_grad()\n",
    "            with autocast(device_type=device.type):\n",
    "                # Adversarial (Fool D)\n",
    "                # Note: We re-calculate d_fake WITH gradients flowing to G\n",
    "                d_input_g = torch.cat([real_ct, torch.sigmoid(refined_logits)], dim=1)\n",
    "                d_pred = disc(d_input_g)\n",
    "                loss_g_adv = criterion_GAN(d_pred, torch.ones_like(d_pred))\n",
    "                \n",
    "                # Segmentation (Structure)\n",
    "                loss_g_seg = criterion_BCE(refined_logits, real_gt) + criterion_Tversky(refined_logits, real_gt)\n",
    "                \n",
    "                loss_g = (LAMBDA_SEG * loss_g_seg) + (LAMBDA_ADV * loss_g_adv)\n",
    "\n",
    "            scaler.scale(loss_g).backward()\n",
    "            scaler.step(opt_g)\n",
    "            scaler.update()\n",
    "\n",
    "            print(f\"Step {step+1}: D Loss={loss_d.item():.4f}, G Loss={loss_g.item():.4f}\")\n",
    "            \n",
    "            if step == 0:\n",
    "                initial_loss = loss_g.item()\n",
    "            elif step == 2:\n",
    "                if loss_g.item() != initial_loss:\n",
    "                    print(\"\\nSUCCESS: Loss is changing. Gradients are flowing.\")\n",
    "                else:\n",
    "                    print(\"\\nWARNING: Loss is identical. Check for detached gradients.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69b6e0",
   "metadata": {},
   "source": [
    "### Building the training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from os.path import join\n",
    "sys.path.append(\"../utils\")\n",
    "from main_train_class import main_train_STU_Net\n",
    "from GANs_networks import Generator, PatchDiscriminator\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import sigmoid, binary_cross_entropy_with_logits\n",
    "# Standard Library Imports\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import sigmoid\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.nn import MSELoss\n",
    "# MONAI Specific Imports\n",
    "import monai\n",
    "from monai.data import CacheDataset\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    CopyItemsd,\n",
    "    LoadImaged, \n",
    "    ScaleIntensityRanged, \n",
    "    ResizeWithPadOrCropd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd\n",
    ")\n",
    "from mask_utils import GetROIMaskdd, GetBinaryLabeld\n",
    "# Local Project Imports\n",
    "\n",
    "class postprocessGANs(main_train_STU_Net):\n",
    "    def __init__(self, config):\n",
    "        # TODO predict and save logits from the segmentation network\n",
    "        self.config = config\n",
    "        self.labda_seg = self.config['labda_seg']\n",
    "        self.G_model, self.D_model = self._build_models()\n",
    "        self.train_loader = self._set_train_dataloader() \n",
    "        self.val_loader = self._set_val_dataloader() \n",
    "        self.opt_G, self.opt_D = self._set_optimizers()\n",
    "        self.wandb_run = self._set_wandb_checkpoint() # Heritage\n",
    "        # Expects lists of predictions # TODO put predictions inside of a list\n",
    "        self.G_voxel_criterion = self._set_train_criterion() # Heritage\n",
    "        self.criterion_GAN = MSELoss()\n",
    "        self.val_metric = self._set_val_metric() # Heritage\n",
    "        self.G_cosAnnealLR, self.D_cosAnnealLR = self._set_scheduler()\n",
    "\n",
    "        # set scaler for mix precision \n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # check resume\n",
    "        self._resume()\n",
    "\n",
    "    def _build_models(self):\n",
    "        # TODO change to 16 the first_channels and initial_filters\n",
    "        gen = Generator(in_channels=2, first_channels=4, out_channels=1, use_checkpointing=True).to(self.config['device'])\n",
    "        disc = PatchDiscriminator(in_channels=2, initial_filters=4).to(self.config['device'])\n",
    "        return gen, disc\n",
    "    \n",
    "    def _resume(self):\n",
    "        if self.config.get('resume'):\n",
    "            # Dis\n",
    "            D_checkpoint = torch.load(self.config['D_resume'], map_location=\"cpu\", weights_only=False) \n",
    "            D_model_weights = D_checkpoint['model_weights']  \n",
    "            self.D_model.load_state_dict(D_model_weights, strict=True)\n",
    "            self.D_model = self.D_model.to(self.config['device'])\n",
    "            # optimizer load\n",
    "            self.opt_D.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "            G_checkpoint = torch.load(self.config['G_resume'], map_location=\"cpu\", weights_only=False) \n",
    "            G_model_weights = G_checkpoint['model_weights']  \n",
    "            self.G_model.load_state_dict(G_model_weights, strict=True)\n",
    "            self.G_model = self.G_model.to(self.config['device'])\n",
    "            # optimizer load\n",
    "            self.opt_G.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
    "            # parameters\n",
    "            self.start_epoch = G_checkpoint['epoch'] + 1 # To continue to the next epoch instead of repeating  \n",
    "            self.val_value = G_checkpoint['val_value']\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            self.val_value = 0\n",
    "\n",
    "    def _set_train_dataloader(self):\n",
    "        \"\"\" Getting the list of cases for training and loading using MONAI (all into memory)\"\"\"\n",
    "        data_list = []\n",
    "        with open(self.config['data_split_json'], \"r\") as f:\n",
    "            split = json.load(f)\n",
    "\n",
    "        train_cases = split[\"train\"]\n",
    "        for train_case in train_cases:\n",
    "            complete_data_dict = {}\n",
    "            complete_data_dict[\"image\"] = join(self.config['vol_data_path'], train_case)\n",
    "            complete_data_dict[\"gt\"] = join(self.config['label_data_path'], train_case)\n",
    "            complete_data_dict[\"bridge_weight_map\"] = join(self.config['bridge_weight_map_path'], train_case)\n",
    "            # TODO in the json the pred_seg_logits needs to be changed!\n",
    "            complete_data_dict[\"pred_seg_logits\"] = join(self.config['pred_seg_logits'], train_case)\n",
    "            data_list.append(complete_data_dict)\n",
    "            \n",
    "            if self.config['debug']:\n",
    "                for i in range(30):\n",
    "                    data_list.append(complete_data_dict)\n",
    "                print(f\"training using case: {data_list[0]}\")\n",
    "                break  # repeat 30 cases for debug mode\n",
    "\n",
    "        print(f\"Train cases: {len(train_cases)}\")\n",
    "        print(f\"Some examples:\")\n",
    "        print(train_cases[:5])\n",
    "\n",
    "        transforms_list = [   \n",
    "                # Load image \n",
    "                LoadImaged(keys=[\"image\", 'gt', 'bridge_weight_map', 'pred_seg_logits']),\n",
    "                EnsureChannelFirstd(keys=[\"image\", 'gt', 'bridge_weight_map', 'pred_seg_logits']),\n",
    "\n",
    "                # Normalize uint8 input\n",
    "                ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=255, b_min=0, b_max=1, clip=True),\n",
    "\n",
    "                # Create a ROI mask for cropping \n",
    "                GetROIMaskdd(keys=[\"gt\"], ignore_mask_value=2, new_key_names=[\"roi_mask\"]),\n",
    "\n",
    "                # Cropping or padding if bigger or smaller (expected to be all equall of smaller)\n",
    "                ResizeWithPadOrCropd(keys=[\"image\", 'gt', 'roi_mask', 'bridge_weight_map', 'pred_seg_logits'], spatial_size=self.config['patch_size'], mode=\"minimum\"),\n",
    "                GetBinaryLabeld(keys=[\"gt\"], ignore_mask_value=2),\n",
    "                EnsureTyped(keys=[\"image\", \"gt\", \"roi_mask\", \"bridge_weight_map\", 'pred_seg_logits'], track_meta=False)\n",
    "        ]\n",
    "\n",
    "        transforms = Compose(transforms_list)\n",
    "        \n",
    "        print(\"Initializing Dataset...\")\n",
    "        train_ds = CacheDataset(\n",
    "            data=data_list, \n",
    "            transform=transforms, \n",
    "            cache_rate=self.config['train_cache_rate'],  \n",
    "            num_workers=self.config['num_workers'], \n",
    "            progress=True\n",
    "        )\n",
    "\n",
    "        print(\"Initializing Train DataLoader...\")\n",
    "        train_loader = monai.data.DataLoader(\n",
    "            train_ds, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            num_workers=self.config['num_workers'],\n",
    "            shuffle=True,      \n",
    "            pin_memory=True\n",
    "        )\n",
    "        return train_loader\n",
    "    \n",
    "    def _set_val_dataloader(self):\n",
    "        data_list = []\n",
    "        with open(self.config['data_split_json'], \"r\") as f:\n",
    "            split = json.load(f)\n",
    "\n",
    "        val_cases = split[\"val\"]\n",
    "\n",
    "        if self.config['debug']: \n",
    "            print(f\"Debug mode: Using training cases for validation dataloader\")\n",
    "            train_cases = split[\"train\"]\n",
    "            for train_case in train_cases:\n",
    "                complete_data_dict = {}\n",
    "                complete_data_dict[\"image\"] = join(self.config['vol_data_path'], train_case)\n",
    "                complete_data_dict[\"gt\"] = join(self.config['label_data_path'], train_case)\n",
    "                complete_data_dict[\"bridge_weight_map\"] = join(self.config['bridge_weight_map_path'], train_case)\n",
    "                complete_data_dict[\"pred_seg_logits\"] = join(self.config['pred_seg_logits'], train_case)\n",
    "                data_list.append(complete_data_dict)\n",
    "                print(f\"Validation using case: {train_case}\")\n",
    "                break  # The same training sample for validation in debug mode\n",
    "        else:\n",
    "            for val_case in val_cases:\n",
    "                complete_data_dict = {}\n",
    "                complete_data_dict[\"image\"] = join(self.config['vol_data_path'], val_case)\n",
    "                complete_data_dict[\"gt\"] = join(self.config['label_data_path'], val_case)\n",
    "                complete_data_dict[\"bridge_weight_map\"] = join(self.config['bridge_weight_map_path'], val_case)\n",
    "                complete_data_dict[\"pred_seg_logits\"] = join(self.config['pred_seg_logits'], val_case)\n",
    "                data_list.append(complete_data_dict)\n",
    "\n",
    "        print(f\"Val cases: {len(val_cases)}\")\n",
    "        print(f\"Some examples:\")\n",
    "        print(val_cases[:5])\n",
    "\n",
    "        transforms = Compose(\n",
    "            [   \n",
    "                # Load image \n",
    "                LoadImaged(keys=[\"image\", 'gt', 'bridge_weight_map', 'pred_seg_logits']),\n",
    "                EnsureChannelFirstd(keys=[\"image\", 'gt', 'bridge_weight_map', 'pred_seg_logits']),\n",
    "                # Normalize uint8 input\n",
    "                ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=255, b_min=0, b_max=1, clip=True),\n",
    "                # Create a ROI mask for cropping \n",
    "                GetROIMaskdd(keys=[\"gt\"], ignore_mask_value=2, new_key_names=[\"roi_mask\"]),\n",
    "                # Get random patches\n",
    "                ResizeWithPadOrCropd(keys=[\"image\", \"gt\", \"roi_mask\", 'bridge_weight_map', 'pred_seg_logits'], spatial_size=self.config['patch_size']),\n",
    "                GetBinaryLabeld(keys=[\"gt\"], ignore_mask_value=2),\n",
    "                EnsureTyped(keys=[\"image\", \"gt\", \"roi_mask\", 'bridge_weight_map', 'pred_seg_logits'], track_meta=False)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"Initializing Dataset...\")\n",
    "        val_ds = CacheDataset(\n",
    "            data=data_list, \n",
    "            transform=transforms, \n",
    "            cache_rate=self.config['val_cache_rate'],  \n",
    "            num_workers=self.config['num_workers'], \n",
    "            progress=True\n",
    "        )\n",
    "        \n",
    "        print(\"Initializing Val DataLoader...\")\n",
    "        val_loader = monai.data.DataLoader(val_ds, batch_size=1, num_workers=self.config['num_workers'])\n",
    "        return val_loader\n",
    "\n",
    "    def _set_optimizers(self):\n",
    "        \"\"\"Define the optimizer (e.g., Adam, SGD).\"\"\"\n",
    "        opt_g = optim.AdamW(self.G_model.parameters(), lr=self.config['learning_rate_G'])\n",
    "        opt_d = optim.AdamW(self.D_model.parameters(), lr=self.config['learning_rate_D'])\n",
    "        return opt_g, opt_d\n",
    "    \n",
    "    def _set_scheduler(self):\n",
    "        \"\"\"Define learning rate scheduler.\"\"\"\n",
    "        # If resuming, last_epoch should be start_epoch - 1\n",
    "        last_epoch = self.config.get('resume_epoch', 0) - 1 if self.config.get('resume') else -1\n",
    "        G_cosAnnealLR = CosineAnnealingLR(self.opt_G, self.config['num_epochs'], eta_min=0.0, last_epoch=last_epoch)\n",
    "        D_cosAnnealLR = CosineAnnealingLR(self.opt_D, self.config['num_epochs'], eta_min=0.0, last_epoch=last_epoch)\n",
    "        return G_cosAnnealLR, D_cosAnnealLR\n",
    "    \n",
    "    def saving_logic(self, best_val_value, val_avg_value, epoch):\n",
    "        \"\"\" Logic to save the best model and periodic checkpoints \"\"\"\n",
    "\n",
    "        self.G_model, self.D_model\n",
    "\n",
    "        if best_val_value < val_avg_value: \n",
    "            best_val_value = val_avg_value\n",
    "            # Dis\n",
    "            D_save_path = join(self.model_save_path, f\"D_model_best.pth\")\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_weights': self.D_model.state_dict(),  \n",
    "                    'optimizer_state_dict': self.opt_D.state_dict(),\n",
    "                    'val_value': val_avg_value,\n",
    "                }, D_save_path)\n",
    "            print(f\"Saved checkpoint: {D_save_path}\")\n",
    "            # Gen\n",
    "            G_save_path = join(self.model_save_path, f\"G_model_best.pth\")\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_weights': self.G_model.state_dict(),  \n",
    "                    'optimizer_state_dict': self.opt_G.state_dict(),\n",
    "                    'val_value': val_avg_value,\n",
    "                }, G_save_path)\n",
    "            print(f\"Saved checkpoint: {G_save_path}\")\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        if epoch % 10 == 0: \n",
    "            # Dis\n",
    "            D_save_path = join(self.model_save_path, f\"D_model_epoch.pth\")\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_weights': self.D_model.state_dict(),  \n",
    "                    'optimizer_state_dict': self.opt_D.state_dict(),\n",
    "                    'val_value': val_avg_value,\n",
    "                }, D_save_path)\n",
    "            print(f\"Saved checkpoint: {D_save_path}\")\n",
    "            # Gen\n",
    "            G_save_path = join(self.model_save_path, f\"G_model_epoch.pth\")\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_weights': self.G_model.state_dict(),  \n",
    "                    'optimizer_state_dict': self.opt_G.state_dict(),\n",
    "                    'val_value': val_avg_value,\n",
    "                }, G_save_path)\n",
    "            print(f\"Saved checkpoint: {G_save_path}\")\n",
    "        return best_val_value\n",
    "\n",
    "    def train_epoch(self, **kwargs):\n",
    "        \"\"\"Logic for a single training epoch. Returns average loss.\"\"\"\n",
    "        epoch = kwargs.get('epoch')\n",
    "        warmup_G = kwargs.get('warmup_G')\n",
    "        \n",
    "        D_epoch_loss = 0\n",
    "        G_epoch_loss = 0\n",
    "\n",
    "        D_per_criterio_loss = {\n",
    "            'loss_d_real' : 0,\n",
    "            'loss_d_fake' : 0\n",
    "        }\n",
    "        G_per_criterio_loss = {} # voxel wise metrics dict\n",
    "\n",
    "        self.G_model.train()\n",
    "        self.D_model.train()\n",
    "\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch}/{self.config['num_epochs']}\")\n",
    "\n",
    "        for idx, batch_dict in enumerate(pbar):\n",
    "            # batch_dict contains:\n",
    "                # image (volume normalized)\n",
    "                # gt (the real ground truth binary)\n",
    "                # the roi_mask (with the region to ignore 0 and to consider 1)\n",
    "                # weighted map for the bridge weight loss\n",
    "                # segmentation logits (pre-computed to save time!)\n",
    "            input_image = batch_dict['image'].to(self.config['device'])\n",
    "            ground_truth = batch_dict['gt'].to(self.config['device'])\n",
    "            roi_mask = batch_dict['roi_mask'].to(self.config['device'])\n",
    "            bridge_weight_map = batch_dict['bridge_weight_map'].to(self.config['device'])\n",
    "            pred_seg_logits = batch_dict['gt'].to(self.config['device']) # TODO change to pred_seg_logits\n",
    "\n",
    "            if not warmup_G:\n",
    "                #################################################\n",
    "                #  UPDATE DISCRIMINATOR\n",
    "                #################################################\n",
    "                self.opt_D.zero_grad()\n",
    "\n",
    "                # --- FP16 FORWARD PASS ---\n",
    "                with autocast(device_type=self.config['device']):\n",
    "                    # --- Train with REAL (CT + Ground Truth) ---\n",
    "                    # adding noise to the gt seg to avoid D to easily distinguish between real and fake\n",
    "                    noisy_real_gt = ground_truth + (torch.randn_like(ground_truth) * 0.05) \n",
    "                    input_D = torch.cat([input_image, noisy_real_gt], dim=1)\n",
    "                    prediction_D_real = self.D_model(input_D) # the prediction has the deep supervision! 3 outputs\n",
    "\n",
    "                    # Target for REAL is 1.0\n",
    "                    target_real = torch.ones_like(prediction_D_real) * 0.9 # we don't want overconfident predictions!\n",
    "                    loss_d_real = self.criterion_GAN(prediction_D_real, target_real)\n",
    "                    \n",
    "                    # --- Train with FAKE (CT + Refined Prediction) ---\n",
    "                    # Generate Fake\n",
    "                    input_G = torch.cat([input_image, pred_seg_logits], dim=1)\n",
    "                    with torch.no_grad():\n",
    "                        refined_logits = self.G_model(input_G)\n",
    "                        refined_probs = torch.sigmoid(refined_logits) # Convert logits to prob for D\n",
    "                    # adding noise here as well, as explained before\n",
    "                    noisy_fake_probs = refined_probs.detach() + (torch.randn_like(refined_probs) * 0.05)\n",
    "                    \n",
    "                    # Detach! Crucial! \n",
    "                    # We don't want to backprop into Generator yet.\n",
    "                    fake_pair = torch.cat([input_image, noisy_fake_probs], dim=1)\n",
    "                    prediction_D_fake = self.D_model(fake_pair)\n",
    "                    \n",
    "                    # Target for FAKE is 0.0\n",
    "                    target_fake = torch.zeros_like(prediction_D_fake)\n",
    "                    loss_d_fake = self.criterion_GAN(prediction_D_fake, target_fake)\n",
    "                    \n",
    "                    # Combine\n",
    "                    loss_d = (loss_d_real + loss_d_fake) * 0.5\n",
    "\n",
    "                # Backward D\n",
    "                self.scaler.scale(loss_d).backward()\n",
    "                self.scaler.step(self.opt_D)\n",
    "\n",
    "            #################################################\n",
    "            #  UPDATE GENERATOR\n",
    "            #  Goal: Fool the Discriminator (D outputs 1 for fake)\n",
    "            #        AND minimize segmentation error\n",
    "            #################################################\n",
    "            self.opt_G.zero_grad()\n",
    "            \n",
    "            with autocast(device_type=self.config['device']):\n",
    "                # Adversarial Loss (Fool D)\n",
    "                # Construct the pair again (this time WITH gradients)\n",
    "                # Use probabilities for D input\n",
    "                input_G = torch.cat([input_image, pred_seg_logits], dim=1)\n",
    "                refined_logits = self.G_model(input_G)\n",
    "                refined_probs_for_g = torch.sigmoid(refined_logits) \n",
    "                input_D_for_G = torch.cat([input_image, refined_probs_for_g], dim=1)\n",
    "                \n",
    "                pred_D_for_G = self.D_model(input_D_for_G)\n",
    "                \n",
    "                # Generator wants D to say \"1.0\" (Real)\n",
    "                target_G = torch.ones_like(pred_D_for_G)\n",
    "                loss_g_adv = self.criterion_GAN(pred_D_for_G, target_G)\n",
    "                \n",
    "                # Segmentation Loss (Ground Truth Accuracy)\n",
    "                # Compare refined_logits directly to real_gt_mask\n",
    "                voxel_wise_loss, G_losses_dict = self.G_voxel_criterion([refined_logits], [ground_truth], roi_mask=[roi_mask], bridge_weight_map=[bridge_weight_map]) \n",
    "                G_losses_dict['loss_g_adv'] = loss_g_adv.item() # to save it latter\n",
    "                # Combine\n",
    "                # loss_seg dominates (weight 100), loss_adv refines\n",
    "                loss_g_total = (self.labda_seg * voxel_wise_loss) + (loss_g_adv)\n",
    "\n",
    "            # Backward G\n",
    "            self.scaler.scale(loss_g_total).backward()\n",
    "            self.scaler.step(self.opt_G)\n",
    "            \n",
    "            # Update Scaler once per batch\n",
    "            self.scaler.update()\n",
    "            \n",
    "            \n",
    "            ##### Handle loss graphs #####\n",
    "            if not warmup_G:\n",
    "                # Dis\n",
    "                # overall loss\n",
    "                D_epoch_loss += loss_d.item()\n",
    "                # Per real vs fake loss\n",
    "                D_per_criterio_loss['loss_d_real'] += loss_d_real.item()\n",
    "                D_per_criterio_loss['loss_d_fake'] += loss_d_fake.item()\n",
    "            \n",
    "\n",
    "            # Gen\n",
    "            # Overall loss\n",
    "            G_epoch_loss += loss_g_total.item()\n",
    "            \n",
    "            # adding all individual metrics to the G dict\n",
    "            for G_criterio_name in G_losses_dict.keys():\n",
    "                if G_criterio_name in G_per_criterio_loss:\n",
    "                    G_per_criterio_loss[G_criterio_name] += G_losses_dict[G_criterio_name]\n",
    "                else:\n",
    "                    G_per_criterio_loss[G_criterio_name] = G_losses_dict[G_criterio_name]\n",
    "            \n",
    "            # Update status bar\n",
    "            if not warmup_G:\n",
    "                # Update status bar\n",
    "                pbar.set_postfix({\n",
    "                    \"G_Loss\": loss_g_total.item(), \n",
    "                    \"D_Loss\": loss_d.item()\n",
    "                })\n",
    "            else:\n",
    "                pbar.set_postfix({\n",
    "                    \"G_Loss\": loss_g_total.item()\n",
    "                })\n",
    "        \n",
    "        if epoch%10 == 0:\n",
    "            # Save a prediction\n",
    "            self.save_vol(refined_probs_for_g, join(self.preds_path, f\"epoch_{epoch}_pred_train.nii.gz\"))\n",
    "            self.save_vol(input_image, join(self.preds_path, f\"epoch_{epoch}_input_train.nii.gz\"))\n",
    "            self.save_vol(pred_seg_logits, join(self.preds_path, f\"epoch_{epoch}_pred_seg_logits_train.nii.gz\"))\n",
    "            self.save_vol(ground_truth, join(self.preds_path, f\"epoch_{epoch}_gt_train.nii.gz\"))\n",
    "\n",
    "        D_train_avg_loss = D_epoch_loss / len(self.train_loader)\n",
    "        G_train_avg_loss = G_epoch_loss / len(self.train_loader)\n",
    "        print(f\"Epoch {epoch} Finished. Avg D_Loss: {D_train_avg_loss:.6f} | Avg G_Loss: {G_train_avg_loss:.6f}\")\n",
    "        \n",
    "        # This will replace each element in the dict with the mean\n",
    "        for criterio_name in G_losses_dict.keys():\n",
    "            G_per_criterio_loss[criterio_name] = G_per_criterio_loss[criterio_name] / len(self.train_loader)\n",
    "        if not warmup_G:\n",
    "            D_per_criterio_loss['loss_d_real'] = D_per_criterio_loss['loss_d_real'] / len(self.train_loader)\n",
    "            D_per_criterio_loss['loss_d_fake'] = D_per_criterio_loss['loss_d_fake'] / len(self.train_loader)\n",
    "\n",
    "        return D_train_avg_loss, G_train_avg_loss, D_per_criterio_loss, G_per_criterio_loss\n",
    "    \n",
    "    def val(self, **kwargs):\n",
    "        \"\"\"Logic for evaluation. Returns a dictionary of metrics.\"\"\"\n",
    "        epoch = kwargs.get('epoch')\n",
    "        self.G_model.eval()\n",
    "        self.D_model.eval()\n",
    "        \n",
    "        # General DSC validation value for quality controll\n",
    "        val_value_sum = 0\n",
    "        epoch_val_pixel_loss = 0\n",
    "        adv_fake = 0\n",
    "        adv_real = 0\n",
    "        # Add the per criterio val loss for checking overfitting\n",
    "        per_criterio_val_loss = {}\n",
    "        for val_criterio_name in self.config['criterion']:\n",
    "            per_criterio_val_loss[f\"val_{val_criterio_name}\"] = 0\n",
    "\n",
    "        pbar = tqdm(self.val_loader, desc=f\"Val epoch {epoch}/{self.config['num_epochs']}\")\n",
    "        for idx, batch_dict in enumerate(pbar):\n",
    "            input_image = batch_dict['image'].to(self.config['device'])\n",
    "            ground_truth = batch_dict['gt'].to(self.config['device']) \n",
    "            # Create the mask of the region to compute the loss\n",
    "            roi_mask = batch_dict['roi_mask'].to(self.config['device'])\n",
    "            bridge_weight_map = batch_dict['bridge_weight_map'].to(self.config['device'])\n",
    "            pred_seg_logits = batch_dict['pred_seg_logits'].to(self.config['device'])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Forward Pass\n",
    "                # --- REAL (CT + Ground Truth) ---\n",
    "                input_D = torch.cat([input_image, ground_truth], dim=1)\n",
    "                prediction_D_real = self.D_model(input_D) # the prediction has the deep supervision! 3 outputs\n",
    "                # Target for REAL is 1.0\n",
    "                target_real = torch.ones_like(prediction_D_real)\n",
    "                loss_d_real = self.criterion_GAN(prediction_D_real, target_real)\n",
    "                \n",
    "                # --- FAKE (CT + Refined Prediction) ---\n",
    "                # Generate Fake\n",
    "                input_G = torch.cat([input_image, pred_seg_logits], dim=1)\n",
    "                refined_logits = self.G_model(input_G)\n",
    "                refined_probs = torch.sigmoid(refined_logits) # Convert logits to prob for D\n",
    "                fake_pair = torch.cat([input_image, refined_probs.detach()], dim=1)\n",
    "                prediction_D_fake = self.D_model(fake_pair)\n",
    "                \n",
    "                # Target for FAKE is 0.0\n",
    "                target_fake = torch.zeros_like(prediction_D_fake)\n",
    "                loss_d_fake = self.criterion_GAN(prediction_D_fake, target_fake)\n",
    "\n",
    "                # Calculate DSC (Compare Prediction vs. GT)\n",
    "                val_value = self.val_metric(pred=refined_logits, target=ground_truth, roi_mask=roi_mask)\n",
    "                # Also compute val losses for logging (no deep supervision here)\n",
    "                val_loss, val_losses_dict = self.G_voxel_criterion([refined_logits], [ground_truth], roi_mask=[roi_mask], bridge_weight_map=[bridge_weight_map], deep_supervision_weights=[1.0]) \n",
    "                # commented to avoid overwhelming \n",
    "                #self.wandb_run.log({\"val_value\": val_value.item()})\n",
    "\n",
    "            val_value_sum += val_value # val metric (DSC)\n",
    "            epoch_val_pixel_loss += val_loss.item() # all losses function used for training (except Adv)\n",
    "            adv_fake += loss_d_fake.item()\n",
    "            adv_real += loss_d_real.item()\n",
    "            for val_criterio_name in val_losses_dict.keys():\n",
    "                per_criterio_val_loss[f\"val_{val_criterio_name}\"] += val_losses_dict[f\"{val_criterio_name}\"]\n",
    "            pbar.set_postfix({\"DSC\": val_value})\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            pred_save = sigmoid(refined_logits)\n",
    "            pred_save[pred_save>0.5] = 1.0\n",
    "            pred_save[pred_save<=0.5] = 0.0\n",
    "            self.save_vol(refined_logits, join(self.preds_path, f\"epoch_{epoch}_logits_val.nii.gz\"))\n",
    "            self.save_vol(pred_save, join(self.preds_path, f\"epoch_{epoch}_pred_val.nii.gz\"))\n",
    "            self.save_vol(input_image, join(self.preds_path, f\"epoch_{epoch}_input_val.nii.gz\"))\n",
    "            self.save_vol(pred_seg_logits, join(self.preds_path, f\"epoch_{epoch}_input_pred_seg_logits_val.nii.gz\"))\n",
    "            self.save_vol(ground_truth, join(self.preds_path, f\"epoch_{epoch}_gt_val.nii.gz\"))\n",
    "\n",
    "        # computing mean of metrics\n",
    "        val_avg_value = val_value_sum / len(self.val_loader)\n",
    "        val_avg_pixel_loss = epoch_val_pixel_loss / len(self.val_loader)\n",
    "\n",
    "        for val_criterio_name in val_losses_dict.keys():\n",
    "            per_criterio_val_loss[f\"val_{val_criterio_name}\"] = per_criterio_val_loss[f\"val_{val_criterio_name}\"] / len(self.val_loader)\n",
    "        per_criterio_val_loss[\"val_adv_fake\"] = adv_fake / len(self.val_loader)\n",
    "        per_criterio_val_loss[\"val_adv_real\"] = adv_real / len(self.val_loader)\n",
    "        print(f\"Epoch {epoch} with validation avg DSC: {val_avg_value:.6f} | avg Loss: {val_avg_pixel_loss:.6f} | adv fake loss {per_criterio_val_loss['val_adv_fake']} | adv real loss {per_criterio_val_loss['val_adv_real']}\")\n",
    "        return val_avg_value, val_avg_pixel_loss, per_criterio_val_loss \n",
    "    \n",
    "    def train_loop(self, **kwargs):\n",
    "        \"\"\"Standardized training loop.\"\"\"\n",
    "        best_val_value = self.val_value\n",
    "        \n",
    "        # Make sure all weights are trainable\n",
    "        for param in self.G_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.D_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for self.epoch in range(self.start_epoch, self.config['num_epochs'] + 1):\n",
    "            if self.epoch<2:\n",
    "                warmup_G = True\n",
    "            else:\n",
    "                warmup_G = False\n",
    "            # Train one epoch\n",
    "            D_train_avg_loss, G_train_avg_loss, D_per_criterio_loss, G_per_criterio_loss = self.train_epoch(\n",
    "                epoch=self.epoch,\n",
    "                warmup_G=warmup_G\n",
    "            )\n",
    "            # Perform evaluation \n",
    "            val_avg_value, val_avg_pixel_loss, per_criterio_val_loss  = self.val(\n",
    "                epoch=self.epoch\n",
    "            )\n",
    "\n",
    "            # Save in wandb\n",
    "            log_train_data = {\n",
    "                    \"epoch_train\": self.epoch,\n",
    "                    \"D_train_avg_loss\": D_train_avg_loss,\n",
    "                    \"G_train_avg_loss\": G_train_avg_loss,\n",
    "                    \"val_Dice\": val_avg_value,\n",
    "                    \"val_avg_pixel_loss\": val_avg_pixel_loss,\n",
    "                    \"G_lr\": self.opt_G.param_groups[0]['lr'],\n",
    "                    \"D_lr\": self.opt_D.param_groups[0]['lr']  \n",
    "                }\n",
    "            # Add the per criterio losses to wandb\n",
    "            for criterio_name in D_per_criterio_loss.keys():\n",
    "                log_train_data[criterio_name] = D_per_criterio_loss[criterio_name]\n",
    "\n",
    "            for criterio_name in G_per_criterio_loss.keys():\n",
    "                log_train_data[criterio_name] = G_per_criterio_loss[criterio_name]\n",
    "                if criterio_name.endswith(\"_fullres\"):\n",
    "                    print(f\"_fullres is still in the loss function! It should not!\")\n",
    "\n",
    "\n",
    "            for val_criterio_name in per_criterio_val_loss.keys():\n",
    "                log_train_data[val_criterio_name] = per_criterio_val_loss[val_criterio_name]\n",
    "\n",
    "            self.wandb_run.log(\n",
    "                log_train_data\n",
    "            )\n",
    "\n",
    "            # Checking if saving \n",
    "            best_val_value = self.saving_logic(\n",
    "                best_val_value=best_val_value, \n",
    "                val_avg_value=val_avg_value, \n",
    "                epoch=self.epoch\n",
    "            )\n",
    "\n",
    "            # Applying learning rate Cosine Annealing\n",
    "            self.G_cosAnnealLR.step() \n",
    "            self.D_cosAnnealLR.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284d273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cases: 707\n",
      "Some examples:\n",
      "['3925240194.nii.gz', '38034250.nii.gz', '196724516.nii.gz', '2220787575.nii.gz', '599381487.nii.gz']\n",
      "Initializing Dataset...\n",
      "Initializing Train DataLoader...\n",
      "Val cases: 79\n",
      "Some examples:\n",
      "['693501383.nii.gz', '118041886.nii.gz', '571334887.nii.gz', '2536049117.nii.gz', '1127903126.nii.gz']\n",
      "Initializing Dataset...\n",
      "Initializing Val DataLoader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshadowtwin\u001b[0m (\u001b[33mfaking_it\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../checkpoints/post_processing_step/wandb/run-20260121_092047-5bab9rh7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/faking_it/Vesuvius/runs/5bab9rh7' target=\"_blank\">main_post-processing_Loss_1.0BCE_1.0DSC__21-1-2026</a></strong> to <a href='https://wandb.ai/faking_it/Vesuvius' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/faking_it/Vesuvius' target=\"_blank\">https://wandb.ai/faking_it/Vesuvius</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/faking_it/Vesuvius/runs/5bab9rh7' target=\"_blank\">https://wandb.ai/faking_it/Vesuvius/runs/5bab9rh7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/100:   0%|          | 0/354 [00:00<?, ?it/s]Input Channel 1 looks like Probabilities (0-1), but the network expects Logits (-inf to inf). Residual learning will fail!\n",
      "Epoch 0/100: 100%|| 354/354 [10:10<00:00,  1.72s/it, G_Loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Finished. Avg D_Loss: 0.000000 | Avg G_Loss: 1.841105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val epoch 0/100: 100%|| 79/79 [01:03<00:00,  1.24it/s, DSC=1]    \n",
      "Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with validation avg DSC: 0.999960 | avg Loss: 1.291701 | adv fake loss 0.1591050332482857 | adv real loss 1.4961232339279562\n",
      "Saved checkpoint: ../checkpoints/post_processing_step/wandb/run-20260121_092047-5bab9rh7/files/model/D_model_best.pth\n",
      "Saved checkpoint: ../checkpoints/post_processing_step/wandb/run-20260121_092047-5bab9rh7/files/model/G_model_best.pth\n",
      "Saved checkpoint: ../checkpoints/post_processing_step/wandb/run-20260121_092047-5bab9rh7/files/model/D_model_epoch.pth\n",
      "Saved checkpoint: ../checkpoints/post_processing_step/wandb/run-20260121_092047-5bab9rh7/files/model/G_model_epoch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|| 354/354 [10:02<00:00,  1.70s/it, G_Loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Finished. Avg D_Loss: 0.000000 | Avg G_Loss: 1.803977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val epoch 1/100: 100%|| 79/79 [01:01<00:00,  1.29it/s, DSC=0.994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with validation avg DSC: 0.994639 | avg Loss: 1.270136 | adv fake loss 0.15604545251478122 | adv real loss 1.496125954615919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   8%|         | 29/354 [00:59<11:07,  2.05s/it, G_Loss=1.77]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     config_content = json.load(f)\n\u001b[32m      4\u001b[39m GANs_train_object = postprocessGANs(config_content)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mGANs_train_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 541\u001b[39m, in \u001b[36mpostprocessGANs.train_loop\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m     warmup_G = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# Train one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m D_train_avg_loss, G_train_avg_loss, D_per_criterio_loss, G_per_criterio_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_G\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_G\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# Perform evaluation \u001b[39;00m\n\u001b[32m    546\u001b[39m val_avg_value, val_avg_pixel_loss, per_criterio_val_loss  = \u001b[38;5;28mself\u001b[39m.val(\n\u001b[32m    547\u001b[39m     epoch=\u001b[38;5;28mself\u001b[39m.epoch\n\u001b[32m    548\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 364\u001b[39m, in \u001b[36mpostprocessGANs.train_epoch\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m    360\u001b[39m     \u001b[38;5;66;03m# Adversarial Loss (Fool D)\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;66;03m# Construct the pair again (this time WITH gradients)\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;66;03m# Use probabilities for D input\u001b[39;00m\n\u001b[32m    363\u001b[39m     input_G = torch.cat([input_image, pred_seg_logits], dim=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     refined_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mG_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_G\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     refined_probs_for_g = torch.sigmoid(refined_logits) \n\u001b[32m    366\u001b[39m     input_D_for_G = torch.cat([input_image, refined_probs_for_g], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/notebooks/../utils/GANs_networks.py:116\u001b[39m, in \u001b[36mGenerator.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Input Logits (Assumption: Input Ch 1 is ALREADY logits)\u001b[39;00m\n\u001b[32m    115\u001b[39m input_logits = x[:, \u001b[32m1\u001b[39m:\u001b[32m2\u001b[39m, :, :, :]\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;129;01mand\u001b[39;00m input_logits.max() <= \u001b[32m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43minput_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m >= \u001b[32m0.0\u001b[39m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    118\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mInput Channel 1 looks like Probabilities (0-1), but the network expects Logits (-inf to inf). Residual learning will fail!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x769b37980890>> (for post_run_cell), with arguments args (<ExecutionResult object at 769b39e45cd0, execution_count=2 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 769b37c0e6d0, raw_cell=\"CONFIG_FILE = '../configs/post_process_GANs.json'\n",
      "..\" transformed_cell=\"CONFIG_FILE = '../configs/post_process_GANs.json'\n",
      "..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/notebooks/11_GANs_postprocess.ipynb#X21sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:604\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:811\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    810\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:392\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vesuvius/lib/python3.11/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = '../configs/post_process_GANs.json'\n",
    "with open(CONFIG_FILE, \"r\") as f:\n",
    "    config_content = json.load(f)\n",
    "GANs_train_object = postprocessGANs(config_content)\n",
    "GANs_train_object.train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check if the D_loss is within the value expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuvius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
