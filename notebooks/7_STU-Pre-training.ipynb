{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95281584",
   "metadata": {},
   "source": [
    "# We will take the pre-trained STU-Net (Large -> Pre-trained on the TotalSegmentator cases) and further pre-train on the scrolls and fragments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "299dee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "PATCH_SIZE = (128,128,128)\n",
    "CRITERION = \"L1\"\n",
    "LR_SCHEDULER = \"CosineAnnealingLR\"\n",
    "DEVICE = 'cuda'\n",
    "SAVE_DIR = \"/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/checkpoints/5_scrolls_pretrain\"\n",
    "CHECKPOINT_PATH = \"/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/checkpoints/pre-trained/Independent/binary_large_ep4k.pth\"\n",
    "DATA_PATH = \"/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training\"\n",
    "VAL_DATA_PATH = \"/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Challenge_dataset\"\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "RESUME = \"/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/checkpoints/5_scrolls_pretrain/wandb/run-20251216_175136-5j432vpd/files/model/model_epoch_10.pth\"\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f11b5",
   "metadata": {},
   "source": [
    "## Building the data loader (DONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37bdefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "from os import listdir, makedirs\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import zarr\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import nibabel as nib\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# MONAI\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureType,\n",
    "    RandCoarseDropout,\n",
    "    RandCropByPosNegLabel,\n",
    "    RandFlip,\n",
    "    RandGaussianNoise,\n",
    "    RandRotate90,\n",
    "    RandSpatialCrop,\n",
    "    ScaleIntensity,\n",
    "    ScaleIntensityRange,\n",
    "    LoadImage,\n",
    "    CopyItemsd,\n",
    "    LoadImaged, \n",
    "    ScaleIntensityRanged, \n",
    "    ResizeWithPadOrCropd, \n",
    "    RandCoarseDropoutd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd\n",
    ")\n",
    "from monai.data import DataLoader, CacheDataset\n",
    "\n",
    "from stunet_model import STUNetReconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6c91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZarrVolumeDataset(Dataset):\n",
    "    def __init__(self, zarr_path, transform_input, transform_deform, patch_size=(128, 128, 128), threshold=0.0):\n",
    "        \"\"\"\n",
    "        it expects to receive:\n",
    "            zarr_path -> path to the root zarr folder\n",
    "            transform_input -> MONAI transforms to load the data\n",
    "            transform_deform -> MONAI transforms for self-supervised training\n",
    "            patch_size -> Patch size (double check if a pre-trained network is being used)\n",
    "            threshold -> Only returns the data if any voxel inside of the patch is greater than threshold.\n",
    "\n",
    "        \"\"\"\n",
    "        self.zarr_path = zarr_path\n",
    "        self.transform_input = transform_input\n",
    "        self.transform_deform = transform_deform\n",
    "        self.patch_size = patch_size\n",
    "        self.threshold = threshold  # Value below which we consider the pixel \"background\"\n",
    "\n",
    "        print(f\"Loading data from -> {zarr_path}\")\n",
    "\n",
    "        self.zarr_vols = []\n",
    "        for zarr_folder in listdir(zarr_path):\n",
    "            if zarr_folder.endswith(\".zarr\"):\n",
    "                complete_zarr_path = join(zarr_path, zarr_folder)\n",
    "\n",
    "                root = zarr.open(complete_zarr_path, mode='r')\n",
    "                \n",
    "                if 'volume' in root:\n",
    "                    vol = root['volume']\n",
    "                else:\n",
    "                    vol = root['0']\n",
    "\n",
    "                shape = vol.shape\n",
    "                entry = {\n",
    "                    \"name\": zarr_folder,\n",
    "                    \"volume\": vol,\n",
    "                    \"shape\": shape\n",
    "                }\n",
    "                self.zarr_vols.append(entry)\n",
    "\n",
    "        # MONAI cropper \n",
    "        self.cropper = RandSpatialCrop(\n",
    "            roi_size=patch_size,\n",
    "            random_size=False\n",
    "        )\n",
    "  \n",
    "    def __len__(self):\n",
    "        # Defining a length of one epoch\n",
    "        return 100 # TODO replace by 1000\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Transformations on the fly\n",
    "        # Using lazy loader (memory doesn't handle such big data)\n",
    "        random_entry = random.choice(self.zarr_vols)\n",
    "        vol = random_entry['volume'] \n",
    "        shape = random_entry['shape'] \n",
    "        print(f\"TODO remove -> vol name {random_entry['name']}\")\n",
    "\n",
    "        z_max = max(0, shape[0] - self.patch_size[0])\n",
    "        y_max = max(0, shape[1] - self.patch_size[1])\n",
    "        x_max = max(0, shape[2] - self.patch_size[2])\n",
    "\n",
    "        # --- THE REJECTION SAMPLING LOOP ---\n",
    "        # Try up to 100 times to find a non-empty chunk (very likely to find one!)\n",
    "        for attempt in range(100):\n",
    "            # We'll do it ourselves, it's easier to understand\n",
    "            # 1. Random Coordinates\n",
    "            z_start = np.random.randint(0, z_max) if z_max > 0 else 0\n",
    "            y_start = np.random.randint(0, y_max) if y_max > 0 else 0\n",
    "            x_start = np.random.randint(0, x_max) if x_max > 0 else 0\n",
    "\n",
    "            # 2. Load the chunk\n",
    "            patch = vol[\n",
    "                z_start : z_start + self.patch_size[0],\n",
    "                y_start : y_start + self.patch_size[1],\n",
    "                x_start : x_start + self.patch_size[2]\n",
    "            ]\n",
    "\n",
    "            # 3. Check if it contains data\n",
    "            # If the max value in this patch is greater than our threshold (0), it's valid.\n",
    "            if np.max(patch) > self.threshold:\n",
    "                # Found valid data! Break the loop and process it.\n",
    "                break\n",
    "            \n",
    "            # If we are here, the patch was empty. The loop continues to the next attempt.\n",
    "        \n",
    "        # Note: If the loop finishes 20 times and finds nothing, it will return the LAST empty patch.\n",
    "        # This prevents the code from hanging forever if the file is truly empty.\n",
    "\n",
    "        # 4. MONAI Formatting\n",
    "        patch = patch.astype(np.float32) # Ensure float for transforms\n",
    "        patch = patch[np.newaxis, ...]   # Add Channel dim -> (1, Z, Y, X)\n",
    "        \n",
    "        #if self.transform:\n",
    "        patch = self.transform_input(patch)\n",
    "        clean_patch = patch.clone()\n",
    "        deform_patch = self.transform_deform(patch)\n",
    "\n",
    "        return {\n",
    "            'clean_patch':clean_patch,\n",
    "            'deform_patch':deform_patch\n",
    "        }\n",
    "\n",
    "def describe_tensor(name, t):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  type:  {type(t)}\")\n",
    "    print(f\"  dtype: {t.dtype}\")\n",
    "    print(f\"  device:{t.device}\")\n",
    "    print(f\"  shape: {tuple(t.shape)}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3a9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # --- Setup ---\n",
    "    transform_input = Compose([\n",
    "        # Load image will be handeled by the lazzy zarr loading data\n",
    "        ScaleIntensityRange(a_min=0, a_max=255, b_min=0, b_max=1, clip=True),\n",
    "        # Not do it, we have a lot of data for pre-training\n",
    "        # RandFlip(\n",
    "        #    prob=0.1, \n",
    "        #    spatial_axis=None\n",
    "        #    ),\n",
    "        # RandRotate90(\n",
    "        #    prob=0.1, \n",
    "        #    max_k=3, \n",
    "        #    spatial_axes=(0, 1), \n",
    "        #    lazy=False)\n",
    "    ])\n",
    "\n",
    "    # The Corruption Transforms\n",
    "    # We want to force the model to fix heavy defects.\n",
    "    transform_deform = Compose([\n",
    "        # Cut out 10 holes, each spatial size roughly 32x48x48\n",
    "        # 35% of data loss\n",
    "        RandCoarseDropout(\n",
    "            holes=10, \n",
    "            spatial_size=(32, 48, 48), \n",
    "            fill_value=0,\n",
    "            prob=1.0 # Always apply\n",
    "        ),\n",
    "        # Add noise (not do it)\n",
    "        #RandGaussianNoise(prob=0.5, mean=0.0, std=0.1),\n",
    "        EnsureType()\n",
    "    ])\n",
    "\n",
    "    print(\"Initializing Dataset...\")\n",
    "    ds = ZarrVolumeDataset(\n",
    "        \"/mounts/disk2/Andre_Data_Augmentation/PhD/Vesuvius\", \n",
    "        transform_input=transform_input,\n",
    "        transform_deform=transform_deform,\n",
    "        patch_size=(128, 128, 128) \n",
    "    )\n",
    "\n",
    "    print(\"Initializing DataLoader...\")\n",
    "    loader = monai.data.DataLoader(ds, batch_size=1, num_workers=4)\n",
    "    data_loader = iter(loader)\n",
    "    first_batch = next(data_loader)\n",
    "    clean_patch = first_batch['clean_patch']\n",
    "    deform_patch = first_batch['deform_patch']\n",
    "    print(\"Fetching a batch to ensure it's not empty...\")\n",
    "\n",
    "    print(f\"Batch Max Value: {clean_patch.max()}\")\n",
    "    if clean_patch.max() == 0:\n",
    "        print(\"WARNING: The batch is still empty. Your threshold might be too high or the volume is empty.\")\n",
    "    else:\n",
    "        print(\"Success! Loaded a non-empty chunk.\")\n",
    "\n",
    "    print(f\"Success! Batch shape: {clean_patch.shape}\")\n",
    "\n",
    "    first_batch = next(data_loader)\n",
    "    clean_patch = first_batch['clean_patch']\n",
    "    deform_patch = first_batch['deform_patch']\n",
    "    describe_tensor(\"clean_patch\", clean_patch)\n",
    "    describe_tensor(\"deform_patch\", deform_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1476980",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ipywidgets import interact, IntSlider\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    # 1. Extract the raw 3D volume from the batch\n",
    "    # MONAI batch shape is (Batch_Size, Channel, Dim1, Dim2, Dim3)\n",
    "    # We select Batch 0 and Channel 0\n",
    "    input_data = deform_patch[0, 0].cpu().numpy()\n",
    "\n",
    "    print(f\"Batch Shape: {clean_patch.shape}\")\n",
    "    print(f\"Visualizing Sample Shape: {clean_patch.shape}\")\n",
    "\n",
    "    # 2. Setup Interactive Viewer\n",
    "    def view_batch_slice(slice_idx, axis):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        \n",
    "        # Allow slicing along different axes to debug orientation\n",
    "        if axis == 0:\n",
    "            # Slicing the first dimension (usually Z if (Z, Y, X))\n",
    "            plt.imshow(input_data[slice_idx, :, :], cmap='gray')\n",
    "            plt.xlabel(\"Axis 2\")\n",
    "            plt.ylabel(\"Axis 1\")\n",
    "        elif axis == 1:\n",
    "            # Slicing the second dimension\n",
    "            plt.imshow(input_data[:, slice_idx, :], cmap='gray')\n",
    "            plt.xlabel(\"Axis 2\")\n",
    "            plt.ylabel(\"Axis 0\")\n",
    "        else:\n",
    "            # Slicing the third dimension\n",
    "            plt.imshow(input_data[:, :, slice_idx], cmap='gray')\n",
    "            plt.xlabel(\"Axis 1\")\n",
    "            plt.ylabel(\"Axis 0\")\n",
    "            \n",
    "        plt.title(f\"Slice {slice_idx} along Axis {axis}\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    # 3. Create Slider\n",
    "    # We default to Axis 0, but you can change the axis variable below to 1 or 2\n",
    "    axis_to_scroll = 0 \n",
    "\n",
    "    interact(\n",
    "        view_batch_slice, \n",
    "        slice_idx=IntSlider(\n",
    "            min=0, \n",
    "            max=input_data.shape[axis_to_scroll]-1, \n",
    "            step=1, \n",
    "            value=input_data.shape[axis_to_scroll]//2,\n",
    "            description='Slice'\n",
    "        ),\n",
    "        axis=IntSlider(min=0, max=2, step=1, value=0, description='View Axis')\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ab577",
   "metadata": {},
   "source": [
    "## Building the pre-training process\n",
    "* The technique will be simple masking (oclusion) and respective reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ed1284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-12-2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshadowtwin\u001b[0m (\u001b[33mfaking_it\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/checkpoints/5_scrolls_pretrain/wandb/run-20251216_194000-5depvky3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/faking_it/Vesuvius/runs/5depvky3' target=\"_blank\">Pre_training_with_5_scrolls_16-12-2025</a></strong> to <a href='https://wandb.ai/faking_it/Vesuvius' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/faking_it/Vesuvius' target=\"_blank\">https://wandb.ai/faking_it/Vesuvius</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/faking_it/Vesuvius/runs/5depvky3' target=\"_blank\">https://wandb.ai/faking_it/Vesuvius/runs/5depvky3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "\n",
    "date_string = f\"{x.day}-{x.month}-{x.year}\"\n",
    "print(date_string)\n",
    "\n",
    "# Initialize your wandb run and specify the directory\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"faking_it\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"Vesuvius\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"architecture\": \"STU-Net\",\n",
    "        \"dataset\": \"5 scrolls\",\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"patch_size\": PATCH_SIZE,\n",
    "        \"criterion\": CRITERION,\n",
    "        \"lr_scheduler\": LR_SCHEDULER,\n",
    "        \"save_dir\": SAVE_DIR,\n",
    "        \"checkpoint_path\": CHECKPOINT_PATH\n",
    "    },\n",
    "    name=f\"Pre_training_with_5_scrolls_{date_string}\", \n",
    "    dir=SAVE_DIR\n",
    ")\n",
    "\n",
    "MODEL_SAVE_PATH = join(run.dir, \"model\")\n",
    "makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "PREDS_PATH = join(run.dir, \"preds\")\n",
    "makedirs(PREDS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef1408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vol(tensor, path):\n",
    "    # prediction: torch.Tensor\n",
    "    # shape example: [B, C, H, W] or [B, 1, H, W]\n",
    "\n",
    "    tensor_cpu = tensor.detach().cpu()\n",
    "\n",
    "    # Remove batch and channel dims if needed\n",
    "    tensor_cpu = tensor_cpu[0]          \n",
    "    tensor_cpu = tensor_cpu.squeeze(0)  \n",
    "\n",
    "    tensor_np = tensor_cpu.numpy()\n",
    "\n",
    "    affine = np.eye(4)  # identity affine (OK if no spatial metadata)\n",
    "\n",
    "    nii = nib.Nifti1Image(tensor_np.astype(np.float32), affine)\n",
    "    nib.save(nii, path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_loader, train_criterion, optimizer, epoch, scaler):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    for idx, batch_dict in enumerate(pbar):\n",
    "        clean_patch = batch_dict['clean_patch'].to(DEVICE)\n",
    "        deform_patch = batch_dict['deform_patch'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # --- FP16 FORWARD PASS ---\n",
    "        with autocast(device_type=DEVICE):\n",
    "            # Forward Pass\n",
    "            # The model tries to predict the CLEAN image from the DEFORMED input\n",
    "            prediction = model(deform_patch)\n",
    "            # Calculate Loss (Compare Prediction vs. Clean)\n",
    "            train_loss = train_criterion(prediction, clean_patch) # TODO make this with only the region inpainted (not the entire volume!)\n",
    "\n",
    "            # commented to avoid overwhelming \n",
    "            # run.log(\n",
    "            #     {\n",
    "            #         \"train_loss\": train_loss.item(),\n",
    "            #         \"train_step\": epoch*len(train_loader)+idx\n",
    "                \n",
    "            #     }\n",
    "            # )\n",
    "            \n",
    "        # Backward\n",
    "        scaler.scale(train_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += train_loss.item()\n",
    "        pbar.set_postfix({\"Loss\": train_loss.item()})\n",
    "        break # TODO remove\n",
    "\n",
    "    train_avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch} Finished. Avg Loss: {train_avg_loss:.6f}\")\n",
    "    return model, optimizer, train_avg_loss\n",
    "\n",
    "def val(model, val_loader, val_criterion, epoch):\n",
    "    model.eval()\n",
    "    val_loss_sum = 0\n",
    "    pbar = tqdm(val_loader, desc=f\"Val epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    for batch_dict in pbar:\n",
    "        clean_patch = batch_dict['image'].to(DEVICE)\n",
    "        deform_patch = batch_dict['deform_patch'].to(DEVICE)\n",
    "        \n",
    "        # --- FP16 FORWARD PASS ---\n",
    "        with torch.no_grad():\n",
    "            # Forward Pass\n",
    "            # The model tries to predict the CLEAN image from the DEFORMED input\n",
    "            prediction = model(deform_patch)\n",
    "            # Calculate Loss (Compare Prediction vs. Clean)\n",
    "            val_loss = val_criterion(prediction, clean_patch) # TODO make this with only the region inpainted (not the entire volume!)\n",
    "            # commented to avoid overwhelming \n",
    "            #run.log({\"val_loss\": val_loss.item()})\n",
    "\n",
    "        val_loss_sum += val_loss.item()\n",
    "        pbar.set_postfix({\"Loss\": val_loss.item()})\n",
    "        break # TODO remove\n",
    "    # Save a prediction\n",
    "    save_vol(prediction, join(PREDS_PATH, f\"{epoch}_pred.nii.gz\"))\n",
    "    save_vol(deform_patch, join(PREDS_PATH, f\"{epoch}_deform.nii.gz\"))\n",
    "    save_vol(clean_patch, join(PREDS_PATH, f\"{epoch}_clean.nii.gz\"))\n",
    "    val_avg_loss = val_loss_sum / len(val_loader)\n",
    "    print(f\"Epoch {epoch} with validation avg Loss: {val_avg_loss:.6f}\")\n",
    "    return val_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010612e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_logic(best_loss, avg_loss, epoch, optimizer, model):\n",
    "    if best_loss > avg_loss: \n",
    "        best_loss = avg_loss\n",
    "        save_path = join(MODEL_SAVE_PATH, f\"model_best.pth\")\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_full': model,  # <--- This pickles the whole class! No need to have the network defined!\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_loss,\n",
    "            }, save_path)\n",
    "        print(f\"Saved checkpoint: {save_path}\")\n",
    "\n",
    "    # Save Checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        save_path = join(MODEL_SAVE_PATH, f\"model_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_full': model, \n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_loss,\n",
    "            }, save_path)\n",
    "        print(f\"Saved checkpoint: {save_path}\")\n",
    "    return best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56984e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, train_loader, val_loader, train_criterion, val_criterion, scheduler, scaler, epoch, val_loss):\n",
    "    best_val_loss = val_loss\n",
    "    \n",
    "    for epoch in range(epoch, NUM_EPOCHS + 1):\n",
    "        # Train one epoch\n",
    "        model, optimizer, train_avg_loss = epoch_train(\n",
    "            model=model, \n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer, \n",
    "            epoch=epoch, \n",
    "            scaler=scaler, \n",
    "            train_criterion=train_criterion\n",
    "        )\n",
    "\n",
    "        # Run validation step\n",
    "        val_avg_loss = val(\n",
    "            model=model, \n",
    "            epoch=epoch, \n",
    "            val_loader=val_loader, \n",
    "            val_criterion=val_criterion\n",
    "        )\n",
    "\n",
    "        # Save in wandb\n",
    "        run.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_avg_loss\": train_avg_loss,\n",
    "                \"val_avg_loss\": val_avg_loss\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Checking if saving \n",
    "        best_val_loss = saving_logic(\n",
    "            best_loss=best_val_loss, \n",
    "            avg_loss=val_avg_loss, \n",
    "            epoch=epoch, \n",
    "            optimizer=optimizer, \n",
    "            model=model\n",
    "        )\n",
    "\n",
    "        # Applying learning rate Cosine Annealing\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a6034ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zarr_dataloader():\n",
    "    # TODO define the data split logic for the training and validation\n",
    "    transform_input = Compose(\n",
    "        [\n",
    "            # Load image will be handeled by the lazzy zarr loading data\n",
    "            ScaleIntensityRange(a_min=0, a_max=255, b_min=0, b_max=1, clip=True),\n",
    "            EnsureType()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # The Corruption Transforms\n",
    "    # We want to force the model to fix heavy defects.\n",
    "    transform_deform = Compose(\n",
    "        [\n",
    "            # Cut out 10 holes, each spatial size roughly 32x48x48\n",
    "            # 35% of data loss\n",
    "            RandCoarseDropout(\n",
    "                holes=10, \n",
    "                spatial_size=(32, 48, 48), \n",
    "                fill_value=0,\n",
    "                prob=1.0 # Always apply\n",
    "            ),\n",
    "            # Add noise (not do it)\n",
    "            #RandGaussianNoise(prob=0.5, mean=0.0, std=0.1),\n",
    "            EnsureType()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Initializing Dataset...\")\n",
    "    ds = ZarrVolumeDataset(\n",
    "        zarr_path=DATA_PATH, \n",
    "        transform_input=transform_input,\n",
    "        transform_deform=transform_deform,\n",
    "        patch_size=PATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(\"Initializing DataLoader...\")\n",
    "    train_loader = monai.data.DataLoader(ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    return train_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab11cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nii_dataloader():\n",
    "    data_list = []\n",
    "\n",
    "    train_images_nii = join(VAL_DATA_PATH, 'train_images_nii')\n",
    "\n",
    "    for file_name in listdir(train_images_nii):\n",
    "        complete_path = join(train_images_nii, file_name)\n",
    "        data_list.append(\n",
    "            {\"image\": complete_path}\n",
    "        )\n",
    "\n",
    "    transforms = Compose(\n",
    "        [   \n",
    "            # Load image \n",
    "            LoadImaged(keys=[\"image\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image\"]),\n",
    "            # Normalize uint8 input\n",
    "            ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=255, b_min=0, b_max=1, clip=True),\n",
    "            ResizeWithPadOrCropd(keys=[\"image\"], spatial_size=PATCH_SIZE),\n",
    "            # Make a clean copy\n",
    "            # Copy the image tensor to a new key\n",
    "            CopyItemsd(keys=[\"image\"], times=1, names=[\"deform_patch\"]),\n",
    "\n",
    "            # Cut out 10 holes, each spatial size roughly 32x48x48\n",
    "            # 35% of data loss\n",
    "            RandCoarseDropoutd(\n",
    "                keys=[\"deform_patch\"],\n",
    "                holes=10, \n",
    "                spatial_size=(32, 48, 48), \n",
    "                fill_value=0,\n",
    "                prob=1.0 # Always apply\n",
    "            ),\n",
    "            EnsureTyped(keys=[\"image\", \"deform_patch\"], track_meta=False)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Initializing Dataset...\")\n",
    "    val_ds = CacheDataset(\n",
    "        data=data_list, \n",
    "        transform=transforms, \n",
    "        cache_rate=0.0,  # TODO change to 1\n",
    "        num_workers=NUM_WORKERS, \n",
    "        progress=True\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing DataLoader...\")\n",
    "    val_loader = monai.data.DataLoader(val_ds, batch_size=1, num_workers=NUM_WORKERS)\n",
    "    return val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca942610",
   "metadata": {},
   "source": [
    "# Test every step before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1042dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    try:\n",
    "        model = STUNetReconstruction()\n",
    "        state_dict = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        model.train()\n",
    "        model.cuda()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with model loading: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b8e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    try:\n",
    "        train_loader = get_zarr_dataloader()\n",
    "        temp_zarr_iterator = iter(train_loader)\n",
    "        one_batch = next(temp_zarr_iterator)\n",
    "        clean_patch = one_batch['clean_patch']\n",
    "        deform_patch = one_batch['deform_patch']\n",
    "        describe_tensor(\"clean_patch\", clean_patch)\n",
    "        describe_tensor(\"deform_patch\", deform_patch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with zarr data loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bbe9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    try:\n",
    "        val_loader = get_nii_dataloader()\n",
    "        temp_nii_iterator = iter(val_loader)\n",
    "        one_batch = next(temp_nii_iterator)\n",
    "        clean_patch = one_batch['image']\n",
    "        describe_tensor(\"clean_patch\", clean_patch)\n",
    "        deform_patch = one_batch['deform_patch']\n",
    "        describe_tensor(\"deform_patch\", deform_patch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with zarr data loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9795c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    epoch = 2\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    avg_loss = 2.0\n",
    "    save_path = join(MODEL_SAVE_PATH, f\"model_epoch_{epoch}.pth\")\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_full': model,  # <--- This pickles the whole class! No need to have the network defined!\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': avg_loss,\n",
    "        }, save_path)\n",
    "    print(f\"Saved checkpoint: {save_path}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3331df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/andre/.conda/envs/vesuvius/lib/python3.11/site-packages/zarr/codecs/numcodecs/_codecs.py:141: ZarrUserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded correctly. Resuming training...\n",
      "Initializing Dataset...\n",
      "Loading data from -> /mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training\n",
      "Initializing DataLoader...\n",
      "Initializing Dataset...\n",
      "Initializing DataLoader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO remove -> vol name PHerc332.zarrTODO remove -> vol name PHerc172.zarrTODO remove -> vol name PHerc332.zarr\n",
      "\n",
      "\n",
      "TODO remove -> vol name PHercParis3.zarr\n",
      "TODO remove -> vol name PHerc332.zarr\n",
      "TODO remove -> vol name PHerc172.zarr\n",
      "TODO remove -> vol name PHerc172.zarr\n",
      "TODO remove -> vol name PHerc332_masked.zarr\n",
      "TODO remove -> vol name PHerc332_masked.zarr\n",
      "TODO remove -> vol name PHercParis3.zarr\n",
      "TODO remove -> vol name PHerc332.zarrTODO remove -> vol name PHercParis3.zarr\n",
      "\n",
      "TODO remove -> vol name PHercParis3.zarr\n",
      "TODO remove -> vol name PHerc332.zarr\n",
      "TODO remove -> vol name PHercParis3.zarr\n",
      "TODO remove -> vol name PHerc332.zarr\n",
      "TODO remove -> vol name PHerc172.zarr\n",
      "TODO remove -> vol name PHercParis3.zarr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:   0%|          | 0/50 [00:04<?, ?it/s, Loss=0.0478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Finished. Avg Loss: 0.000956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val epoch 10/1000:   0%|          | 0/806 [00:02<?, ?it/s, Loss=0.079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 with validation avg Loss: 0.000098\n",
      "Saved checkpoint: /mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/checkpoints/5_scrolls_pretrain/wandb/run-20251216_194000-5depvky3/files/model/model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/andre/.conda/envs/vesuvius/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/checkpoints/5_scrolls_pretrain/wandb/run-20251216_194000-5depvky3/files/model/model_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO remove -> vol name PHerc332_masked.zarrTODO remove -> vol name PHerc172.zarrTODO remove -> vol name PHerc332.zarr\n",
      "\n",
      "\n",
      "TODO remove -> vol name PHerc172.zarr\n",
      "TODO remove -> vol name PHerc332.zarr\n",
      "TODO remove -> vol name PHerc332_masked.zarr\n",
      "TODO remove -> vol name PHerc332_masked.zarr\n",
      "TODO remove -> vol name PHerc332.zarr\n",
      "TODO remove -> vol name PHerc332_masked.zarr\n",
      "TODO remove -> vol name PHerc332_masked.zarr\n",
      "TODO remove -> vol name PHercParis3.zarr\n",
      "TODO remove -> vol name PHerc332_masked.zarr\n",
      "TODO remove -> vol name PHerc332.zarr\n",
      "TODO remove -> vol name PHerc332_masked.zarr\n",
      "TODO remove -> vol name PHerc332.zarr\n",
      "TODO remove -> vol name PHerc172.zarr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:   0%|          | 0/50 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     42\u001b[39m     train_loop(\n\u001b[32m     43\u001b[39m         model=model, \n\u001b[32m     44\u001b[39m         optimizer=optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m \n\u001b[32m     54\u001b[39m     )\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m==\u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43m__main__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36m__main__\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     39\u001b[39m scaler = GradScaler()\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Start training loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_criterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loss\u001b[49m\n\u001b[32m     53\u001b[39m \n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, optimizer, train_loader, val_loader, train_criterion, val_criterion, scheduler, scaler, epoch, val_loss)\u001b[39m\n\u001b[32m      2\u001b[39m best_val_loss = val_loss\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Train one epoch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     model, optimizer, train_avg_loss = \u001b[43mepoch_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_criterion\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Run validation step\u001b[39;00m\n\u001b[32m     16\u001b[39m     val_avg_loss = val(\n\u001b[32m     17\u001b[39m         model=model, \n\u001b[32m     18\u001b[39m         epoch=epoch, \n\u001b[32m     19\u001b[39m         val_loader=val_loader, \n\u001b[32m     20\u001b[39m         val_criterion=val_criterion\n\u001b[32m     21\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mepoch_train\u001b[39m\u001b[34m(model, train_loader, train_criterion, optimizer, epoch, scaler)\u001b[39m\n\u001b[32m     16\u001b[39m     train_loss = train_criterion(prediction, clean_patch)\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# commented to avoid overwhelming \u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# run.log(\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m#     {\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m scaler.step(optimizer)\n\u001b[32m     30\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/torch/_tensor.py:616\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[32m    574\u001b[39m \n\u001b[32m    575\u001b[39m \u001b[33;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    613\u001b[39m \u001b[33;03m        used to compute the :attr:`tensors`. Defaults to ``None``.\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m torch.autograd.backward(\n\u001b[32m    626\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    627\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/torch/overrides.py:1750\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m     warnings.warn(\n\u001b[32m   1743\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1744\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1745\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1746\u001b[39m     )\n\u001b[32m   1748\u001b[39m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[32m   1749\u001b[39m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m result = \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/monai/data/meta_tensor.py:283\u001b[39m, in \u001b[36mMetaTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    282\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/torch/_tensor.py:1654\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1651\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1656\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7711e2bcf290>> (for post_run_cell), with arguments args (<ExecutionResult object at 770f6a841910, execution_count=17 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 770f6a839d90, raw_cell=\"def __main__():\n",
      "    # Load model\n",
      "    model = STUNe..\" transformed_cell=\"def __main__():\n",
      "    # Load model\n",
      "    model = STUNe..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bgpuserver/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/notebooks/7_STU-Pre-training.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:604\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:811\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    810\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/asyncio/streams.py:392\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vesuvius/lib/python3.11/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "def __main__():\n",
    "    # Load model\n",
    "    model = STUNetReconstruction()\n",
    "    state_dict = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # Load states if resume\n",
    "    if RESUME == None:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        epoch = 0\n",
    "        val_loss = 10000\n",
    "        model = model.to(DEVICE)\n",
    "    else:\n",
    "        checkpoint = torch.load(RESUME, map_location=\"cpu\", weights_only=False) \n",
    "\n",
    "        epoch = checkpoint['epoch']\n",
    "        model = checkpoint['model_full']   # already reconstructed\n",
    "        val_loss = checkpoint['val_loss']\n",
    "\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        print(\"Model loaded correctly. Resuming training...\")\n",
    " \n",
    "    # Data loader\n",
    "    train_loader = get_zarr_dataloader()\n",
    "    val_loader = get_nii_dataloader()\n",
    "\n",
    "    # Loss\n",
    "    train_criterion = nn.L1Loss() # Sharpness preference (Better for restoration)\n",
    "    val_criterion = nn.L1Loss() # Using the same metric for evaluation\n",
    "\n",
    "    # Defining learning rate scheduler\n",
    "    scheduler = CosineAnnealingLR(optimizer, NUM_EPOCHS, eta_min=0.0, last_epoch=-1)\n",
    "\n",
    "    # FP16 initialization \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Start training loop\n",
    "    train_loop(\n",
    "        model=model, \n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        train_criterion=train_criterion, \n",
    "        val_criterion=val_criterion, \n",
    "        scheduler=scheduler, \n",
    "        scaler=scaler,\n",
    "        epoch=epoch,\n",
    "        val_loss=val_loss\n",
    "\n",
    "    )\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    __main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d3b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24b34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9dfa72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuvius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
