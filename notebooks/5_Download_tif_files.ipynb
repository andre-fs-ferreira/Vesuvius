{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5136931",
   "metadata": {},
   "source": [
    "# Download data for pre-training. \n",
    "* Each scroll is veryyyyy large, so we need to do some pre-processing on the fly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f423d2",
   "metadata": {},
   "source": [
    "## First we tried downloading per chunk (1024 slices per chunk)\n",
    "* We can download smaller portions of the dataset with this code\n",
    "* We only need to change the TARGET_URL\n",
    "* The idea was to download as much as data possible, but then we found later that we could do download then some cleaning to reduce the size. I recommend using the second approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137bff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "\"\"\"\n",
    "# List of available scrolls:\n",
    "['https://dl.ash2txt.org/full-scrolls/Scroll2/PHercParis3.volpkg/volumes_zarr_standardized/54keV_7.91um_Scroll2A.zarr', \n",
    "'http:full-scrolls/Scroll3/PHerc332.volpkg/volumes_zarr_standardized/53keV_7.91um_Scroll3.zarr',\n",
    "''\n",
    "]\n",
    "\"\"\"\n",
    "TARGET_URL = \"https://dl.ash2txt.org/full-scrolls/Scroll1/PHercParis4.volpkg/volumes_masked/20230205180739/\"\n",
    "DOWNLOAD_DIR = \"./temp_raw_downloads\"\n",
    "OUTPUT_DIR = \"../DataSet/Pre-training/cropped_output\"\n",
    "\n",
    "def get_tif_links(url):\n",
    "    \"\"\"Scrapes the directory for .tif files.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = [a['href'] for a in soup.find_all('a') if a['href'].endswith('.tif')]\n",
    "        return sorted(links)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching links: {e}\")\n",
    "        return []\n",
    "    \n",
    "def download_file(file_url, local_path):\n",
    "    \"\"\"Downloads a single file if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(local_path):\n",
    "        return\n",
    "    \n",
    "    with requests.get(file_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "def crop_foreground(tiff_path, bounds):\n",
    "\n",
    "    x_min = bounds['x_min']\n",
    "    x_max = bounds['x_max']\n",
    "    y_min = bounds['y_min']\n",
    "    y_max = bounds['y_max']\n",
    "    \n",
    "    # Load image\n",
    "    tiff_img = tifffile.imread(tiff_path)\n",
    "\n",
    "\n",
    "\n",
    "    # Crop\n",
    "    cropped = tiff_img[y_min:y_max+1, x_min:x_max+1]\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def crop_foreground(tiff_path, bounds):\n",
    "\n",
    "    x_min = bounds['x_min']\n",
    "    x_max = bounds['x_max']\n",
    "    y_min = bounds['y_min']\n",
    "    y_max = bounds['y_max']\n",
    "    \n",
    "    # Load image\n",
    "    tiff_img = tifffile.imread(tiff_path)\n",
    "\n",
    "\n",
    "\n",
    "    # Crop\n",
    "    cropped = tiff_img[y_min:y_max+1, x_min:x_max+1]\n",
    "    return cropped\n",
    "\n",
    "def get_small_sample_list(tif_files):\n",
    "    # Suppose tif_files is your list of file paths\n",
    "    n_samples = 100\n",
    "\n",
    "    # Handle case when there are fewer than 100 files\n",
    "    n_samples = min(n_samples, len(tif_files))\n",
    "\n",
    "    # Get 100 equally spaced indices\n",
    "    indices = np.linspace(0, len(tif_files)-1, n_samples, dtype=int)\n",
    "\n",
    "    # Select files\n",
    "    selected_files = [tif_files[i] for i in indices]\n",
    "    return selected_files\n",
    "\n",
    "def calculate_global_bounds(file_list):\n",
    "    \"\"\"\n",
    "    Iterates through 2D files to calculate the 3D bounding box \n",
    "    without loading the whole volume into RAM.\n",
    "    Implements your 'detect_deges_with_content' logic iteratively.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Scanning files to calculate Global Bounding Box ---\")\n",
    "    \n",
    "    # Initialize with inverted infinity to find min/max\n",
    "    min_x, max_x = float('inf'), float('-inf')\n",
    "    min_y, max_y = float('inf'), float('-inf')\n",
    "    min_z, max_z = float('inf'), float('-inf')\n",
    "    \n",
    "    has_data = False\n",
    "\n",
    "    for z_index, filename in enumerate(tqdm(file_list, desc=\"Scanning Geometry\")):\n",
    "        file_path = os.path.join(DOWNLOAD_DIR, filename)\n",
    "        \n",
    "        # Load 2D slice\n",
    "        img = tifffile.imread(file_path)\n",
    "        \n",
    "        # Check if slice has any data (Your Z-axis check)\n",
    "        if np.any(img > 0):\n",
    "            # Update Z bounds\n",
    "            if z_index < min_z: min_z = z_index\n",
    "            if z_index > max_z: max_z = z_index\n",
    "            \n",
    "            has_data = True\n",
    "            \n",
    "            # Check Y axis (Rows) for this slice\n",
    "            # np.any(img, axis=1) gives boolean array of rows containing data\n",
    "            y_indices = np.where(np.any(img > 0, axis=1))[0]\n",
    "            if len(y_indices) > 0:\n",
    "                current_y_min, current_y_max = y_indices[0], y_indices[-1]\n",
    "                min_y = min(min_y, current_y_min)\n",
    "                max_y = max(max_y, current_y_max)\n",
    "\n",
    "            # Check X axis (Cols) for this slice\n",
    "            # np.any(img, axis=0) gives boolean array of cols containing data\n",
    "            x_indices = np.where(np.any(img > 0, axis=0))[0]\n",
    "            if len(x_indices) > 0:\n",
    "                current_x_min, current_x_max = x_indices[0], x_indices[-1]\n",
    "                min_x = min(min_x, current_x_min)\n",
    "                max_x = max(max_x, current_x_max)\n",
    "\n",
    "    if not has_data:\n",
    "        return None\n",
    "\n",
    "    # Convert to integers\n",
    "    bounds = {\n",
    "        'x_min': int(min_x), 'x_max': int(max_x),\n",
    "        'y_min': int(min_y), 'y_max': int(max_y),\n",
    "        'z_min': int(min_z), 'z_max': int(max_z)\n",
    "    }\n",
    "    return bounds\n",
    "\n",
    "\n",
    "# 1. Setup\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# 2. Get File List\n",
    "print(\"Fetching file list...\")\n",
    "tif_files = get_tif_links(TARGET_URL)\n",
    "if not tif_files:\n",
    "    print(\"No files found.\")\n",
    "    exit()\n",
    "    \n",
    "print(f\"Found {len(tif_files)} files.\")\n",
    "\n",
    "# 3. Download (One by one), crop and store locally\n",
    "# We will divide by chunks of x, y, 1024 -> so we can still have volumes, a good representation of data and save disk space\n",
    "print(\"Downloading files...\")\n",
    "\n",
    "chunk_size = 1024\n",
    "\n",
    "chunk_list = [tif_files[i:i + chunk_size] for i in range(0, len(tif_files), chunk_size)]\n",
    "\n",
    "for chunk_idx, tif_files in enumerate(chunk_list):\n",
    "    print(f\"\\nProcessing chunk {chunk_idx+1}/{len(chunk_list)}...\")\n",
    "    OUTPUT_DIR_chunk = os.path.join(OUTPUT_DIR, f\"chunk_{chunk_idx}\")\n",
    "    os.makedirs(OUTPUT_DIR_chunk, exist_ok=True)\n",
    "    \n",
    "    for idx_z, f in enumerate(tqdm(tif_files, desc=\"Downloading\")):\n",
    "        url = urljoin(TARGET_URL, f)\n",
    "        path = os.path.join(DOWNLOAD_DIR, f)\n",
    "        download_file(url, path)\n",
    "  \n",
    "    # Calculate bounds\n",
    "    bounds = calculate_global_bounds(tif_files)\n",
    "    print(f\"Current bounds at slice {idx_z}: {bounds}\")\n",
    "    \n",
    "    for idx_z, f in enumerate(tqdm(tif_files, desc=\"Croping and Saving\")):\n",
    "        # crop edges without content, save and remove tmp file\n",
    "        path = os.path.join(DOWNLOAD_DIR, f)\n",
    "        cropped_numpy = crop_foreground(tiff_path=path, bounds=bounds)\n",
    "        output_path = os.path.join(OUTPUT_DIR_chunk, f)\n",
    "        tifffile.imwrite(output_path, cropped_numpy, compression='deflate')\n",
    "        os.remove(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to your tiffs\n",
    "TIFF_FOLDER = \"/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/notebooks/temp_raw_downloads\"\n",
    "files = sorted(glob.glob(os.path.join(TIFF_FOLDER, \"*.tif\")))\n",
    "\n",
    "print(f\"Scanning {len(files)} remaining files...\")\n",
    "\n",
    "for f in tqdm(files):\n",
    "    print(f\"{f}\")\n",
    "    # Just try to read the header/first byte to check validity\n",
    "    tifffile.imread(f)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052d9af-1e37-4587-92c8-8f4e3f0fd9e0",
   "metadata": {},
   "source": [
    "### See some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffb85d-3d94-48f9-9f7a-f8f82e08cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_zarr(zarr_path, res='0'):\n",
    "    # Open the Group\n",
    "    # We use mode='r' for safety.\n",
    "    root = zarr.open(zarr_path, mode='r')\n",
    "    \n",
    "    # Access the 'volume' array\n",
    "    # This object behaves like a numpy array but reads from disk on demand\n",
    "    try:\n",
    "        vol = root['volume']\n",
    "    except:\n",
    "        vol = root[res]\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Volume Shape: {vol.shape}\") # Expecting (Layers, Height, Width) or similar\n",
    "    print(f\"Data Type: {vol.dtype}\")\n",
    "    return vol\n",
    "\n",
    "def plot_slice(vol, several_slices=False):\n",
    "    # Define the slice\n",
    "    # We pick the middle layer (Z-axis) to ensure we see content (top/bottom layers can be empty)\n",
    "    z_index = vol.shape[0] // 2\n",
    "    y_index = 0 #vol.shape[1] // 2\n",
    "    x_index = 0 # vol.shape[2] // 2\n",
    "    # Define crop coordinates (Top-Left x=0, y=0)\n",
    "    x_crop = 100000\n",
    "    y_crop = 100000\n",
    "    \n",
    "    # Load the specific crop into memory\n",
    "    # Syntax: vol[z_index, y_start:y_end, x_start:x_end]\n",
    "    if several_slices: \n",
    "        for i in range (0, vol.shape[0], 1000):\n",
    "            crop = vol[i, y_index:y_index+y_crop, x_index:x_index+x_crop]\n",
    "\n",
    "            # Plot the mask\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            # We use 'gray' so 0 (False) is black and 1 (True) is white\n",
    "            plt.imshow(crop) \n",
    "            plt.title(\"Crop\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    else:\n",
    "        crop = vol[z_index, y_index:y_index+y_crop, x_index:x_index+x_crop]\n",
    "          \n",
    "        # Plot the mask\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # We use 'gray' so 0 (False) is black and 1 (True) is white\n",
    "        plt.imshow(crop) \n",
    "        plt.title(\"Crop\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5fa700-3088-4851-bd5e-5a75a0021299",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = load_zarr(zarr_path='/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training/PHerc172_masked.zarr', res='0')\n",
    "plot_slice(vol, several_slices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = load_zarr(zarr_path='/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training/PHercParis3_masked.zarr', res='0')\n",
    "plot_slice(vol, several_slices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = load_zarr(zarr_path='/mounts/disk2/Andre_Data_Augmentation/PhD/Vesuvius/PHercParis4.zarr', res='0')\n",
    "plot_slice(vol, several_slices=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b2a61",
   "metadata": {},
   "source": [
    "## Downloading entire volumes and doing some processing to save space\n",
    "\n",
    "* Example bash command to download a scroll (it will not download the low resoltuin versions!):\n",
    "```\n",
    "rclone copy --transfers=16 --checkers=32 --tpslimit=10 -P \\\n",
    "  --exclude \"/[1-5]/**\" \\\n",
    "  :http:full-scrolls/Scroll2/PHercParis3.volpkg/volumes_zarr_standardized/54keV_7.91um_Scroll2A.zarr/ \\\n",
    "  /mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training/PHercParis3.zarr \\\n",
    "  --http-url https://dl.ash2txt.org/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cde846-7c2b-4cc6-958a-5403a82b3f1a",
   "metadata": {},
   "source": [
    "### Save the low resolution scan to create a rough mask\n",
    "* Using the downsampled volume, 3D slicer was used to create a foreground mask, to remove the background and crop background volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28011b4-f2f8-4096-853d-1d2ceb9c0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "def save_lowres(vol, zarr_path, res_factor):\n",
    "    # --- 1. Load Data (Already done in your snippet) ---\n",
    "    # Assuming 'vol' is your Zarr array or loaded data\n",
    "    full_sample = vol[:,:,:] \n",
    "    print(f\"Data Loaded. Shape: {full_sample.shape}, Dtype: {full_sample.dtype}\")\n",
    "    \n",
    "    full_sample[::res_factor, :, :]\n",
    "    # --- 2. Calculate Statistics ---\n",
    "    p_min = np.percentile(full_sample, 0.1) \n",
    "    p_max = np.percentile(full_sample, 99.9)\n",
    "    \n",
    "    print(f\"Windowing: {p_min} to {p_max}\")\n",
    "    \n",
    "    # --- 3. Apply Windowing (The Math) ---\n",
    "    # We work on 'full_sample' directly\n",
    "    # A. Clip outliers\n",
    "    img_processed = np.clip(full_sample, p_min, p_max)\n",
    "    \n",
    "    # B. Normalize to 0.0 - 1.0\n",
    "    # We use .astype(float) to prevent integer division errors if input is uint16\n",
    "    img_processed = (img_processed.astype(np.float32) - p_min) / (p_max - p_min)\n",
    "    \n",
    "    # C. Scale to 255\n",
    "    img_processed = img_processed * 255.0\n",
    "    \n",
    "    # D. Round and Cast to uint8 (CRITICAL STEP)\n",
    "    img_final = np.round(img_processed).astype(np.uint8)\n",
    "    \n",
    "    # --- 4. Save as NIfTI (.nii.gz) ---\n",
    "     # '/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training/PHerc1667_Nov17.zarr'\n",
    "    file_name = zarr_path.split('/')[-1].split('.zarr')[0]\n",
    "    save_path = zarr_path.split(file_name)[0]\n",
    "    file_name = f\"{file_name}.nii.gz\"\n",
    "    save_path = os.path.join(save_path, file_name)\n",
    "    \n",
    "    # np.eye(4) \n",
    "    affine = np.eye(4)\n",
    "    \n",
    "    nifti_img = nib.Nifti1Image(img_final, affine)\n",
    "    nib.save(nifti_img, save_path)\n",
    "    \n",
    "    print(f\"✅ Success! Saved to {save_path}\")\n",
    "    return img_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed04634-b1d5-47a7-afc4-53d3b826b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_path = '/mounts/disk2/Andre_Data_Augmentation/PhD/Vesuvius/PHercParis4.zarr'\n",
    "vol = load_zarr(zarr_path=zarr_path, res='0')\n",
    "img_final_unit8 = save_lowres(vol, zarr_path, res_factor=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0209ce8",
   "metadata": {},
   "source": [
    "### Doing the post processing\n",
    "* Convert to unit8 (these scans have a huge constrast, there's no need to store in unit16)\n",
    "* Crop the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f22e1-25cd-4401-bae0-8d4ef50ddcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_damage(vol):\n",
    "    \"\"\"\n",
    "    Checking if we can still see the contrast difference in the volume\n",
    "    \"\"\"\n",
    "    # Grab a slice from the middle\n",
    "    mid_z = vol.shape[0] // 2\n",
    "    img_slice = vol[mid_z]\n",
    "    \n",
    "    print(f\"Slice Statistics:\")\n",
    "    print(f\"Min: {np.min(img_slice)}\")\n",
    "    print(f\"Max: {np.max(img_slice)}\")\n",
    "    print(f\"Mean: {np.mean(img_slice)}\")\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(img_slice.flatten(), bins=255, range=(1, 255)) # Ignore 0 (background)\n",
    "    plt.title(\"Histogram of Converted Data (Does it look 'squashed'?)\")\n",
    "    plt.xlabel(\"Pixel Value (0-255)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "check_damage(img_final_unit8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62de554-2733-4634-b0fb-6a5af7f7525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def apply_mask(mask_file):\n",
    "    # Load mask\n",
    "    mask = nib.load(mask_file).get_fdata()\n",
    "    \n",
    "    # Ensure mask is same shape as img_final\n",
    "    print(img_final.shape, mask.shape)\n",
    "    \n",
    "    # Apply mask\n",
    "    img_final_crop = img_final * mask\n",
    "    \n",
    "    # --- 4. Save as NIfTI (.nii.gz) ---\n",
    "    file_name = zarr_path.split('/')[-1].split('.zarr')[0]\n",
    "    save_path = zarr_path.split(file_name)[0]\n",
    "    file_name = f\"{file_name}_crop.nii.gz\"\n",
    "    save_path = os.path.join(save_path, file_name)\n",
    "    \n",
    "    # np.eye(4) \n",
    "    affine = np.eye(4)\n",
    "    \n",
    "    nifti_img = nib.Nifti1Image(img_final_crop, affine)\n",
    "    nib.save(nifti_img, save_path)\n",
    "    \n",
    "    print(f\"✅ Success! Saved to {save_path}\")\n",
    "    return img_final_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab009d51-1e09-4565-9fc3-c9d22a650b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_file = \"/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training/PHerc1667_Nov17_1_mask.nii.gz\"\n",
    "img_final_crop = apply_mask(mask_file)\n",
    "check_damage(img_final_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84bcffd-640f-4c0b-a678-e15b0e588236",
   "metadata": {},
   "source": [
    "### Apply mask to the full scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed9b48-a997-4f16-b0f8-9eea0c10a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "# Use the codec from numcodecs if possible, or just zarr wrappers\n",
    "from zarr.codecs import Blosc\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Path to your High-Res Zarr\n",
    "ZARR_PATH_IN = '/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training/PHerc172.zarr'\n",
    "\n",
    "# Output Path (Where the masked data will go)\n",
    "ZARR_PATH_OUT = '/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training/PHerc172_masked.zarr'\n",
    "\n",
    "# Path to your Low-Res Mask\n",
    "MASK_PATH = \"/mounts/disk4_tiago_e_andre/vesuvius/Vesuvius/DataSet/Pre-training/Downsample/PHerc172_downsample_mask.nii.gz\" \n",
    "\n",
    "# How many Z-slices to process at once?\n",
    "# 500 slices * 8000 * 8000 pixels * 1 byte = ~32 GB RAM per batch.\n",
    "# With 252 GB RAM, you can safely do 500-1000.\n",
    "SLAB_SIZE = 200 \n",
    "\n",
    "# Threshold (Mask is 0 or 1, so 0.5 works)\n",
    "MASK_THRESHOLD = 0.5 \n",
    "\n",
    "# =================================================\n",
    "\n",
    "def estimate_global_stats(vol, samples=100):\n",
    "    \"\"\"\n",
    "    Reads random slices to estimate p_min and p_max for the WHOLE volume.\n",
    "    This ensures consistent brightness across all slabs.\n",
    "    \"\"\"\n",
    "    print(f\"   calibrating contrast (reading {samples} random slices)...\")\n",
    "    z_depth = vol.shape[0]\n",
    "    # Pick random indices\n",
    "    indices = np.linspace(0, z_depth-1, samples, dtype=int)\n",
    "    \n",
    "    pixel_buffer = []\n",
    "    for z in indices:\n",
    "        # Read one slice\n",
    "        slc = vol[z, :, :]\n",
    "        pixel_buffer.append(slc.flatten())\n",
    "    \n",
    "    full_sample = np.concatenate(pixel_buffer)\n",
    "    \n",
    "    # Calculate robust percentiles for Surface Detection\n",
    "    # 0.1% bottom (ignore dead pixels)\n",
    "    # 99.9% top (ignore metal artifacts)\n",
    "    p_min = np.percentile(full_sample, 0.1)\n",
    "    p_max = np.percentile(full_sample, 99.9)\n",
    "    \n",
    "    print(f\"   Calibration Complete: Clip range [{p_min:.1f}, {p_max:.1f}]\")\n",
    "    return p_min, p_max\n",
    "    \n",
    "def apply_mask_convert_save():\n",
    "    # 1. Open Source\n",
    "    print(f\"Opening Source: {ZARR_PATH_IN}\")\n",
    "    src_vol = load_zarr(ZARR_PATH_IN, res='0')\n",
    "    print(f\"   Input: {src_vol.shape} | {src_vol.dtype}\")\n",
    "\n",
    "    # 2. Calibration (Get stats for uint8 conversion)\n",
    "    p_min, p_max = estimate_global_stats(src_vol)\n",
    "\n",
    "    # 3. Create Output (uint8)\n",
    "    print(f\"Creating Output: {ZARR_PATH_OUT}\")\n",
    "    root_out = zarr.open(ZARR_PATH_OUT, mode='w')\n",
    "    \n",
    "    # Zstd with Bitshuffle is best for masks + images\n",
    "    compressor = Blosc(cname='zstd', clevel=3, shuffle=2)\n",
    "    \n",
    "    dst_vol = root_out.create_dataset('0', \n",
    "                                      shape=src_vol.shape, \n",
    "                                      chunks=src_vol.chunks, \n",
    "                                      dtype=np.uint8,  # <--- FORCE UINT8 HERE\n",
    "                                      compressor=compressor,\n",
    "                                      overwrite=True)\n",
    "    \n",
    "    # Copy metadata\n",
    "    if hasattr(src_vol, 'attrs'):\n",
    "        try: dst_vol.attrs.update(src_vol.attrs)\n",
    "        except: pass\n",
    "\n",
    "    # 4. Load Mask\n",
    "    print(f\"Loading Mask...\")\n",
    "    mask_nii = nib.load(MASK_PATH)\n",
    "    mask_data = mask_nii.get_fdata()\n",
    "    \n",
    "    z_shape, y_shape, x_shape = src_vol.shape\n",
    "    scale_z = mask_data.shape[0] / z_shape\n",
    "    scale_y = mask_data.shape[1] / y_shape\n",
    "    scale_x = mask_data.shape[2] / x_shape\n",
    "\n",
    "    # 5. Process Slabs\n",
    "    total_slabs = math.ceil(z_shape / SLAB_SIZE)\n",
    "    print(f\"   Processing {total_slabs} slabs...\")\n",
    "\n",
    "    for i in tqdm(range(total_slabs)):\n",
    "        z_start = i * SLAB_SIZE\n",
    "        z_end = min((i + 1) * SLAB_SIZE, z_shape)\n",
    "        current_depth = z_end - z_start\n",
    "        \n",
    "        # A. Read High-Res Slab (uint16)\n",
    "        # Convert to float32 immediately for math\n",
    "        slab_data = src_vol[z_start:z_end, :, :].astype(np.float32)\n",
    "        \n",
    "        # B. Prepare Mask\n",
    "        mz_start = int(z_start * scale_z)\n",
    "        mz_end = min(int(math.ceil(z_end * scale_z)), mask_data.shape[0])\n",
    "        mask_slab_low = mask_data[mz_start:mz_end, :, :]\n",
    "        \n",
    "        if mask_slab_low.size == 0:\n",
    "            mask_slab_high = np.zeros(slab_data.shape, dtype=bool)\n",
    "        else:\n",
    "            zoom_z = current_depth / mask_slab_low.shape[0]\n",
    "            zoom_y = y_shape / mask_slab_low.shape[1]\n",
    "            zoom_x = x_shape / mask_slab_low.shape[2]\n",
    "            mask_slab_high = scipy.ndimage.zoom(mask_slab_low, (zoom_z, zoom_y, zoom_x), order=0)\n",
    "            \n",
    "        if mask_slab_high.shape != slab_data.shape:\n",
    "             mask_slab_high = mask_slab_high[:current_depth, :y_shape, :x_shape]\n",
    "        \n",
    "        # --- C. CORE LOGIC: WINDOWING & CONVERSION ---\n",
    "        \n",
    "        # 1. Clip Outliers (Metal/Air)\n",
    "        np.clip(slab_data, p_min, p_max, out=slab_data)\n",
    "        \n",
    "        # 2. Normalize to 0..1\n",
    "        slab_data -= p_min\n",
    "        slab_data /= (p_max - p_min)\n",
    "        \n",
    "        # 3. Scale to 255\n",
    "        slab_data *= 255.0\n",
    "        \n",
    "        # 4. Apply Mask (Set background to exactly 0)\n",
    "        # Doing this BEFORE casting ensures clean zeros\n",
    "        slab_data[mask_slab_high < MASK_THRESHOLD] = 0\n",
    "        \n",
    "        # 5. Round and Cast to uint8\n",
    "        slab_uint8 = np.round(slab_data).astype(np.uint8)\n",
    "        \n",
    "        # D. Write to Disk\n",
    "        dst_vol[z_start:z_end, :, :] = slab_uint8\n",
    "        \n",
    "        del slab_data, slab_uint8, mask_slab_high\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"✅ Done. Converted to uint8 + Masked.\")\n",
    "\n",
    "apply_mask_convert_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8e6a4-7633-4ddf-8720-c7b58950b87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14091c-77d9-4ddd-97b7-29d6e5c4fc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuvius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
