{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1c1c37",
   "metadata": {},
   "source": [
    "# Code to perform inferences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ac673",
   "metadata": {},
   "source": [
    "### Checking metadata to see if I need to worry about that in the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51c60b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files and calculating statistics...\n",
      "\n",
      "--- Analysis of 786 files ---\n",
      "⚠️ Metadata discrepancies found (Note: 'shape' is expected for sliding window):\n",
      "shape    8\n",
      "dtype: int64\n",
      "\n",
      "--- Detailed Discrepancies ---\n",
      "\n",
      "Field: [shape] | Standard: (314, 314, 320)\n",
      "  Found 49 outlier(s). Examples:\n",
      "    - 2669341205.nii.gz: (314, 297, 320)\n",
      "    - 4024699648.nii.gz: (250, 250, 256)\n",
      "    - 1531506078.nii.gz: (250, 250, 256)\n",
      "    - 1354910392.nii.gz: (250, 250, 256)\n",
      "    - 3049591468.nii.gz: (250, 216, 256)\n",
      "\n",
      "--- Intensity Statistics (Voxel Values) ---\n",
      "            min         max        mean\n",
      "min    0.000000  255.000000   37.179200\n",
      "max   19.000000  255.000008  147.771200\n",
      "mean   0.558524  255.000000   86.001276\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def analyze_nifti_folder(folder_path):\n",
    "    metadata_list = []\n",
    "    \n",
    "    print(\"Reading files and calculating statistics...\")\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".nii.gz\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            img = nib.load(file_path)\n",
    "            header = img.header\n",
    "            \n",
    "            # Load data to check intensity (useful for Vesuvius ink detection)\n",
    "            data = img.get_fdata()\n",
    "            \n",
    "            meta = {\n",
    "                \"filename\": filename,\n",
    "                \"shape\": tuple(img.shape),\n",
    "                \"pixdim\": tuple(np.round(header.get_zooms(), 4)), # Rounding to avoid float precision noise\n",
    "                \"datatype\": str(header.get_data_dtype()),\n",
    "                \"min\": np.min(data),\n",
    "                \"max\": np.max(data),\n",
    "                \"mean\": np.round(np.mean(data), 4),\n",
    "                \"qform\": int(header['qform_code']),\n",
    "                \"sform\": int(header['sform_code'])\n",
    "            }\n",
    "            metadata_list.append(meta)\n",
    "    \n",
    "    df = pd.DataFrame(metadata_list)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No .nii.gz files found.\")\n",
    "        return df\n",
    "\n",
    "    print(f\"\\n--- Analysis of {len(df)} files ---\")\n",
    "    \n",
    "    # 1. Check for global consistency\n",
    "    # Excluding 'filename', 'min', 'max', 'mean' for the strict metadata check\n",
    "    meta_cols = ['shape', 'pixdim', 'datatype', 'qform', 'sform']\n",
    "    consistency_check = df[meta_cols].nunique()\n",
    "    \n",
    "    if consistency_check.max() == 1:\n",
    "        print(\"✅ Success: Core metadata (pixdim, dtype, orientation) is uniform.\")\n",
    "    else:\n",
    "        print(\"⚠️ Metadata discrepancies found (Note: 'shape' is expected for sliding window):\")\n",
    "        print(consistency_check[consistency_check > 1])\n",
    "        \n",
    "        print(\"\\n--- Detailed Discrepancies ---\")\n",
    "        for column in meta_cols:\n",
    "            counts = df[column].value_counts()\n",
    "            if len(counts) > 1:\n",
    "                majority_value = counts.idxmax()\n",
    "                outliers = df[df[column] != majority_value]\n",
    "                print(f\"\\nField: [{column}] | Standard: {majority_value}\")\n",
    "                print(f\"  Found {len(outliers)} outlier(s). Examples:\")\n",
    "                for _, row in outliers.head(5).iterrows():\n",
    "                    print(f\"    - {row['filename']}: {row[column]}\")\n",
    "\n",
    "    # 2. Intensity Summary (Critical for Ink Detection)\n",
    "    print(\"\\n--- Intensity Statistics (Voxel Values) ---\")\n",
    "    print(df[['min', 'max', 'mean']].describe().loc[['min', 'max', 'mean']])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "path = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/Challenge_dataset_updated/train_images_nii_crop\"\n",
    "df = analyze_nifti_folder(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5642a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 786 image-label pairs...\n",
      "\n",
      "✅ Properly Aligned: 786\n",
      "✅ No shape mismatches detected between images and labels.\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_image_label_alignment(image_folder, label_folder):\n",
    "    mismatches = []\n",
    "    missing_labels = []\n",
    "    aligned_count = 0\n",
    "    \n",
    "    # We iterate through the image folder\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith(\".nii.gz\")]\n",
    "    \n",
    "    print(f\"Comparing {len(image_files)} image-label pairs...\")\n",
    "\n",
    "    for filename in image_files:\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        lbl_path = os.path.join(label_folder, filename)\n",
    "        \n",
    "        if not os.path.exists(lbl_path):\n",
    "            missing_labels.append(filename)\n",
    "            continue\n",
    "            \n",
    "        # Load headers only (fast)\n",
    "        img_hdr = nib.load(img_path).header\n",
    "        lbl_hdr = nib.load(lbl_path).header\n",
    "        \n",
    "        img_shape = img_hdr.get_data_shape()\n",
    "        lbl_shape = lbl_hdr.get_data_shape()\n",
    "        \n",
    "        if img_shape != lbl_shape:\n",
    "            mismatches.append({\n",
    "                \"filename\": filename,\n",
    "                \"image_shape\": img_shape,\n",
    "                \"label_shape\": lbl_shape\n",
    "            })\n",
    "        else:\n",
    "            aligned_count += 1\n",
    "\n",
    "    # --- Report Results ---\n",
    "    print(f\"\\n✅ Properly Aligned: {aligned_count}\")\n",
    "    \n",
    "    if missing_labels:\n",
    "        print(f\"❌ Missing Labels: {len(missing_labels)} (Images exist but no matching label file found)\")\n",
    "        # for m in missing_labels[:5]: print(f\"   - {m}\") # Uncomment to see names\n",
    "\n",
    "    if mismatches:\n",
    "        print(f\"⚠️ Shape Mismatches: {len(mismatches)}\")\n",
    "        df_mismatch = pd.DataFrame(mismatches)\n",
    "        print(df_mismatch)\n",
    "    else:\n",
    "        print(\"✅ No shape mismatches detected between images and labels.\")\n",
    "\n",
    "    return pd.DataFrame(mismatches)\n",
    "\n",
    "# Paths\n",
    "img_dir = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/Challenge_dataset_updated/train_images_nii_crop\"\n",
    "lbl_dir = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/Challenge_dataset_updated/train_labels_nii_crop\"\n",
    "\n",
    "# Run check\n",
    "mismatch_df = check_image_label_alignment(img_dir, lbl_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8961fd3d",
   "metadata": {},
   "source": [
    "## Test inference class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9e5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "from infer_class import VesuviusInferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3f8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: /home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/checkpoints/second_step/wandb/run-20260115_141541-36wyonm0/files/model/model_epoch_40.pth\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from main_train_class import main_train_STU_Net\n",
    "\n",
    "# Warning Suppressions (Must be at the top)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"zarr.codecs.numcodecs._codecs\")\n",
    "\n",
    "\n",
    "CONFIG_FILE = '../configs/infer.json'\n",
    "with open(CONFIG_FILE, \"r\") as f:\n",
    "    config_content = json.load(f)\n",
    "infer_object = VesuviusInferer(config_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d363e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/183 [00:00<?, ?it/s]Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "100%|██████████| 183/183 [01:05<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "input_data = {\n",
    "    'image': '/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_images/3925240194.tif',\n",
    "    'gt': '/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_labels/3925240194.tif'\n",
    "}\n",
    "pred, gt, roi_mask = infer_object.infer(input_data, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57c6983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.metrics.meandice DiceHelper.__init__:softmax: Argument `softmax` has been deprecated since version 1.5. It will be removed in version 1.7. Use `apply_argmax` instead.\n",
      "monai.metrics.meandice DiceHelper.__init__:sigmoid: Argument `sigmoid` has been deprecated since version 1.5. It will be removed in version 1.7. Use `threshold` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.8009], device='cuda:0'), tensor([1.], device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "results = infer_object.evaluate(predictions=pred, gt=gt, roi_mask=roi_mask)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ccc44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "infer_object.save_nifti(\n",
    "    torch_tensor=pred, \n",
    "    save_path=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/trash/infer/2290837_pred.nii.gz\", \n",
    "    real_file=None)\n",
    "\n",
    "infer_object.save_nifti(\n",
    "    torch_tensor=gt, \n",
    "    save_path=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/trash/infer/2290837_gt.nii.gz\", \n",
    "    real_file=None)\n",
    "\n",
    "infer_object.save_nifti(\n",
    "    torch_tensor=roi_mask, \n",
    "    save_path=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/trash/infer/2290837_roi_mask.nii.gz\", \n",
    "    real_file=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba68e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "infer_object.save_tiff(\n",
    "    torch_tensor=pred, \n",
    "    save_path=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/trash/infer/2290837_pred.tiff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe04b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it needs to contain the paths to submission['tif_paths']\n",
    "# solution -> row['tif_paths'] (paths to the ground truth tifs)\n",
    "# row['pred_paths'] comes from submission['tif_paths']\n",
    "\n",
    "def create_dfs(path_dir):\n",
    "    # Generate DataFrame\n",
    "    result_df = glob.glob(os.path.join(path_dir, '**/*.tif'), recursive=True)\n",
    "    result_df = pd.DataFrame({'tif_paths': result_df})\n",
    "    result_df['id'] = result_df['tif_paths'].apply(lambda x: os.path.basename(x).split('.')[0])\n",
    "\n",
    "    # save dataframe to csv (take name from the path_dir last folder)\n",
    "    csv_name = os.path.basename(os.path.normpath(path_dir)) + '_df.csv'\n",
    "    result_df.to_csv(os.path.join(path_dir, csv_name), index=False)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "create_dfs(\n",
    "    path_dir=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_labels\"\n",
    "    )\n",
    "create_dfs(\n",
    "    path_dir=\"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_pred\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e304da6",
   "metadata": {},
   "source": [
    "#### dataset infer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54d44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def dataset_inference(dataset_path, pred_save_dir):\n",
    "    \"\"\" inference on all cases in a dataset directory and save predictions \"\"\"\n",
    "    os.makedirs(pred_save_dir, exist_ok=True)\n",
    "    all_cases = glob.glob(os.path.join(dataset_path, '**/*.tif'), recursive=True)\n",
    "    \n",
    "    for case in all_cases:\n",
    "        print(f\"Processing case: {case}\")\n",
    "        input_data = {\n",
    "            'image': str(case),\n",
    "            'gt': None\n",
    "        }\n",
    "        \n",
    "        pred = infer_object.infer(input_data, test=True)\n",
    "        infer_object.save_tiff(\n",
    "            torch_tensor=pred, \n",
    "            save_path=os.path.join(pred_save_dir, f\"{case.split('/')[-1].split('.')[0]}.tif\")\n",
    "        )\n",
    "    # Creating the dataframe for testing\n",
    "    infer_object.create_dfs(pred_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e05b6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing case: /home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_images/2290837.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/183 [00:00<?, ?it/s]Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "100%|██████████| 183/183 [01:03<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing case: /home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_images/3925240194.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [01:02<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_inference(\n",
    "    dataset_path = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_images\", \n",
    "    pred_save_dir = \"/home/shadowtwin/Desktop/AI_work/Vesuvius_Challenge/Vesuvius/DataSet/test_pred\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c095233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuvius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
